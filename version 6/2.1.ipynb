{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1nhXZArHqJp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "current_learning_iteration = 0\n",
        "\n",
        "# Device Class\n",
        "class Device:\n",
        "    def __init__(self, device_id, ram, storage, cpu, bandwidth, battery, charging):\n",
        "        self.device_id = device_id\n",
        "        self.ram = ram\n",
        "        self.storage = storage\n",
        "        self.cpu = cpu\n",
        "        self.bandwidth = bandwidth\n",
        "        self.battery = battery\n",
        "        self.charging = charging\n",
        "        self.energy_consumption = ram + storage + cpu + bandwidth\n",
        "        self.model = self.create_model()\n",
        "        self.last_round_participated = 0\n",
        "        self.data = None  # Placeholder for dataset partition\n",
        "\n",
        "        self.test_data = None\n",
        "        # TODO: add test data for each device\n",
        "        # TODO: use the test data retrieved from load dataset from mnist and distribute it between devices and global model\n",
        "\n",
        "\n",
        "        self.number_of_times_fitted = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = keras.Sequential([\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "        model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "                      # new\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Global Model\n",
        "# Define the global model with the same architecture\n",
        "# def create_global_model():\n",
        "#     model = keras.Sequential([\n",
        "#         layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "#         layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "#         layers.Flatten(),\n",
        "#         layers.Dense(128, activation='relu'),\n",
        "#         layers.Dense(10, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "#                   # new\n",
        "#                   loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# global_model = create_global_model()\n",
        "global_model = Device.create_model()\n",
        "\n",
        "\n",
        "# def multiply_tensors(a, b):\n",
        "#     if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
        "#         return a * b\n",
        "#     elif isinstance(a, list) and isinstance(b, list):\n",
        "#         return [multiply_tensors(x, y) for x, y in zip(a, b)]\n",
        "#     else:\n",
        "#         raise ValueError(\"Incompatible types for multiplication\")\n",
        "\n",
        "# def add_tensors(a, b):\n",
        "#     if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
        "#         return a + b\n",
        "#     elif isinstance(a, list) and isinstance(b, list):\n",
        "#         return [add_tensors(x, y) for x, y in zip(a, b)]\n",
        "#     else:\n",
        "#         raise ValueError(\"Incompatible types for addition\")\n",
        "\n",
        "def sum_all_nested_lists(list_of_lists):\n",
        "    def recursive_sum(lists):\n",
        "        if isinstance(lists[0], list):\n",
        "            return [recursive_sum([lst[i] for lst in lists]) for i in range(len(lists[0]))]\n",
        "        else:\n",
        "            return sum(lists)\n",
        "    \n",
        "    return recursive_sum(list_of_lists)\n",
        "\n",
        "def multiply_nested_list(lst, factor):\n",
        "    result = []\n",
        "    for item in lst:\n",
        "        if isinstance(item, list):\n",
        "            # Recursively handle sublists\n",
        "            result.append(multiply_nested_list(item, factor))\n",
        "        else:\n",
        "            # Multiply number\n",
        "            result.append(item * factor)\n",
        "    return result\n",
        "\n",
        "import numpy as np\n",
        "def aggregate_weights(bitstring):\n",
        "    \"\"\"Computes the weighted average of model weights from all devices and updates the global model.\"\"\"\n",
        "    # new\n",
        "    global devices\n",
        "    selected_devices = []\n",
        "    for device in devices:\n",
        "        if int(bitstring[int(device.device_id)]) == 1:\n",
        "            selected_devices.append(device)\n",
        "    \n",
        "    num_devices = len(selected_devices)\n",
        "    if num_devices == 0:\n",
        "        print(\"No devices available for aggregation.\")\n",
        "        return\n",
        "\n",
        "    device_weights_all_layers = []\n",
        "    device_participation_ratio = []\n",
        "    data_lengths = []\n",
        "\n",
        "    for device in selected_devices:\n",
        "        device_weights_all_layers.append(device.model.get_weights())\n",
        "        print(\"*******************\")\n",
        "        print(device.device_id)\n",
        "        # print(\"this device's weights:\")\n",
        "        # print(device.model.get_weights())\n",
        "        \n",
        "        device_participation_ratio.append(device.last_round_participated / current_learning_iteration)\n",
        "        print(\"this device's participation ratio:\")\n",
        "        print(device.last_round_participated / current_learning_iteration)\n",
        "        \n",
        "        data_lengths.append(len(device.data[0]))\n",
        "        print(\"this device's data to all ratio:\")\n",
        "        print(len(device.data[0])/60000.0)\n",
        "\n",
        "\n",
        "    sum_data = 0\n",
        "    for data_len in data_lengths:\n",
        "        sum_data += data_len\n",
        "    \n",
        "    data_fractions = []\n",
        "    for device in selected_devices:\n",
        "        data_fractions.append(len(device.data[0])/float(sum_data))\n",
        "\n",
        "    aggregated_weights_devices = []\n",
        "    for d in range(len(selected_devices)):\n",
        "        aggregated_weights_devices.append(multiply_nested_list(selected_devices[d].model.get_weights(), data_fractions[d]*device_participation_ratio[d]))\n",
        "    \n",
        "    aggregated_weights = sum_all_nested_lists(aggregated_weights_devices)\n",
        "    # TODO: Weighted multiplication for each node in each layer of the neural network of the received devices and then summing\n",
        "    #       the related parts together so that we get a full weighted average of all these devices' models\n",
        "\n",
        "    print(\"Aggregated weights:\")\n",
        "    for layer_idx, layer_weights in enumerate(aggregated_weights):\n",
        "        print(f\"Layer {layer_idx}: {layer_weights.shape}\")\n",
        "    return aggregated_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RtoM8SQmHqJs",
        "outputId": "646ea129-2068-4c84-9813-ec9b268b8ee9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.1054 - loss: 2.3094\n",
            "Global Model Accuracy: 0.1090\n",
            "------------------------------------------------------------\n",
            "0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3141 - loss: 2.2022\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7168 - loss: 1.6142\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8033 - loss: 0.8711\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8536 - loss: 0.6068\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8706 - loss: 0.4851\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8625 - loss: 0.4652\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9026 - loss: 0.3834\n",
            "0.0\n",
            "1\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2894 - loss: 2.2001\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7664 - loss: 1.5760\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8287 - loss: 0.8384\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8753 - loss: 0.5775\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8918 - loss: 0.4480\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9181 - loss: 0.3388\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9130 - loss: 0.3440\n",
            "1.0\n",
            "2\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2874 - loss: 2.1993\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7110 - loss: 1.5759\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8255 - loss: 0.8735\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8526 - loss: 0.5913\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8893 - loss: 0.4483\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8966 - loss: 0.4093\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8955 - loss: 0.3837\n",
            "2.0\n",
            "3\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2753 - loss: 2.2069\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7190 - loss: 1.6371\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8044 - loss: 0.9289\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8534 - loss: 0.6324\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8776 - loss: 0.4930\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8872 - loss: 0.4235\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8979 - loss: 0.3747\n",
            "3.0\n",
            "4\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3016 - loss: 2.2023\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7044 - loss: 1.5994\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8152 - loss: 0.9007\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8585 - loss: 0.6044\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8723 - loss: 0.4941\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8756 - loss: 0.4581\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8925 - loss: 0.3861\n",
            "4.0\n",
            "5\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3124 - loss: 2.2014\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7347 - loss: 1.5649\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8444 - loss: 0.8226\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8600 - loss: 0.5655\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8973 - loss: 0.4373\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8959 - loss: 0.3833\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9117 - loss: 0.3325\n",
            "5.0\n",
            "6\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2647 - loss: 2.2094\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6964 - loss: 1.6444\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8109 - loss: 0.9413\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8391 - loss: 0.6703\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8607 - loss: 0.5113\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8816 - loss: 0.4365\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8874 - loss: 0.4062\n",
            "6.0\n",
            "7\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2949 - loss: 2.2133\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7217 - loss: 1.6478\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7969 - loss: 0.9818\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8405 - loss: 0.6682\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8585 - loss: 0.5249\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8686 - loss: 0.4777\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8887 - loss: 0.3990\n",
            "7.0\n",
            "8\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2890 - loss: 2.2054\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6943 - loss: 1.6850\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8032 - loss: 0.9640\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8585 - loss: 0.6295\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8755 - loss: 0.5143\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8843 - loss: 0.4185\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8822 - loss: 0.4124\n",
            "8.0\n",
            "9\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3069 - loss: 2.1974\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7795 - loss: 1.5479\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8507 - loss: 0.8097\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8954 - loss: 0.5221\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8678 - loss: 0.4727\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9082 - loss: 0.3690\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9104 - loss: 0.3366\n",
            "9.0\n",
            "10\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3274 - loss: 2.1926\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7377 - loss: 1.5570\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8386 - loss: 0.8337\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8628 - loss: 0.6008\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8852 - loss: 0.4579\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8986 - loss: 0.3987\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9012 - loss: 0.3668\n",
            "10.0\n",
            "11\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2679 - loss: 2.2096\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7264 - loss: 1.6425\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8194 - loss: 0.9280\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8508 - loss: 0.6130\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8480 - loss: 0.5046\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8851 - loss: 0.4273\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8991 - loss: 0.3687\n",
            "11.0\n",
            "12\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2852 - loss: 2.2076\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7440 - loss: 1.6024\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8240 - loss: 0.8832\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8547 - loss: 0.6051\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8729 - loss: 0.4817\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8964 - loss: 0.4018\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9007 - loss: 0.3677\n",
            "12.0\n",
            "13\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2842 - loss: 2.1966\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7352 - loss: 1.5753\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8389 - loss: 0.8736\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8707 - loss: 0.5945\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8745 - loss: 0.4802\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9021 - loss: 0.3998\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8946 - loss: 0.3674\n",
            "13.0\n",
            "14\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2909 - loss: 2.2021\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6936 - loss: 1.6187\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8148 - loss: 0.9249\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8581 - loss: 0.6255\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8793 - loss: 0.4947\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8712 - loss: 0.4620\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8931 - loss: 0.3883\n",
            "14.0\n",
            "15\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2913 - loss: 2.2127\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6827 - loss: 1.7023\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7979 - loss: 1.0426\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8290 - loss: 0.7036\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8593 - loss: 0.5646\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8665 - loss: 0.4934\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8852 - loss: 0.4346\n",
            "15.0\n",
            "16\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2817 - loss: 2.2110\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7478 - loss: 1.6333\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8108 - loss: 0.9457\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8597 - loss: 0.6051\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8786 - loss: 0.4785\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8847 - loss: 0.4248\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9084 - loss: 0.3709\n",
            "16.0\n",
            "17\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2756 - loss: 2.2120\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7263 - loss: 1.6422\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8209 - loss: 0.9237\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8563 - loss: 0.5911\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8769 - loss: 0.4735\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9003 - loss: 0.3841\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9012 - loss: 0.3510\n",
            "17.0\n",
            "18\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2757 - loss: 2.2038\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7412 - loss: 1.6153\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8214 - loss: 0.9049\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8329 - loss: 0.6376\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8558 - loss: 0.5193\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8712 - loss: 0.4562\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8834 - loss: 0.4053\n",
            "18.0\n",
            "19\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2783 - loss: 2.2093\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7224 - loss: 1.6289\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7934 - loss: 0.9323\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8459 - loss: 0.6361\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8638 - loss: 0.5095\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8871 - loss: 0.4427\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8886 - loss: 0.4209\n",
            "19.0\n",
            "20\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3031 - loss: 2.2141\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7254 - loss: 1.6357\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8274 - loss: 0.9201\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8375 - loss: 0.6543\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8778 - loss: 0.4901\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8916 - loss: 0.4158\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9009 - loss: 0.3748\n",
            "20.0\n",
            "21\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2477 - loss: 2.2101\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7151 - loss: 1.6325\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8145 - loss: 0.9161\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8393 - loss: 0.6302\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8606 - loss: 0.5390\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8777 - loss: 0.4645\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8868 - loss: 0.4124\n",
            "21.0\n",
            "22\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2944 - loss: 2.2031\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7350 - loss: 1.6424\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8175 - loss: 0.9167\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8549 - loss: 0.6255\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8807 - loss: 0.4940\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8713 - loss: 0.4536\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9027 - loss: 0.3768\n",
            "22.0\n",
            "23\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2670 - loss: 2.2142\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7185 - loss: 1.6351\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8031 - loss: 0.9458\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8475 - loss: 0.6212\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8614 - loss: 0.5222\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8803 - loss: 0.4444\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8793 - loss: 0.4289\n",
            "23.0\n",
            "24\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3331 - loss: 2.2087\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7144 - loss: 1.6237\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7882 - loss: 0.9333\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8307 - loss: 0.6345\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8721 - loss: 0.4894\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8690 - loss: 0.4704\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9024 - loss: 0.3688\n",
            "24.0\n",
            "25\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3194 - loss: 2.2008\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7756 - loss: 1.5817\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8510 - loss: 0.8553\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8716 - loss: 0.5752\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8882 - loss: 0.4554\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8962 - loss: 0.4092\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9090 - loss: 0.3738\n",
            "25.0\n",
            "26\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2941 - loss: 2.2039\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7134 - loss: 1.6092\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8060 - loss: 0.9164\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8222 - loss: 0.6678\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8445 - loss: 0.5479\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8714 - loss: 0.4554\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8830 - loss: 0.4199\n",
            "26.0\n",
            "27\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3028 - loss: 2.2015\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7943 - loss: 1.5493\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8354 - loss: 0.8293\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8803 - loss: 0.5532\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.4484\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9106 - loss: 0.3571\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9056 - loss: 0.3467\n",
            "27.0\n",
            "28\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3149 - loss: 2.1990\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7655 - loss: 1.5812\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8269 - loss: 0.8747\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8792 - loss: 0.5578\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8989 - loss: 0.4341\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8985 - loss: 0.3667\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9112 - loss: 0.3381\n",
            "28.0\n",
            "29\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3300 - loss: 2.1850\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8223 - loss: 1.4285\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9079 - loss: 0.6402\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9106 - loss: 0.4068\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9311 - loss: 0.3113\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9335 - loss: 0.2735\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9480 - loss: 0.2280\n",
            "29.0\n",
            "*******************\n",
            "0.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "1.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "2.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "3.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "4.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "5.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "6.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "7.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "8.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "9.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "10.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "11.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "12.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "13.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "14.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "15.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "16.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "17.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "18.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "19.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "20.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "21.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "22.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "23.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "24.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "25.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "26.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "27.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "28.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "29.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "Aggregated weights:\n",
            "Layer 0: (3, 3, 1, 32)\n",
            "Layer 1: (32,)\n",
            "Layer 2: (5408, 128)\n",
            "Layer 3: (128,)\n",
            "Layer 4: (128, 10)\n",
            "Layer 5: (10,)\n",
            "------------------------------------------------------------\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.2300\n",
            "Global Model Accuracy: 0.9290\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from CSV\n",
        "csv_file = 'devices.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "\n",
        "# Convert CSV rows into device objects\n",
        "devices = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    device = Device(\n",
        "        row['id'], row['ram'], row['storage'], row['cpu'], row['bandwidth'], row['battery'],\n",
        "        row.get('charging', 0)\n",
        "    )\n",
        "    devices.append(device)\n",
        "\n",
        "\n",
        "# LIMIT TO 30 DEVICES\n",
        "devices = devices[:30]\n",
        "\n",
        "# Load MNIST dataset  # new\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "# new\n",
        "from keras.utils import to_categorical\n",
        "# Convert labels to categorical (one-hot encoded)\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Normalize data and reshape for CNN\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)  # Add channel dimension\n",
        "\n",
        "# new\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1)  # Add channel dimension\n",
        "\n",
        "# new\n",
        "# Shuffle data\n",
        "# indices = np.arange(len(x_train))\n",
        "# np.random.shuffle(indices)\n",
        "# x_train, y_train = x_train[indices], y_train[indices]\n",
        "\n",
        "# new\n",
        "# Correct test split\n",
        "split_index = int(0.8 * len(x_test))\n",
        "x_test_devices, y_test_devices = x_test[:split_index], y_test[:split_index]\n",
        "x_test_global, y_test_global = x_test[split_index:], y_test[split_index:]\n",
        "\n",
        "# Training data (for devices)\n",
        "x_train_devices, y_train_devices = x_train, y_train\n",
        "\n",
        "# Split training data among devices\n",
        "num_devices = len(devices)\n",
        "split_size = len(x_train_devices) // num_devices\n",
        "\n",
        "for i, device in enumerate(devices):\n",
        "    start = i * split_size\n",
        "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_train_devices)\n",
        "    device.data = (x_train_devices[start:end], y_train_devices[start:end])\n",
        "\n",
        "# Split test data (device-level)\n",
        "split_size = len(x_test_devices) // num_devices\n",
        "\n",
        "for i, device in enumerate(devices):\n",
        "    start = i * split_size\n",
        "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_test_devices)\n",
        "    device.test_data = (x_test_devices[start:end], y_test_devices[start:end])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# First Iteration\n",
        "bitstring = [1 for _ in range(len(devices))]\n",
        "print(bitstring)\n",
        "\n",
        "\n",
        "# global model sends its weights to all devices\n",
        "# for device in devices:\n",
        "#     device.model.set_weight(global_model.get_weight())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LAST_PARTICIPATED_ROUND_FOR_ALL_DEVICES = [0 for _ in range(len(devices))]\n",
        "# print(len(LAST_PARTICIPATED_ROUND_FOR_ALL_DEVICES))\n",
        "LAST_WEIGHTS_SENT_FOR_ALL_DEVICES = [None for _ in range(len(devices))]\n",
        "\n",
        "def fit_bitstring_devices(bitstring):\n",
        "    global current_learning_iteration\n",
        "    # global LAST_PARTICIPATED_ROUND_FOR_ALL_DEVICES\n",
        "    global LAST_WEIGHTS_SENT_FOR_ALL_DEVICES\n",
        "    current_learning_iteration += 1\n",
        "    for device in devices:\n",
        "        print(int(device.device_id))\n",
        "        if bitstring[int(device.device_id)] == 1:\n",
        "            device.model.fit(device.data[0], device.data[1], epochs=7, verbose=1)\n",
        "            print(device.device_id)\n",
        "            # print(len(LAST_PARTICIPATED_ROUND_FOR_ALL_DEVICES))\n",
        "            # LAST_PARTICIPATED_ROUND_FOR_ALL_DEVICES[int(device.device_id)] = current_learning_iteration\n",
        "            device.last_round_participated = current_learning_iteration\n",
        "            LAST_WEIGHTS_SENT_FOR_ALL_DEVICES[int(device.device_id)] = device.model.get_weights()\n",
        "            device.number_of_times_fitted += 1\n",
        "\n",
        "\n",
        "\n",
        "def give_global_model_weights_to_bitstring_devices(bitstring):\n",
        "    for device in devices:\n",
        "        if int(bitstring[int(device.device_id)]) == 1:\n",
        "            device.model.set_weights(global_model.get_weights())\n",
        "# Call this function after training the local models:\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "\n",
        "# global model sends its weights to all devices\n",
        "give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "# new\n",
        "# print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "fit_bitstring_devices(bitstring)\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "# give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "# print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XliqF4L-Sd",
        "outputId": "9805c23f-170d-4535-c652-97e4b6b9203f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymoo in d:\\github repos\\fl\\venv\\lib\\site-packages (0.6.1.3)\n",
            "Requirement already satisfied: numpy>=1.15 in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1 in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (1.14.1)\n",
            "Requirement already satisfied: matplotlib>=3 in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (1.7.0)\n",
            "Requirement already satisfied: cma==3.2.2 in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (3.2.2)\n",
            "Requirement already satisfied: alive-progress in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (3.2.0)\n",
            "Requirement already satisfied: dill in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (0.3.9)\n",
            "Requirement already satisfied: Deprecated in d:\\github repos\\fl\\venv\\lib\\site-packages (from pymoo) (1.2.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\github repos\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (2.9.0.post0)\n",
            "Requirement already satisfied: about-time==4.2.1 in d:\\github repos\\fl\\venv\\lib\\site-packages (from alive-progress->pymoo) (4.2.1)\n",
            "Requirement already satisfied: grapheme==0.6.0 in d:\\github repos\\fl\\venv\\lib\\site-packages (from alive-progress->pymoo) (0.6.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in d:\\github repos\\fl\\venv\\lib\\site-packages (from Deprecated->pymoo) (1.17.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\github repos\\fl\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiEDWLPjHqJt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.operators.sampling.rnd import BinaryRandomSampling\n",
        "from pymoo.operators.crossover.pntx import TwoPointCrossover\n",
        "from pymoo.operators.mutation.bitflip import BitflipMutation\n",
        "from pymoo.operators.selection.tournament import TournamentSelection\n",
        "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
        "from keras.models import clone_model\n",
        "\n",
        "\n",
        "# Parameters\n",
        "NUM_DEVICES = num_devices   # Number of devices (length of bitstring)\n",
        "POPULATION_SIZE = 50\n",
        "NUM_GENERATIONS = 3\n",
        "\n",
        "# Step 1: Define the Problem\n",
        "import numpy as np\n",
        "from pymoo.core.problem import Problem\n",
        "\n",
        "class FederatedLearningProblem(Problem):\n",
        "    def __init__(self, num_devices, devices, global_model, x_test_global, y_test_global):\n",
        "        super().__init__(\n",
        "            n_var=num_devices,         # Number of variables (bitstring length)\n",
        "            n_obj=3,                   # Number of objectives\n",
        "            n_constr=0,                # No constraints\n",
        "            xl=np.zeros(num_devices),  # Lower bound (0)\n",
        "            xu=np.ones(num_devices),   # Upper bound (1)\n",
        "            type_var=np.bool_          # Binary variables (bitstrings)\n",
        "        )\n",
        "        self.devices = devices\n",
        "        self.global_model = global_model\n",
        "        self.x_test_global = x_test_global\n",
        "        self.y_test_global = y_test_global\n",
        "\n",
        "        # Save the initial global model weights\n",
        "        self.initial_global_weights = global_model.get_weights()\n",
        "\n",
        "    def _evaluate(self, X, out, *args, **kwargs):\n",
        "        \"\"\"Evaluates objective values for each solution in the population.\"\"\"\n",
        "        num_solutions = len(X)\n",
        "        F = np.zeros((num_solutions, 3))  # Initialize objective matrix\n",
        "\n",
        "        for i, bitstring in enumerate(X):\n",
        "            # Reset the global model to its initial state\n",
        "            # Update device participation based on the bitstring\n",
        "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if int(bit) == 1]\n",
        "            # Objective 1: Hardware Objectives (maximize)\n",
        "\n",
        "            # new\n",
        "            hardware_score = 0.0\n",
        "            for device in selected_devices:\n",
        "                device_hardware_score = float(6 - (device.ram + device.storage + device.cpu + device.bandwidth + device.battery + device.charging)) / 6.0\n",
        "                hardware_score += device_hardware_score\n",
        "\n",
        "            F[i, 0] = hardware_score  # Minimize (negative of hardware score)\n",
        "\n",
        "            # new\n",
        "            fairness_score = 0\n",
        "            for device in self.devices:\n",
        "                if bitstring[int(device.device_id)] == 1:\n",
        "                    # new\n",
        "                    _, accuracy = global_model.evaluate(device.test_data[0], device.test_data[1], verbose=0)\n",
        "                    fairness_score += accuracy\n",
        "\n",
        "            F[i, 1] = fairness_score/float(len(selected_devices))  # Minimize (negative of fairness score)  # Added (/Selected Devices) to normalize between 0 and 1\n",
        "            \n",
        "            # Objective 3: Global Model Accuracy (Performance) (maximize)\n",
        "            temp_global_model = clone_model(global_model)\n",
        "            temp_global_model.set_weights(aggregate_weights(selected_devices))\n",
        "            _, global_accuracy = temp_global_model.evaluate(self.x_test_global, self.y_test_global, verbose=0)\n",
        "            F[i, 2] = 1 - global_accuracy  # Minimize (1 - accuracy)\n",
        "        out[\"F\"] = F  # Set the objective values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x60J9IXFHqJt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "problem = FederatedLearningProblem(\n",
        "    num_devices=NUM_DEVICES,\n",
        "    devices=devices,\n",
        "    global_model=global_model,\n",
        "    x_test_global=x_test_global,\n",
        "    y_test_global=y_test_global\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Configure NSGA-II Algorithm\n",
        "algorithm = NSGA2(\n",
        "    pop_size=POPULATION_SIZE,\n",
        "    sampling=BinaryRandomSampling(),      # Random bitstrings\n",
        "    crossover=TwoPointCrossover(),        # Two-point crossover\n",
        "    mutation=BitflipMutation(),           # Bit flip mutation\n",
        "    eliminate_duplicates=True             # Avoid duplicate solutions\n",
        ")\n",
        "\n",
        "\n",
        "current_learning_iteration += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhEjXpgNGaxb",
        "outputId": "f2bb719a-aa35-4c87-a62a-3c17b675a8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 28, 28, 1)\n",
            "(2000, 10)\n",
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n"
          ]
        }
      ],
      "source": [
        "# DEBUG:\n",
        "print(x_test_global.shape)\n",
        "print(y_test_global.shape)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "FM0EBc17HqJu",
        "outputId": "b0ea08e3-fb08-467b-f648-29dc724a5ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      2 |             - |             -\n",
            "     2 |      100 |      4 |  0.3068293454 |         ideal\n",
            "     3 |      150 |      9 |  0.2503217633 |         ideal\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "print(\"GLOBAL MODEL BEFORE OPTIMIZATION\")\n",
        "print(global_model.get_weights())\n",
        "\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    # termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")\n",
        "print(\"GLOBAL MODEL AFTER OPTIMIZATION\")\n",
        "print(global_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrcLQ8XXHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Pareto Front (Bitstrings):\n",
            "0000111100110100000000000010000101001000001110110011111100100110100010011001010001001010110100100010\n",
            "0000111100110110000000000010001100101011000001101000101000100110100010011001010001001010110100100010\n",
            "1011010111000011000010100110000000100101010001100100000001000100010110010011001001010110100000001100\n",
            "0101010010010000001111100011010011001000001110110010111011100111010010000010001010111101101011000100\n",
            "0110100110001010001110000100001000100011101110101010101111000110111100100111000010000110110011100100\n",
            "0101010010010010001111100011010011001111000011011101101100101110100010010010001010111101101011100100\n",
            "0001000000100100000000000011100101000101110100011101111010011100101100000101001001100000001011010010\n",
            "0100000100011100011100011110010000000001001011001110111100100110100010011011000000010101100101000110\n",
            "1011011100001110010010111000001000000000010101100100000001000100010110000011001001010110100000001100\n",
            "100\n",
            "0000111100110100000000000010000101001000001110110011111100100110100010011001010001001010110100100010\n",
            "[array([[[[ 0.02163844,  0.03165662,  0.01536216,  0.02086407,\n",
            "           0.00331281,  0.02121367,  0.01848504,  0.01150802,\n",
            "           0.03173638,  0.02259891,  0.02047072,  0.02573678,\n",
            "           0.01738155,  0.02337671,  0.01120739,  0.01508757,\n",
            "           0.02137124,  0.0264954 ,  0.00931556,  0.01206335,\n",
            "           0.02920035,  0.01045677,  0.00095183,  0.01798654,\n",
            "           0.02254122,  0.02802641,  0.02036513,  0.03844364,\n",
            "           0.00547812,  0.01080036,  0.01550483,  0.01953718]],\n",
            "\n",
            "        [[ 0.03435625,  0.04247217,  0.01023734,  0.0182803 ,\n",
            "           0.01904002,  0.02299859,  0.04000052,  0.01513761,\n",
            "           0.02024942,  0.03281915,  0.04219521,  0.03342689,\n",
            "           0.00235677,  0.01846253,  0.02548883,  0.02547626,\n",
            "           0.02811066,  0.03476975,  0.02877311,  0.02541521,\n",
            "           0.03266086,  0.01476631,  0.03283615,  0.02409212,\n",
            "           0.01258491,  0.02351465,  0.01677969,  0.03407633,\n",
            "           0.00899955,  0.02372419,  0.02900696,  0.0493896 ]],\n",
            "\n",
            "        [[ 0.03069419,  0.01965932,  0.02978477,  0.02261717,\n",
            "           0.00782708,  0.02655669,  0.03547546,  0.02810523,\n",
            "           0.02711985,  0.02087435,  0.02719437,  0.0270643 ,\n",
            "           0.02538384,  0.00376703,  0.01678729,  0.04640728,\n",
            "           0.02967253,  0.01992953,  0.01619375,  0.02847573,\n",
            "           0.01907675,  0.02617019,  0.02089209,  0.02950403,\n",
            "           0.0044822 ,  0.02813999,  0.0234157 ,  0.01864149,\n",
            "           0.01082446,  0.01746293,  0.03670273,  0.03040437]]],\n",
            "\n",
            "\n",
            "       [[[ 0.02179527,  0.02353085,  0.01253272,  0.03063113,\n",
            "           0.01675271,  0.01012602,  0.03741077,  0.01803867,\n",
            "           0.03299787,  0.02063033,  0.01833322,  0.02680518,\n",
            "           0.00813697,  0.01178641,  0.00751024,  0.02483618,\n",
            "           0.02008579,  0.03977331,  0.0284966 ,  0.02540227,\n",
            "           0.02722556,  0.01873281,  0.00553854,  0.04415154,\n",
            "           0.02398619,  0.02511653,  0.03613895, -0.00107676,\n",
            "           0.02267091,  0.01768253,  0.00663421,  0.03619024]],\n",
            "\n",
            "        [[ 0.03412209,  0.05419376,  0.02584277,  0.04370415,\n",
            "           0.00616576,  0.0163239 ,  0.04813666,  0.03277582,\n",
            "           0.04060124,  0.03491943,  0.03859857,  0.04924028,\n",
            "           0.02575266,  0.01242817,  0.0308031 ,  0.01619864,\n",
            "           0.02824103,  0.01134912,  0.01798396,  0.03144389,\n",
            "           0.03952203,  0.03386394,  0.0151209 ,  0.03359675,\n",
            "           0.00412535,  0.03005141,  0.02878548,  0.02904952,\n",
            "           0.01632365,  0.03152637,  0.02161657,  0.04469049]],\n",
            "\n",
            "        [[ 0.02932788,  0.01066193,  0.0207088 ,  0.02734845,\n",
            "           0.01377348,  0.01078014,  0.03943196,  0.02122463,\n",
            "           0.03150119,  0.04345502,  0.01606583,  0.03177649,\n",
            "           0.01528419,  0.03228947,  0.01498954,  0.0288757 ,\n",
            "           0.01982708,  0.02734917,  0.02456037,  0.03029169,\n",
            "           0.04144406,  0.0058661 ,  0.02496732,  0.03936191,\n",
            "           0.01635065,  0.03014603,  0.01022783,  0.02888263,\n",
            "           0.02128987,  0.02161526,  0.03570714,  0.02330006]]],\n",
            "\n",
            "\n",
            "       [[[ 0.04580358,  0.02103001,  0.03670556,  0.03728947,\n",
            "           0.00983107,  0.00982094,  0.02531261,  0.02133322,\n",
            "           0.04458257,  0.02946623,  0.01924349,  0.02865917,\n",
            "           0.01010662,  0.02824609,  0.03098361,  0.04137108,\n",
            "           0.01199276,  0.0128887 ,  0.02286526,  0.02167   ,\n",
            "           0.03591657,  0.0198824 ,  0.01895889,  0.02213709,\n",
            "           0.03385644,  0.04221727,  0.03377451,  0.00623734,\n",
            "           0.01790401,  0.01636129, -0.00663094,  0.02456803]],\n",
            "\n",
            "        [[ 0.03044718,  0.03819089,  0.02257792,  0.04271488,\n",
            "           0.0132247 ,  0.03490835,  0.02901182,  0.01893552,\n",
            "           0.02334329,  0.03388512,  0.03617794,  0.03070121,\n",
            "           0.0180849 ,  0.02890121,  0.03392322,  0.00449499,\n",
            "           0.02055695,  0.02439613,  0.02069166,  0.04056245,\n",
            "           0.02765921,  0.01460089,  0.01516819,  0.02630469,\n",
            "           0.02813002,  0.01677008,  0.01591147,  0.01843088,\n",
            "           0.02238568,  0.02481787,  0.01357066,  0.04375408]],\n",
            "\n",
            "        [[ 0.00184909,  0.0223181 ,  0.02643706,  0.01863939,\n",
            "           0.03241798,  0.03093559,  0.03103056,  0.00592143,\n",
            "           0.01408355,  0.01562223,  0.01817946,  0.01762557,\n",
            "          -0.00359387,  0.02766722,  0.01448292,  0.01481443,\n",
            "           0.01246664,  0.01842475,  0.03262625,  0.01472648,\n",
            "           0.02833849,  0.01355199,  0.01523282,  0.01934613,\n",
            "           0.02386866,  0.00779431,  0.01492306,  0.02929507,\n",
            "           0.02398528,  0.01970656,  0.02788099,  0.04092738]]]],\n",
            "      dtype=float32), array([0.0110611 , 0.00993465, 0.00911483, 0.01042484, 0.01095482,\n",
            "       0.00888965, 0.00864569, 0.0114671 , 0.01111779, 0.01234015,\n",
            "       0.01204192, 0.01249615, 0.00710953, 0.00936308, 0.00635765,\n",
            "       0.01190947, 0.01013579, 0.01090753, 0.01155571, 0.01281004,\n",
            "       0.01077579, 0.01050876, 0.00889457, 0.00985936, 0.00821548,\n",
            "       0.00895204, 0.01141514, 0.00871108, 0.01017071, 0.0078604 ,\n",
            "       0.00944196, 0.00965781], dtype=float32), array([[ 8.8799046e-04, -3.5560442e-04,  9.6553162e-04, ...,\n",
            "        -1.1554930e-03,  3.6021187e-03,  9.3881646e-04],\n",
            "       [ 2.8886343e-03, -6.0941611e-04,  2.4563349e-03, ...,\n",
            "        -1.6516431e-03,  9.5807359e-04, -6.4984692e-04],\n",
            "       [ 8.7253365e-04, -1.4227733e-03, -2.9843084e-03, ...,\n",
            "         3.0820994e-03, -7.9432342e-05, -7.3088327e-04],\n",
            "       ...,\n",
            "       [-2.9385735e-03, -7.8576634e-04, -2.0652807e-03, ...,\n",
            "        -4.1724113e-03,  2.5440524e-03, -2.2707747e-03],\n",
            "       [-1.4154722e-03, -5.0943820e-03,  1.6159619e-03, ...,\n",
            "         1.1863241e-03,  2.1483430e-03, -2.2390448e-03],\n",
            "       [-1.6966413e-03, -8.5566519e-04, -5.8480614e-06, ...,\n",
            "         6.6960907e-05, -2.3198710e-03,  2.6306491e-03]], dtype=float32), array([0.00295058, 0.00266023, 0.00290815, 0.00244445, 0.00269006,\n",
            "       0.00298334, 0.00308309, 0.00266591, 0.00272542, 0.0031543 ,\n",
            "       0.00331729, 0.00294812, 0.00311086, 0.00265398, 0.00253249,\n",
            "       0.00295447, 0.0033586 , 0.0029076 , 0.00265741, 0.00223787,\n",
            "       0.00214426, 0.00215683, 0.00160506, 0.00237337, 0.00297972,\n",
            "       0.00271935, 0.00277078, 0.00146368, 0.00301434, 0.00249824,\n",
            "       0.00176579, 0.00292439, 0.0035227 , 0.00316778, 0.00300345,\n",
            "       0.0029528 , 0.00308061, 0.00306374, 0.0033205 , 0.00337159,\n",
            "       0.00278861, 0.00206092, 0.00284891, 0.00324779, 0.00252564,\n",
            "       0.00257969, 0.00341288, 0.00235073, 0.00361591, 0.00202147,\n",
            "       0.00223588, 0.00206248, 0.00229684, 0.00318026, 0.00254676,\n",
            "       0.00292881, 0.00360652, 0.00252997, 0.0024605 , 0.00282161,\n",
            "       0.00314295, 0.00280447, 0.00342554, 0.00333371, 0.00322107,\n",
            "       0.00315097, 0.0026903 , 0.00239384, 0.00227398, 0.00328944,\n",
            "       0.00303414, 0.00334612, 0.00216129, 0.00185375, 0.00276901,\n",
            "       0.0026448 , 0.00259495, 0.00332096, 0.00258231, 0.00302302,\n",
            "       0.00345907, 0.0045426 , 0.00289515, 0.00257791, 0.00327577,\n",
            "       0.00311166, 0.00325413, 0.00254008, 0.00249276, 0.0017837 ,\n",
            "       0.00274814, 0.00249012, 0.0037803 , 0.00285515, 0.00399509,\n",
            "       0.00241808, 0.00339773, 0.00306348, 0.00391613, 0.00276881,\n",
            "       0.00306301, 0.00257751, 0.00269179, 0.00255032, 0.00267172,\n",
            "       0.00237684, 0.00294302, 0.00341357, 0.00281185, 0.00254315,\n",
            "       0.00378323, 0.00319183, 0.00206929, 0.00264193, 0.00323692,\n",
            "       0.00333253, 0.00270719, 0.00421662, 0.00310089, 0.00342672,\n",
            "       0.00270409, 0.00363379, 0.003277  , 0.00281234, 0.00285901,\n",
            "       0.00230889, 0.00394314, 0.00227474], dtype=float32), array([[ 0.00300181,  0.0093561 ,  0.00868736, ...,  0.01154513,\n",
            "        -0.00627168,  0.02091969],\n",
            "       [ 0.00578585, -0.00011792, -0.00351165, ..., -0.00051184,\n",
            "         0.02508642,  0.00953566],\n",
            "       [-0.00076092, -0.02033313, -0.01357833, ..., -0.01029941,\n",
            "         0.01221748,  0.01192099],\n",
            "       ...,\n",
            "       [-0.00139054, -0.00932492,  0.02064117, ...,  0.01096401,\n",
            "         0.01598773,  0.00310041],\n",
            "       [ 0.00292474,  0.02103701,  0.00506729, ..., -0.02215052,\n",
            "         0.01627206,  0.01209445],\n",
            "       [ 0.01482638, -0.0083301 ,  0.00625416, ...,  0.00557302,\n",
            "         0.01674876,  0.01023862]], dtype=float32), array([-0.01168002,  0.02843595, -0.00583529, -0.00454334,  0.00344293,\n",
            "       -0.00041681, -0.00262436,  0.00833839, -0.01463623, -0.00048123],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "[array([[[[ 0.02163844,  0.03165662,  0.01536216,  0.02086407,\n",
            "           0.00331281,  0.02121367,  0.01848504,  0.01150802,\n",
            "           0.03173638,  0.02259891,  0.02047072,  0.02573678,\n",
            "           0.01738155,  0.02337671,  0.01120739,  0.01508757,\n",
            "           0.02137124,  0.0264954 ,  0.00931556,  0.01206335,\n",
            "           0.02920035,  0.01045677,  0.00095183,  0.01798654,\n",
            "           0.02254122,  0.02802641,  0.02036513,  0.03844364,\n",
            "           0.00547812,  0.01080036,  0.01550483,  0.01953718]],\n",
            "\n",
            "        [[ 0.03435625,  0.04247217,  0.01023734,  0.0182803 ,\n",
            "           0.01904002,  0.02299859,  0.04000052,  0.01513761,\n",
            "           0.02024942,  0.03281915,  0.04219521,  0.03342689,\n",
            "           0.00235677,  0.01846253,  0.02548883,  0.02547626,\n",
            "           0.02811066,  0.03476975,  0.02877311,  0.02541521,\n",
            "           0.03266086,  0.01476631,  0.03283615,  0.02409212,\n",
            "           0.01258491,  0.02351465,  0.01677969,  0.03407633,\n",
            "           0.00899955,  0.02372419,  0.02900696,  0.0493896 ]],\n",
            "\n",
            "        [[ 0.03069419,  0.01965932,  0.02978477,  0.02261717,\n",
            "           0.00782708,  0.02655669,  0.03547546,  0.02810523,\n",
            "           0.02711985,  0.02087435,  0.02719437,  0.0270643 ,\n",
            "           0.02538384,  0.00376703,  0.01678729,  0.04640728,\n",
            "           0.02967253,  0.01992953,  0.01619375,  0.02847573,\n",
            "           0.01907675,  0.02617019,  0.02089209,  0.02950403,\n",
            "           0.0044822 ,  0.02813999,  0.0234157 ,  0.01864149,\n",
            "           0.01082446,  0.01746293,  0.03670273,  0.03040437]]],\n",
            "\n",
            "\n",
            "       [[[ 0.02179527,  0.02353085,  0.01253272,  0.03063113,\n",
            "           0.01675271,  0.01012602,  0.03741077,  0.01803867,\n",
            "           0.03299787,  0.02063033,  0.01833322,  0.02680518,\n",
            "           0.00813697,  0.01178641,  0.00751024,  0.02483618,\n",
            "           0.02008579,  0.03977331,  0.0284966 ,  0.02540227,\n",
            "           0.02722556,  0.01873281,  0.00553854,  0.04415154,\n",
            "           0.02398619,  0.02511653,  0.03613895, -0.00107676,\n",
            "           0.02267091,  0.01768253,  0.00663421,  0.03619024]],\n",
            "\n",
            "        [[ 0.03412209,  0.05419376,  0.02584277,  0.04370415,\n",
            "           0.00616576,  0.0163239 ,  0.04813666,  0.03277582,\n",
            "           0.04060124,  0.03491943,  0.03859857,  0.04924028,\n",
            "           0.02575266,  0.01242817,  0.0308031 ,  0.01619864,\n",
            "           0.02824103,  0.01134912,  0.01798396,  0.03144389,\n",
            "           0.03952203,  0.03386394,  0.0151209 ,  0.03359675,\n",
            "           0.00412535,  0.03005141,  0.02878548,  0.02904952,\n",
            "           0.01632365,  0.03152637,  0.02161657,  0.04469049]],\n",
            "\n",
            "        [[ 0.02932788,  0.01066193,  0.0207088 ,  0.02734845,\n",
            "           0.01377348,  0.01078014,  0.03943196,  0.02122463,\n",
            "           0.03150119,  0.04345502,  0.01606583,  0.03177649,\n",
            "           0.01528419,  0.03228947,  0.01498954,  0.0288757 ,\n",
            "           0.01982708,  0.02734917,  0.02456037,  0.03029169,\n",
            "           0.04144406,  0.0058661 ,  0.02496732,  0.03936191,\n",
            "           0.01635065,  0.03014603,  0.01022783,  0.02888263,\n",
            "           0.02128987,  0.02161526,  0.03570714,  0.02330006]]],\n",
            "\n",
            "\n",
            "       [[[ 0.04580358,  0.02103001,  0.03670556,  0.03728947,\n",
            "           0.00983107,  0.00982094,  0.02531261,  0.02133322,\n",
            "           0.04458257,  0.02946623,  0.01924349,  0.02865917,\n",
            "           0.01010662,  0.02824609,  0.03098361,  0.04137108,\n",
            "           0.01199276,  0.0128887 ,  0.02286526,  0.02167   ,\n",
            "           0.03591657,  0.0198824 ,  0.01895889,  0.02213709,\n",
            "           0.03385644,  0.04221727,  0.03377451,  0.00623734,\n",
            "           0.01790401,  0.01636129, -0.00663094,  0.02456803]],\n",
            "\n",
            "        [[ 0.03044718,  0.03819089,  0.02257792,  0.04271488,\n",
            "           0.0132247 ,  0.03490835,  0.02901182,  0.01893552,\n",
            "           0.02334329,  0.03388512,  0.03617794,  0.03070121,\n",
            "           0.0180849 ,  0.02890121,  0.03392322,  0.00449499,\n",
            "           0.02055695,  0.02439613,  0.02069166,  0.04056245,\n",
            "           0.02765921,  0.01460089,  0.01516819,  0.02630469,\n",
            "           0.02813002,  0.01677008,  0.01591147,  0.01843088,\n",
            "           0.02238568,  0.02481787,  0.01357066,  0.04375408]],\n",
            "\n",
            "        [[ 0.00184909,  0.0223181 ,  0.02643706,  0.01863939,\n",
            "           0.03241798,  0.03093559,  0.03103056,  0.00592143,\n",
            "           0.01408355,  0.01562223,  0.01817946,  0.01762557,\n",
            "          -0.00359387,  0.02766722,  0.01448292,  0.01481443,\n",
            "           0.01246664,  0.01842475,  0.03262625,  0.01472648,\n",
            "           0.02833849,  0.01355199,  0.01523282,  0.01934613,\n",
            "           0.02386866,  0.00779431,  0.01492306,  0.02929507,\n",
            "           0.02398528,  0.01970656,  0.02788099,  0.04092738]]]],\n",
            "      dtype=float32), array([0.0110611 , 0.00993465, 0.00911483, 0.01042484, 0.01095482,\n",
            "       0.00888965, 0.00864569, 0.0114671 , 0.01111779, 0.01234015,\n",
            "       0.01204192, 0.01249615, 0.00710953, 0.00936308, 0.00635765,\n",
            "       0.01190947, 0.01013579, 0.01090753, 0.01155571, 0.01281004,\n",
            "       0.01077579, 0.01050876, 0.00889457, 0.00985936, 0.00821548,\n",
            "       0.00895204, 0.01141514, 0.00871108, 0.01017071, 0.0078604 ,\n",
            "       0.00944196, 0.00965781], dtype=float32), array([[ 8.8799046e-04, -3.5560442e-04,  9.6553162e-04, ...,\n",
            "        -1.1554930e-03,  3.6021187e-03,  9.3881646e-04],\n",
            "       [ 2.8886343e-03, -6.0941611e-04,  2.4563349e-03, ...,\n",
            "        -1.6516431e-03,  9.5807359e-04, -6.4984692e-04],\n",
            "       [ 8.7253365e-04, -1.4227733e-03, -2.9843084e-03, ...,\n",
            "         3.0820994e-03, -7.9432342e-05, -7.3088327e-04],\n",
            "       ...,\n",
            "       [-2.9385735e-03, -7.8576634e-04, -2.0652807e-03, ...,\n",
            "        -4.1724113e-03,  2.5440524e-03, -2.2707747e-03],\n",
            "       [-1.4154722e-03, -5.0943820e-03,  1.6159619e-03, ...,\n",
            "         1.1863241e-03,  2.1483430e-03, -2.2390448e-03],\n",
            "       [-1.6966413e-03, -8.5566519e-04, -5.8480614e-06, ...,\n",
            "         6.6960907e-05, -2.3198710e-03,  2.6306491e-03]], dtype=float32), array([0.00295058, 0.00266023, 0.00290815, 0.00244445, 0.00269006,\n",
            "       0.00298334, 0.00308309, 0.00266591, 0.00272542, 0.0031543 ,\n",
            "       0.00331729, 0.00294812, 0.00311086, 0.00265398, 0.00253249,\n",
            "       0.00295447, 0.0033586 , 0.0029076 , 0.00265741, 0.00223787,\n",
            "       0.00214426, 0.00215683, 0.00160506, 0.00237337, 0.00297972,\n",
            "       0.00271935, 0.00277078, 0.00146368, 0.00301434, 0.00249824,\n",
            "       0.00176579, 0.00292439, 0.0035227 , 0.00316778, 0.00300345,\n",
            "       0.0029528 , 0.00308061, 0.00306374, 0.0033205 , 0.00337159,\n",
            "       0.00278861, 0.00206092, 0.00284891, 0.00324779, 0.00252564,\n",
            "       0.00257969, 0.00341288, 0.00235073, 0.00361591, 0.00202147,\n",
            "       0.00223588, 0.00206248, 0.00229684, 0.00318026, 0.00254676,\n",
            "       0.00292881, 0.00360652, 0.00252997, 0.0024605 , 0.00282161,\n",
            "       0.00314295, 0.00280447, 0.00342554, 0.00333371, 0.00322107,\n",
            "       0.00315097, 0.0026903 , 0.00239384, 0.00227398, 0.00328944,\n",
            "       0.00303414, 0.00334612, 0.00216129, 0.00185375, 0.00276901,\n",
            "       0.0026448 , 0.00259495, 0.00332096, 0.00258231, 0.00302302,\n",
            "       0.00345907, 0.0045426 , 0.00289515, 0.00257791, 0.00327577,\n",
            "       0.00311166, 0.00325413, 0.00254008, 0.00249276, 0.0017837 ,\n",
            "       0.00274814, 0.00249012, 0.0037803 , 0.00285515, 0.00399509,\n",
            "       0.00241808, 0.00339773, 0.00306348, 0.00391613, 0.00276881,\n",
            "       0.00306301, 0.00257751, 0.00269179, 0.00255032, 0.00267172,\n",
            "       0.00237684, 0.00294302, 0.00341357, 0.00281185, 0.00254315,\n",
            "       0.00378323, 0.00319183, 0.00206929, 0.00264193, 0.00323692,\n",
            "       0.00333253, 0.00270719, 0.00421662, 0.00310089, 0.00342672,\n",
            "       0.00270409, 0.00363379, 0.003277  , 0.00281234, 0.00285901,\n",
            "       0.00230889, 0.00394314, 0.00227474], dtype=float32), array([[ 0.00300181,  0.0093561 ,  0.00868736, ...,  0.01154513,\n",
            "        -0.00627168,  0.02091969],\n",
            "       [ 0.00578585, -0.00011792, -0.00351165, ..., -0.00051184,\n",
            "         0.02508642,  0.00953566],\n",
            "       [-0.00076092, -0.02033313, -0.01357833, ..., -0.01029941,\n",
            "         0.01221748,  0.01192099],\n",
            "       ...,\n",
            "       [-0.00139054, -0.00932492,  0.02064117, ...,  0.01096401,\n",
            "         0.01598773,  0.00310041],\n",
            "       [ 0.00292474,  0.02103701,  0.00506729, ..., -0.02215052,\n",
            "         0.01627206,  0.01209445],\n",
            "       [ 0.01482638, -0.0083301 ,  0.00625416, ...,  0.00557302,\n",
            "         0.01674876,  0.01023862]], dtype=float32), array([-0.01168002,  0.02843595, -0.00583529, -0.00454334,  0.00344293,\n",
            "       -0.00041681, -0.00262436,  0.00833839, -0.01463623, -0.00048123],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2594 - loss: 2.2967\n",
            "Global Model Accuracy: 0.2670\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Extract the Best Pareto Front\n",
        "pareto_front = res.F   # Objective values of solutions in Pareto front\n",
        "pareto_solutions = res.X  # Corresponding bitstrings\n",
        "\n",
        "# Print the Best Pareto Front Solutions\n",
        "print(\"Best Pareto Front (Bitstrings):\")\n",
        "for bitstring in pareto_solutions:\n",
        "    print(\"\".join(map(str, bitstring)).replace('True','1').replace('False','0'))\n",
        "\n",
        "bitstring = pareto_solutions[0] # for now!\n",
        "bitstring = str(bitstring).replace('False','0').replace('True','1')\n",
        "for char in bitstring:\n",
        "    if char != '0' and char != '1':\n",
        "        bitstring = bitstring.replace(char,'')\n",
        "\n",
        "print(len(bitstring))\n",
        "print(bitstring)\n",
        "temp_bitstring = []\n",
        "for bit in bitstring:\n",
        "    temp_bitstring.append(bit)\n",
        "bitstring = temp_bitstring\n",
        "\n",
        "########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
        "give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "fit_bitstring_devices(bitstring)\n",
        "\n",
        "# new\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(\"------------------------------------------------------------\")\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q6xCkaJlHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0']\n",
            "100\n",
            "39\n"
          ]
        }
      ],
      "source": [
        "print(bitstring) # for now!\n",
        "print(len(bitstring))\n",
        "print(bitstring.count(\"1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0DernaKkHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      5 |             - |             -\n",
            "     2 |      100 |      6 |  0.0129361668 |         ideal\n",
            "     3 |      150 |      5 |  0.0786670050 |         ideal\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(awiodawo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FiQqM7-0HqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 4.0: Loss before: [2.2980031967163086, 0.14666666090488434], Loss after: [2.284519910812378, 0.35333332419395447]\n",
            "Device 5.0: Loss before: [2.298715114593506, 0.11999999731779099], Loss after: [2.2860357761383057, 0.34333333373069763]\n",
            "Device 6.0: Loss before: [2.298321008682251, 0.14166666567325592], Loss after: [2.2865428924560547, 0.4116666615009308]\n",
            "Device 7.0: Loss before: [2.298210859298706, 0.13833333551883698], Loss after: [2.282723903656006, 0.22833333909511566]\n",
            "Device 10.0: Loss before: [2.2980704307556152, 0.1616666615009308], Loss after: [2.2842841148376465, 0.2266666740179062]\n",
            "Device 11.0: Loss before: [2.2986857891082764, 0.13333334028720856], Loss after: [2.285165786743164, 0.22333332896232605]\n",
            "Device 13.0: Loss before: [2.2982802391052246, 0.14499999582767487], Loss after: [2.2767813205718994, 0.1550000011920929]\n",
            "Device 26.0: Loss before: [2.2987937927246094, 0.12166666984558105], Loss after: [2.286884307861328, 0.3266666531562805]\n",
            "Device 31.0: Loss before: [2.298445224761963, 0.1366666704416275], Loss after: [2.286431312561035, 0.3216666579246521]\n",
            "Device 33.0: Loss before: [2.2983906269073486, 0.1483333259820938], Loss after: [2.286226511001587, 0.29499998688697815]\n",
            "Device 36.0: Loss before: [2.298095464706421, 0.14499999582767487], Loss after: [2.2831168174743652, 0.12999999523162842]\n",
            "Device 42.0: Loss before: [2.2977967262268066, 0.15666666626930237], Loss after: [2.2825639247894287, 0.20333333313465118]\n",
            "Device 43.0: Loss before: [2.2984626293182373, 0.12833333015441895], Loss after: [2.2870965003967285, 0.4183333218097687]\n",
            "Device 44.0: Loss before: [2.298483371734619, 0.125], Loss after: [2.2861244678497314, 0.2266666740179062]\n",
            "Device 46.0: Loss before: [2.297833204269409, 0.14000000059604645], Loss after: [2.283404588699341, 0.2750000059604645]\n",
            "Device 47.0: Loss before: [2.2982921600341797, 0.14166666567325592], Loss after: [2.284034490585327, 0.3933333456516266]\n",
            "Device 50.0: Loss before: [2.298464059829712, 0.1366666704416275], Loss after: [2.287050247192383, 0.22833333909511566]\n",
            "Device 51.0: Loss before: [2.2985193729400635, 0.11666666716337204], Loss after: [2.2863316535949707, 0.2316666692495346]\n",
            "Device 52.0: Loss before: [2.2987890243530273, 0.125], Loss after: [2.291125535964966, 0.23499999940395355]\n",
            "Device 53.0: Loss before: [2.298614978790283, 0.13833333551883698], Loss after: [2.288135290145874, 0.2266666740179062]\n",
            "Device 54.0: Loss before: [2.29848313331604, 0.12666666507720947], Loss after: [2.287320852279663, 0.24833333492279053]\n",
            "Device 55.0: Loss before: [2.2981367111206055, 0.14499999582767487], Loss after: [2.2849795818328857, 0.24166665971279144]\n",
            "Device 58.0: Loss before: [2.298809766769409, 0.14499999582767487], Loss after: [2.287022113800049, 0.2866666615009308]\n",
            "Device 61.0: Loss before: [2.2984554767608643, 0.14499999582767487], Loss after: [2.2861742973327637, 0.15666666626930237]\n",
            "Device 62.0: Loss before: [2.2990224361419678, 0.11500000208616257], Loss after: [2.2846298217773438, 0.14666666090488434]\n",
            "Device 64.0: Loss before: [2.297769069671631, 0.16333332657814026], Loss after: [2.283010959625244, 0.20333333313465118]\n",
            "Device 68.0: Loss before: [2.298501968383789, 0.125], Loss after: [2.283834218978882, 0.13833333551883698]\n",
            "Device 71.0: Loss before: [2.2984633445739746, 0.14000000059604645], Loss after: [2.2856762409210205, 0.2933333218097687]\n",
            "Device 72.0: Loss before: [2.2984213829040527, 0.13500000536441803], Loss after: [2.2863972187042236, 0.4116666615009308]\n",
            "Device 75.0: Loss before: [2.2984416484832764, 0.125], Loss after: [2.2861156463623047, 0.3316666781902313]\n",
            "Device 77.0: Loss before: [2.298542022705078, 0.1483333259820938], Loss after: [2.285374879837036, 0.26499998569488525]\n",
            "Device 81.0: Loss before: [2.2985634803771973, 0.125], Loss after: [2.2850818634033203, 0.3283333480358124]\n",
            "Device 84.0: Loss before: [2.298508405685425, 0.12999999523162842], Loss after: [2.2876505851745605, 0.25]\n",
            "Device 86.0: Loss before: [2.2986748218536377, 0.11833333224058151], Loss after: [2.2889621257781982, 0.34833332896232605]\n",
            "Device 88.0: Loss before: [2.2982442378997803, 0.13333334028720856], Loss after: [2.2840864658355713, 0.3383333384990692]\n",
            "Device 89.0: Loss before: [2.2988176345825195, 0.11166666448116302], Loss after: [2.286825656890869, 0.2199999988079071]\n",
            "Device 91.0: Loss before: [2.298393726348877, 0.1133333370089531], Loss after: [2.2810146808624268, 0.14000000059604645]\n",
            "Device 94.0: Loss before: [2.298144578933716, 0.1366666704416275], Loss after: [2.283414363861084, 0.19166666269302368]\n",
            "Device 98.0: Loss before: [2.298098564147949, 0.14499999582767487], Loss after: [2.283278226852417, 0.4350000023841858]\n",
            "[array([[[[ 0.01560311,  0.02257121,  0.01110409,  0.01507172,\n",
            "           0.00252358,  0.01510464,  0.01344061,  0.00847014,\n",
            "           0.02270118,  0.01620024,  0.01468922,  0.01839421,\n",
            "           0.0124475 ,  0.01659005,  0.00809593,  0.01099574,\n",
            "           0.01550036,  0.018876  ,  0.00679025,  0.00889408,\n",
            "           0.02077803,  0.00761622,  0.00098165,  0.01305017,\n",
            "           0.01599254,  0.02000332,  0.01462005,  0.02717552,\n",
            "           0.00411549,  0.00790651,  0.0111158 ,  0.01423351]],\n",
            "\n",
            "        [[ 0.02454352,  0.03025914,  0.00766521,  0.01335663,\n",
            "           0.01355273,  0.01646741,  0.02856857,  0.01110631,\n",
            "           0.01486227,  0.02343186,  0.02989276,  0.02386882,\n",
            "           0.00201672,  0.01325708,  0.01809904,  0.01838938,\n",
            "           0.02030334,  0.02471407,  0.02045402,  0.01833379,\n",
            "           0.02333464,  0.01066581,  0.02327173,  0.01741481,\n",
            "           0.00916306,  0.01697847,  0.01220456,  0.02428175,\n",
            "           0.00664643,  0.01701493,  0.02061685,  0.0351745 ]],\n",
            "\n",
            "        [[ 0.02193822,  0.01435862,  0.02124896,  0.01630747,\n",
            "           0.00577648,  0.01895138,  0.02538386,  0.02006774,\n",
            "           0.01959194,  0.0150976 ,  0.01944805,  0.0194125 ,\n",
            "           0.01799021,  0.00302283,  0.01204218,  0.03292189,\n",
            "           0.02132382,  0.01433191,  0.01168544,  0.0204161 ,\n",
            "           0.01386432,  0.01853496,  0.01494559,  0.02114179,\n",
            "           0.00350407,  0.02010733,  0.01673094,  0.0135516 ,\n",
            "           0.00789044,  0.01261784,  0.02595696,  0.021892  ]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01583286,  0.01706405,  0.00928797,  0.02194787,\n",
            "           0.01194718,  0.00750862,  0.0267721 ,  0.01310973,\n",
            "           0.02374508,  0.01495223,  0.01332487,  0.01925408,\n",
            "           0.00602902,  0.00864317,  0.0056372 ,  0.01789839,\n",
            "           0.01463715,  0.02821155,  0.02027787,  0.01830474,\n",
            "           0.01953365,  0.01347347,  0.00425999,  0.03135091,\n",
            "           0.01717493,  0.01812383,  0.02568616, -0.00023046,\n",
            "           0.01616774,  0.01278134,  0.00497106,  0.02595186]],\n",
            "\n",
            "        [[ 0.02445064,  0.03852054,  0.018642  ,  0.0310827 ,\n",
            "           0.00468171,  0.01192496,  0.03432898,  0.02344587,\n",
            "           0.02912918,  0.02498843,  0.02749189,  0.03494409,\n",
            "           0.01831012,  0.00914495,  0.02187797,  0.01200425,\n",
            "           0.02039989,  0.0085194 ,  0.01306869,  0.0226127 ,\n",
            "           0.02819599,  0.02399428,  0.01103092,  0.02411702,\n",
            "           0.00344842,  0.02161986,  0.02060069,  0.02084474,\n",
            "           0.01183216,  0.02250262,  0.01550617,  0.03198152]],\n",
            "\n",
            "        [[ 0.02100236,  0.00818088,  0.01502692,  0.01960614,\n",
            "           0.00996529,  0.00806254,  0.02817566,  0.01533024,\n",
            "           0.02270831,  0.03086884,  0.01177241,  0.02273101,\n",
            "           0.0109973 ,  0.02290827,  0.01083251,  0.0207416 ,\n",
            "           0.014455  ,  0.01956153,  0.01758006,  0.02170456,\n",
            "           0.0294641 ,  0.00442351,  0.01783276,  0.02806052,\n",
            "           0.0118739 ,  0.02154494,  0.00755924,  0.02070526,\n",
            "           0.01523361,  0.01553781,  0.02527748,  0.01697074]]],\n",
            "\n",
            "\n",
            "       [[[ 0.0324344 ,  0.015249  ,  0.02606078,  0.02648943,\n",
            "           0.00713766,  0.00728631,  0.01831263,  0.01531798,\n",
            "           0.03172095,  0.0210232 ,  0.01390663,  0.02049077,\n",
            "           0.00733794,  0.02008646,  0.02194182,  0.02932457,\n",
            "           0.00885229,  0.00942696,  0.01636209,  0.01565125,\n",
            "           0.0255361 ,  0.01422815,  0.01359377,  0.01596601,\n",
            "           0.02408645,  0.02995463,  0.02401149,  0.00478586,\n",
            "           0.0128755 ,  0.01185623, -0.00434332,  0.01779371]],\n",
            "\n",
            "        [[ 0.02174696,  0.02728774,  0.0163013 ,  0.03028866,\n",
            "           0.00957214,  0.02481695,  0.02091229,  0.01369449,\n",
            "           0.0169973 ,  0.02415604,  0.02573123,  0.02196436,\n",
            "           0.01292184,  0.0205738 ,  0.02399925,  0.00373072,\n",
            "           0.01486036,  0.01748678,  0.01490798,  0.02883988,\n",
            "           0.01987246,  0.0105205 ,  0.011029  ,  0.01893784,\n",
            "           0.0201656 ,  0.0122781 ,  0.01156722,  0.01336054,\n",
            "           0.01603947,  0.01780558,  0.00980261,  0.03119609]],\n",
            "\n",
            "        [[ 0.00169485,  0.01614244,  0.01890111,  0.01342804,\n",
            "           0.02288543,  0.0220295 ,  0.02215771,  0.00453429,\n",
            "           0.0104293 ,  0.01137705,  0.01312875,  0.01275303,\n",
            "          -0.0021973 ,  0.01964887,  0.01038801,  0.01078565,\n",
            "           0.00911237,  0.01325769,  0.0231106 ,  0.01071291,\n",
            "           0.02024084,  0.00966646,  0.01100577,  0.01399866,\n",
            "           0.01709351,  0.00586824,  0.0107098 ,  0.02085855,\n",
            "           0.01707222,  0.01415403,  0.01973128,  0.0290544 ]]]],\n",
            "      dtype=float32), array([0.00771213, 0.00681153, 0.00631751, 0.00721437, 0.0075969 ,\n",
            "       0.0061396 , 0.00598041, 0.00799789, 0.00771072, 0.00854967,\n",
            "       0.00835016, 0.00864327, 0.00493658, 0.00647637, 0.00440599,\n",
            "       0.00825316, 0.00704589, 0.00756239, 0.00800533, 0.00886914,\n",
            "       0.00747405, 0.00731832, 0.00616572, 0.00680429, 0.00570102,\n",
            "       0.00622225, 0.00792158, 0.00602227, 0.00704377, 0.00543422,\n",
            "       0.00656562, 0.00667889], dtype=float32), array([[ 6.1755313e-04, -2.4664932e-04,  6.7078852e-04, ...,\n",
            "        -8.0360885e-04,  2.5043902e-03,  6.5234961e-04],\n",
            "       [ 2.0079555e-03, -4.2310532e-04,  1.7069298e-03, ...,\n",
            "        -1.1483760e-03,  6.6667481e-04, -4.5175949e-04],\n",
            "       [ 6.0673966e-04, -9.8841707e-04, -2.0743026e-03, ...,\n",
            "         2.1416135e-03, -5.4449702e-05, -5.0806307e-04],\n",
            "       ...,\n",
            "       [-2.0420072e-03, -5.4576492e-04, -1.4355610e-03, ...,\n",
            "        -2.9001830e-03,  1.7687751e-03, -1.5782582e-03],\n",
            "       [-9.8339305e-04, -3.5401853e-03,  1.1228601e-03, ...,\n",
            "         8.2406885e-04,  1.4938946e-03, -1.5562265e-03],\n",
            "       [-1.1787841e-03, -5.9427781e-04, -4.3058144e-06, ...,\n",
            "         4.6118908e-05, -1.6114977e-03,  1.8282176e-03]], dtype=float32), array([0.00208667, 0.00189387, 0.00199729, 0.00154911, 0.00176541,\n",
            "       0.00206093, 0.00210721, 0.00184068, 0.00180145, 0.00228131,\n",
            "       0.00232149, 0.00193657, 0.00205213, 0.00194205, 0.00170934,\n",
            "       0.00203107, 0.00255653, 0.00209281, 0.00172378, 0.00140728,\n",
            "       0.00139628, 0.00142322, 0.00107531, 0.00155734, 0.00208906,\n",
            "       0.00194707, 0.0019741 , 0.00080239, 0.00225596, 0.00167879,\n",
            "       0.00110195, 0.00208302, 0.00242299, 0.00226593, 0.00213515,\n",
            "       0.00210372, 0.00218468, 0.00213812, 0.00230887, 0.0023536 ,\n",
            "       0.00205644, 0.00135849, 0.00198219, 0.00222345, 0.00185209,\n",
            "       0.00178986, 0.00245615, 0.00157912, 0.00251533, 0.00143394,\n",
            "       0.00166692, 0.00145071, 0.00156005, 0.00219932, 0.00183136,\n",
            "       0.00198621, 0.00259816, 0.00179712, 0.00165607, 0.00188844,\n",
            "       0.00211427, 0.00188984, 0.00236808, 0.00228475, 0.00222471,\n",
            "       0.00214465, 0.0018378 , 0.00170703, 0.00159359, 0.00233416,\n",
            "       0.00213527, 0.00218075, 0.00142351, 0.00123618, 0.00193511,\n",
            "       0.00188489, 0.00185067, 0.00230161, 0.00176024, 0.00205667,\n",
            "       0.00251559, 0.00318715, 0.00205174, 0.00178891, 0.00225481,\n",
            "       0.00219605, 0.00222357, 0.00166145, 0.0017003 , 0.00114618,\n",
            "       0.00203703, 0.00166267, 0.00272077, 0.00205865, 0.0027835 ,\n",
            "       0.00170427, 0.00261458, 0.00222372, 0.00276402, 0.00197034,\n",
            "       0.00208029, 0.00165495, 0.0019255 , 0.00175673, 0.00191493,\n",
            "       0.00154906, 0.00210837, 0.00250689, 0.00197622, 0.00173402,\n",
            "       0.00255804, 0.00226427, 0.00136213, 0.00181246, 0.00224639,\n",
            "       0.00226412, 0.00194216, 0.00293876, 0.00218041, 0.0023717 ,\n",
            "       0.00189303, 0.00259776, 0.00223523, 0.00201245, 0.00198338,\n",
            "       0.00155573, 0.00282332, 0.00157042], dtype=float32), array([[ 2.7212135e-03,  6.0715782e-03,  6.3353442e-03, ...,\n",
            "         7.5136763e-03, -4.0855831e-03,  1.4342298e-02],\n",
            "       [ 4.1163838e-03, -1.4130400e-04, -2.2077980e-03, ...,\n",
            "        -4.9452571e-04,  1.8071074e-02,  6.4545963e-03],\n",
            "       [-4.8383321e-05, -1.5000661e-02, -9.2021246e-03, ...,\n",
            "        -7.4389344e-03,  8.8850204e-03,  8.3303526e-03],\n",
            "       ...,\n",
            "       [-5.6947261e-04, -7.5646313e-03,  1.4763591e-02, ...,\n",
            "         7.4852011e-03,  1.1638147e-02,  2.2772280e-03],\n",
            "       [ 2.5996622e-03,  1.4541779e-02,  3.6641737e-03, ...,\n",
            "        -1.5829407e-02,  1.2036985e-02,  8.1936205e-03],\n",
            "       [ 1.1044403e-02, -6.6315653e-03,  4.2182137e-03, ...,\n",
            "         4.1996576e-03,  1.2093508e-02,  7.1917675e-03]], dtype=float32), array([-0.00883807,  0.02402571, -0.00454409, -0.00271043,  0.00178446,\n",
            "       -0.00320016, -0.00219019,  0.00676279, -0.01052285, -0.00056717],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "[array([[[[ 0.01853657,  0.0255899 ,  0.01325048,  0.01794863,\n",
            "           0.00372288,  0.0170061 ,  0.01661289,  0.01069272,\n",
            "           0.02590805,  0.01887829,  0.01721864,  0.02123614,\n",
            "           0.01414526,  0.018442  ,  0.00980237,  0.01353807,\n",
            "           0.01833743,  0.02134998,  0.00860709,  0.01143229,\n",
            "           0.02350541,  0.00942591,  0.00255571,  0.01588496,\n",
            "           0.01780889,  0.02272947,  0.01703819,  0.0295219 ,\n",
            "           0.00567911,  0.00992756,  0.01292842,  0.01759894]],\n",
            "\n",
            "        [[ 0.02806525,  0.0340521 ,  0.01042626,  0.01677048,\n",
            "           0.01522873,  0.01894756,  0.03265935,  0.01388947,\n",
            "           0.01874037,  0.02680896,  0.03302965,  0.02741195,\n",
            "           0.00390089,  0.01554412,  0.02027846,  0.02171273,\n",
            "           0.02374921,  0.02766285,  0.02292923,  0.0216443 ,\n",
            "           0.02682882,  0.01285453,  0.02550878,  0.02092806,\n",
            "           0.01137699,  0.02025804,  0.01501914,  0.02727219,\n",
            "           0.00864736,  0.01966366,  0.02307271,  0.03950934]],\n",
            "\n",
            "        [[ 0.02513651,  0.01783038,  0.02396577,  0.01943383,\n",
            "           0.00749725,  0.02141257,  0.0292639 ,  0.02265292,\n",
            "           0.02317629,  0.01827555,  0.02240883,  0.02271207,\n",
            "           0.01975618,  0.00517385,  0.0141285 ,  0.03614635,\n",
            "           0.02452176,  0.01694963,  0.01401895,  0.02353514,\n",
            "           0.01713959,  0.02049664,  0.01712154,  0.02447153,\n",
            "           0.00551249,  0.02300004,  0.01915327,  0.01644222,\n",
            "           0.00979655,  0.01508781,  0.02842446,  0.02584581]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01940412,  0.02075067,  0.01213542,  0.02545523,\n",
            "           0.01356196,  0.00990428,  0.03080706,  0.01583898,\n",
            "           0.02775078,  0.01825983,  0.01645891,  0.02274335,\n",
            "           0.00786063,  0.01103601,  0.00791961,  0.02103575,\n",
            "           0.01774827,  0.03119366,  0.0228071 ,  0.02154345,\n",
            "           0.0229629 ,  0.01579341,  0.00628144,  0.03483718,\n",
            "           0.01972052,  0.02150875,  0.02864049,  0.00240234,\n",
            "           0.01827069,  0.01529946,  0.00697541,  0.03011682]],\n",
            "\n",
            "        [[ 0.02834689,  0.04279963,  0.02198792,  0.03494903,\n",
            "           0.00671215,  0.01482777,  0.03894792,  0.02659522,\n",
            "           0.0335697 ,  0.02884293,  0.03106824,  0.03893242,\n",
            "           0.02036047,  0.01187469,  0.02449614,  0.01561989,\n",
            "           0.02396379,  0.0118091 ,  0.0160464 ,  0.02638634,\n",
            "           0.03219614,  0.02646436,  0.01358275,  0.02811726,\n",
            "           0.00630124,  0.02533112,  0.02371197,  0.02408264,\n",
            "           0.01430692,  0.02550556,  0.0181355 ,  0.03677465]],\n",
            "\n",
            "        [[ 0.0242896 ,  0.01191207,  0.01809579,  0.02292836,\n",
            "           0.01194662,  0.01082233,  0.03228762,  0.01807938,\n",
            "           0.02658   ,  0.03438237,  0.01495643,  0.02624138,\n",
            "           0.01283382,  0.02540681,  0.01314146,  0.02398441,\n",
            "           0.01759083,  0.02242739,  0.0202575 ,  0.0250039 ,\n",
            "           0.03304265,  0.00640731,  0.02021794,  0.0316869 ,\n",
            "           0.01434296,  0.02460437,  0.01000277,  0.02375208,\n",
            "           0.01747752,  0.01820194,  0.02782734,  0.02109626]]],\n",
            "\n",
            "\n",
            "       [[[ 0.0356867 ,  0.0185859 ,  0.02883152,  0.02972068,\n",
            "           0.00873862,  0.00959581,  0.02204921,  0.01775191,\n",
            "           0.03541991,  0.02405087,  0.01679223,  0.02370545,\n",
            "           0.00891395,  0.02249303,  0.02420833,  0.0321755 ,\n",
            "           0.01139511,  0.0119675 ,  0.01883093,  0.01866261,\n",
            "           0.02878221,  0.01636475,  0.01560853,  0.01908396,\n",
            "           0.02675719,  0.03308934,  0.02677112,  0.00707978,\n",
            "           0.01502777,  0.01430301, -0.00273201,  0.02162039]],\n",
            "\n",
            "        [[ 0.02499968,  0.03102347,  0.01934377,  0.03371151,\n",
            "           0.01150476,  0.02754932,  0.02490355,  0.01631334,\n",
            "           0.02082849,  0.02749114,  0.02888351,  0.02543498,\n",
            "           0.01465236,  0.02319067,  0.02640093,  0.00672082,\n",
            "           0.01766387,  0.02027829,  0.01764144,  0.03214663,\n",
            "           0.02345015,  0.01260883,  0.01336302,  0.02239945,\n",
            "           0.02304409,  0.01545657,  0.01425051,  0.01609392,\n",
            "           0.01844178,  0.02058656,  0.01192927,  0.03539412]],\n",
            "\n",
            "        [[ 0.00403864,  0.01918044,  0.02147606,  0.01612904,\n",
            "           0.02467168,  0.02451272,  0.02538406,  0.00658017,\n",
            "           0.01347701,  0.01414546,  0.01570525,  0.01550094,\n",
            "          -0.00079654,  0.02188141,  0.01226082,  0.01318491,\n",
            "           0.01133393,  0.01556333,  0.02539704,  0.01324551,\n",
            "           0.02318135,  0.01117328,  0.01302498,  0.01686782,\n",
            "           0.01942463,  0.00820328,  0.0125973 ,  0.02328101,\n",
            "           0.01910487,  0.01645717,  0.02173702,  0.03239977]]]],\n",
            "      dtype=float32), array([0.00791286, 0.00664543, 0.00634012, 0.00722591, 0.00768217,\n",
            "       0.00607533, 0.00600228, 0.00820303, 0.00777486, 0.00858091,\n",
            "       0.00843693, 0.00860894, 0.00495559, 0.00644935, 0.00439579,\n",
            "       0.00824613, 0.00714699, 0.00764531, 0.00800015, 0.00885637,\n",
            "       0.00748831, 0.00748187, 0.00614393, 0.00673403, 0.00580411,\n",
            "       0.00628124, 0.0079714 , 0.00603828, 0.00706088, 0.00539738,\n",
            "       0.0066829 , 0.00670133], dtype=float32), array([[ 6.1856786e-04, -2.4534186e-04,  6.6978426e-04, ...,\n",
            "        -8.0483733e-04,  2.5066894e-03,  6.5216440e-04],\n",
            "       [ 2.0088295e-03, -4.2197676e-04,  1.7060770e-03, ...,\n",
            "        -1.1494471e-03,  6.6866004e-04, -4.5192245e-04],\n",
            "       [ 6.0756796e-04, -9.8734244e-04, -2.0751080e-03, ...,\n",
            "         2.1406098e-03, -5.2572748e-05, -5.0819246e-04],\n",
            "       ...,\n",
            "       [-2.0412509e-03, -5.4487697e-04, -1.4362830e-03, ...,\n",
            "        -2.9009562e-03,  1.7703994e-03, -1.5783275e-03],\n",
            "       [-9.8248001e-04, -3.5391094e-03,  1.1219664e-03, ...,\n",
            "         8.2312821e-04,  1.4958800e-03, -1.5563300e-03],\n",
            "       [-1.1778235e-03, -5.9321790e-04, -5.2123910e-06, ...,\n",
            "         4.5230514e-05, -1.6094951e-03,  1.8281366e-03]], dtype=float32), array([0.00221744, 0.0020619 , 0.00186364, 0.00101891, 0.00139873,\n",
            "       0.00202919, 0.00202261, 0.0022134 , 0.00150688, 0.00259773,\n",
            "       0.00239793, 0.00177225, 0.00164881, 0.0026467 , 0.00152347,\n",
            "       0.00197099, 0.00413277, 0.00232837, 0.00147915, 0.00080917,\n",
            "       0.00107103, 0.0011233 , 0.00094483, 0.0012241 , 0.00234105,\n",
            "       0.00219342, 0.00216899, 0.00046742, 0.00283221, 0.00146803,\n",
            "       0.00064305, 0.00226656, 0.00236333, 0.00261148, 0.00245453,\n",
            "       0.00232736, 0.00230149, 0.00218355, 0.00254268, 0.00240096,\n",
            "       0.00304351, 0.00110904, 0.00204078, 0.00219072, 0.00281837,\n",
            "       0.00190702, 0.00280661, 0.0013865 , 0.00256553, 0.00153939,\n",
            "       0.0022145 , 0.0014683 , 0.00151678, 0.00221632, 0.00206562,\n",
            "       0.00182502, 0.00293344, 0.00191361, 0.0014366 , 0.00165379,\n",
            "       0.0018765 , 0.00184492, 0.00234792, 0.00213479, 0.00217167,\n",
            "       0.00198494, 0.00190697, 0.00185022, 0.00160692, 0.00257179,\n",
            "       0.00225655, 0.00175707, 0.00113932, 0.00102609, 0.002165  ,\n",
            "       0.00219698, 0.00200279, 0.0023175 , 0.00165136, 0.00196189,\n",
            "       0.00337929, 0.00327299, 0.00219381, 0.00179419, 0.00265797,\n",
            "       0.00236423, 0.00206318, 0.00129626, 0.00155956, 0.00075998,\n",
            "       0.00283353, 0.00137045, 0.00306412, 0.00249581, 0.00282707,\n",
            "       0.00179229, 0.00462045, 0.0025543 , 0.00335422, 0.00207876,\n",
            "       0.00194944, 0.00114239, 0.00218997, 0.00175528, 0.00225806,\n",
            "       0.00121856, 0.00244316, 0.00325789, 0.00202544, 0.00162824,\n",
            "       0.00232493, 0.00244552, 0.00106317, 0.00173956, 0.0022284 ,\n",
            "       0.0020846 , 0.00214001, 0.00309466, 0.00229742, 0.00238161,\n",
            "       0.00216526, 0.00279532, 0.00214244, 0.0022297 , 0.00195882,\n",
            "       0.00139459, 0.00311958, 0.00154858], dtype=float32), array([[ 0.00405737,  0.00499256,  0.00716525, ...,  0.00652536,\n",
            "        -0.00363511,  0.01416043],\n",
            "       [ 0.00463203, -0.00036774, -0.00145562, ..., -0.00087343,\n",
            "         0.02038709,  0.00579013],\n",
            "       [ 0.00201285, -0.01893913, -0.00842966, ..., -0.0085602 ,\n",
            "         0.0100541 ,  0.00867455],\n",
            "       ...,\n",
            "       [ 0.00040482, -0.01158772,  0.01632705, ...,  0.00729322,\n",
            "         0.01320907,  0.00282498],\n",
            "       [ 0.00353433,  0.01540238,  0.00466592, ..., -0.01783254,\n",
            "         0.01437582,  0.00734646],\n",
            "       [ 0.01462393, -0.00985311,  0.00397546, ...,  0.00544713,\n",
            "         0.01380046,  0.00708707]], dtype=float32), array([-0.01321435,  0.04472741, -0.00518561, -0.00074014, -0.0018087 ,\n",
            "       -0.01687076, -0.0046033 ,  0.01223934, -0.01256207, -0.00198182],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "global model stayed the same?\n",
            "False\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4120 - loss: 2.2855\n",
            "Global Model Accuracy: 0.4080\n"
          ]
        }
      ],
      "source": [
        "########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ay0ud45DHqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      4 |             - |             -\n",
            "     2 |      100 |      6 |  0.2727559400 |         ideal\n",
            "     3 |      150 |      9 |  0.0710064480 |         ideal\n",
            "Device 4.0: Loss before: [2.299807071685791, 0.12333333492279053], Loss after: [2.2950029373168945, 0.12333333492279053]\n",
            "Device 5.0: Loss before: [2.300266981124878, 0.10833333432674408], Loss after: [2.295975685119629, 0.20333333313465118]\n",
            "Device 6.0: Loss before: [2.3000829219818115, 0.1133333370089531], Loss after: [2.296647548675537, 0.1133333370089531]\n",
            "Device 7.0: Loss before: [2.299997329711914, 0.10333333164453506], Loss after: [2.294761896133423, 0.13500000536441803]\n",
            "Device 10.0: Loss before: [2.2999212741851807, 0.11666666716337204], Loss after: [2.2957682609558105, 0.15000000596046448]\n",
            "Device 11.0: Loss before: [2.3003227710723877, 0.1133333370089531], Loss after: [2.296252965927124, 0.2266666740179062]\n",
            "Device 13.0: Loss before: [2.2998769283294678, 0.11999999731779099], Loss after: [2.292170763015747, 0.23000000417232513]\n",
            "Device 26.0: Loss before: [2.3004539012908936, 0.10833333432674408], Loss after: [2.2967872619628906, 0.21166667342185974]\n",
            "Device 31.0: Loss before: [2.3000717163085938, 0.11999999731779099], Loss after: [2.295858144760132, 0.12333333492279053]\n",
            "Device 33.0: Loss before: [2.3001513481140137, 0.11500000208616257], Loss after: [2.2962687015533447, 0.11500000208616257]\n",
            "Device 36.0: Loss before: [2.300018548965454, 0.11166666448116302], Loss after: [2.2950491905212402, 0.2266666740179062]\n",
            "Device 42.0: Loss before: [2.2997279167175293, 0.12666666507720947], Loss after: [2.294978618621826, 0.12666666507720947]\n",
            "Device 43.0: Loss before: [2.3002068996429443, 0.11166666448116302], Loss after: [2.296827793121338, 0.1483333259820938]\n",
            "Device 44.0: Loss before: [2.300229072570801, 0.11500000208616257], Loss after: [2.2959046363830566, 0.21666666865348816]\n",
            "Device 46.0: Loss before: [2.2997443675994873, 0.11833333224058151], Loss after: [2.2947025299072266, 0.13333334028720856]\n",
            "Device 47.0: Loss before: [2.3001575469970703, 0.1066666692495346], Loss after: [2.2964301109313965, 0.2750000059604645]\n",
            "Device 50.0: Loss before: [2.300027847290039, 0.12166666984558105], Loss after: [2.2961578369140625, 0.12833333015441895]\n",
            "Device 51.0: Loss before: [2.3001415729522705, 0.10833333432674408], Loss after: [2.2955944538116455, 0.1666666716337204]\n",
            "Device 52.0: Loss before: [2.300358772277832, 0.11500000208616257], Loss after: [2.2980241775512695, 0.11500000208616257]\n",
            "Device 53.0: Loss before: [2.300273895263672, 0.11999999731779099], Loss after: [2.2957959175109863, 0.11999999731779099]\n",
            "Device 54.0: Loss before: [2.300271987915039, 0.11166666448116302], Loss after: [2.297130823135376, 0.11500000208616257]\n",
            "Device 55.0: Loss before: [2.2997541427612305, 0.13333334028720856], Loss after: [2.2947449684143066, 0.13333334028720856]\n",
            "Device 58.0: Loss before: [2.300499439239502, 0.10999999940395355], Loss after: [2.296035051345825, 0.2750000059604645]\n",
            "Device 61.0: Loss before: [2.3002665042877197, 0.10833333432674408], Loss after: [2.296799898147583, 0.19333332777023315]\n",
            "Device 62.0: Loss before: [2.300647497177124, 0.0949999988079071], Loss after: [2.2955031394958496, 0.12166666984558105]\n",
            "Device 64.0: Loss before: [2.2996182441711426, 0.14000000059604645], Loss after: [2.2935292720794678, 0.14000000059604645]\n",
            "Device 68.0: Loss before: [2.300259828567505, 0.10999999940395355], Loss after: [2.2954745292663574, 0.20999999344348907]\n",
            "Device 71.0: Loss before: [2.300182342529297, 0.10999999940395355], Loss after: [2.2962915897369385, 0.2150000035762787]\n",
            "Device 72.0: Loss before: [2.3002490997314453, 0.10833333432674408], Loss after: [2.296476364135742, 0.1899999976158142]\n",
            "Device 75.0: Loss before: [2.3002707958221436, 0.10499999672174454], Loss after: [2.29699444770813, 0.27166667580604553]\n",
            "Device 77.0: Loss before: [2.300211191177368, 0.10499999672174454], Loss after: [2.2960739135742188, 0.31166666746139526]\n",
            "Device 81.0: Loss before: [2.3003127574920654, 0.09833333641290665], Loss after: [2.295391321182251, 0.22166666388511658]\n",
            "Device 84.0: Loss before: [2.3002877235412598, 0.10833333432674408], Loss after: [2.2973263263702393, 0.18000000715255737]\n",
            "Device 86.0: Loss before: [2.3003811836242676, 0.10999999940395355], Loss after: [2.297664165496826, 0.11833333224058151]\n",
            "Device 88.0: Loss before: [2.2999584674835205, 0.11833333224058151], Loss after: [2.2953388690948486, 0.1599999964237213]\n",
            "Device 89.0: Loss before: [2.300579071044922, 0.10166666656732559], Loss after: [2.296921968460083, 0.11666666716337204]\n",
            "Device 91.0: Loss before: [2.3001792430877686, 0.09833333641290665], Loss after: [2.2938570976257324, 0.12333333492279053]\n",
            "Device 94.0: Loss before: [2.2999751567840576, 0.10833333432674408], Loss after: [2.295044422149658, 0.12666666507720947]\n",
            "Device 98.0: Loss before: [2.300133228302002, 0.10166666656732559], Loss after: [2.29656720161438, 0.3083333373069763]\n",
            "[array([[[[ 0.0104019 ,  0.01456954,  0.00742552,  0.01006455,\n",
            "           0.00196505,  0.00970365,  0.00921195,  0.00589242,\n",
            "           0.01472005,  0.01065658,  0.00970208,  0.01202225,\n",
            "           0.00804764,  0.01056569,  0.0054691 ,  0.00751565,\n",
            "           0.01030334,  0.01216461,  0.00473745,  0.00626705,\n",
            "           0.01339198,  0.00522474,  0.00119633,  0.00884867,\n",
            "           0.01019728,  0.01293183,  0.00961764,  0.01703923,\n",
            "           0.00305167,  0.0054794 ,  0.0073023 ,  0.00975773]],\n",
            "\n",
            "        [[ 0.01593596,  0.01943301,  0.00562483,  0.00925633,\n",
            "           0.00869492,  0.01073792,  0.01854609,  0.00767518,\n",
            "           0.01033074,  0.01521997,  0.01895976,  0.01554399,\n",
            "           0.00193141,  0.00875781,  0.01158874,  0.01220714,\n",
            "           0.01339054,  0.0158137 ,  0.01310139,  0.01216915,\n",
            "           0.01520795,  0.00718198,  0.01468034,  0.01170296,\n",
            "           0.00630018,  0.01135292,  0.00833906,  0.01557345,\n",
            "           0.00472391,  0.01112853,  0.01319045,  0.02256079]],\n",
            "\n",
            "        [[ 0.01426401,  0.00987343,  0.01366727,  0.01089505,\n",
            "           0.00409848,  0.01220435,  0.01657431,  0.01291508,\n",
            "           0.01302245,  0.01019731,  0.01269388,  0.01280491,\n",
            "           0.01136292,  0.00263244,  0.00795869,  0.0207912 ,\n",
            "           0.01389933,  0.00952451,  0.00784343,  0.01332998,\n",
            "           0.00950352,  0.01176247,  0.00971634,  0.01384273,\n",
            "           0.00286237,  0.01305851,  0.01087173,  0.00916796,\n",
            "           0.00542504,  0.00844987,  0.01636345,  0.01453124]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01078696,  0.01156245,  0.00662137,  0.01439027,\n",
            "           0.00771842,  0.00538942,  0.01745842,  0.00884285,\n",
            "           0.01565097,  0.01016162,  0.00912837,  0.0127849 ,\n",
            "           0.00429155,  0.00606149,  0.00423488,  0.01184328,\n",
            "           0.00989805,  0.01790188,  0.01301793,  0.01212391,\n",
            "           0.01292737,  0.00889904,  0.00331596,  0.01996118,\n",
            "           0.01118324,  0.01207359,  0.01639264,  0.00089005,\n",
            "           0.01041301,  0.00856566,  0.00373119,  0.01702244]],\n",
            "\n",
            "        [[ 0.01602692,  0.02452437,  0.01236583,  0.01995027,\n",
            "           0.00356969,  0.00820757,  0.02216992,  0.01513946,\n",
            "           0.01901511,  0.01632972,  0.01770663,  0.02228894,\n",
            "           0.01166364,  0.0064906 ,  0.01400202,  0.00853262,\n",
            "           0.01349386,  0.00633783,  0.0089154 ,  0.01488859,\n",
            "           0.01828968,  0.01519994,  0.00754023,  0.01586953,\n",
            "           0.00315866,  0.01427518,  0.01343647,  0.01363066,\n",
            "           0.00798557,  0.0145227 ,  0.01022577,  0.02084502]],\n",
            "\n",
            "        [[ 0.01374343,  0.00630915,  0.01011283,  0.01292864,\n",
            "           0.00668546,  0.00586009,  0.01832122,  0.01016811,\n",
            "           0.01498356,  0.01968579,  0.00822673,  0.01485611,\n",
            "           0.00724131,  0.01456667,  0.00732778,  0.01357138,\n",
            "           0.00979961,  0.01272419,  0.01147504,  0.01416478,\n",
            "           0.01887767,  0.0033983 ,  0.01151099,  0.01806353,\n",
            "           0.00800811,  0.01397651,  0.00543812,  0.01347338,\n",
            "           0.00991374,  0.01025811,  0.01599242,  0.01167826]]],\n",
            "\n",
            "\n",
            "       [[[ 0.02051281,  0.01034913,  0.01654332,  0.01697725,\n",
            "           0.00485939,  0.00522392,  0.01232276,  0.0100379 ,\n",
            "           0.02026369,  0.01365456,  0.00937665,  0.01341158,\n",
            "           0.00496849,  0.01285653,  0.01390275,  0.01851111,\n",
            "           0.00624406,  0.00658414,  0.01067102,  0.01046084,\n",
            "           0.0164174 ,  0.00927531,  0.0088514 ,  0.01068916,\n",
            "           0.01533288,  0.01899562,  0.01532307,  0.00373424,\n",
            "           0.00847885,  0.00798894, -0.00194863,  0.01205001]],\n",
            "\n",
            "        [[ 0.01417176,  0.01764766,  0.01085867,  0.01930618,\n",
            "           0.00643319,  0.01579035,  0.01396455,  0.00914675,\n",
            "           0.01157923,  0.01563327,  0.01649658,  0.01438573,\n",
            "           0.00834186,  0.0132277 ,  0.01517621,  0.0033797 ,\n",
            "           0.00991052,  0.01146418,  0.00991145,  0.01840129,\n",
            "           0.01318629,  0.00705661,  0.00745414,  0.01258648,\n",
            "           0.01308753,  0.00852461,  0.0079097 ,  0.00899327,\n",
            "           0.01045365,  0.01164923,  0.00664562,  0.02014691]],\n",
            "\n",
            "        [[ 0.00191969,  0.01076267,  0.01221889,  0.0090207 ,\n",
            "           0.01427533,  0.01403929,  0.01440518,  0.00348824,\n",
            "           0.00737666,  0.00783006,  0.00879456,  0.00863848,\n",
            "          -0.00075743,  0.01252902,  0.00689395,  0.0073352 ,\n",
            "           0.00627308,  0.00876543,  0.014604  ,  0.00734404,\n",
            "           0.01315636,  0.00632309,  0.00731758,  0.00942484,\n",
            "           0.01105128,  0.00439249,  0.00709061,  0.01332083,\n",
            "           0.01092225,  0.00929628,  0.01248947,  0.01854364]]]],\n",
            "      dtype=float32), array([0.00465415, 0.00397673, 0.00375721, 0.00428502, 0.00454075,\n",
            "       0.00361776, 0.0035569 , 0.00482542, 0.00460004, 0.00508499,\n",
            "       0.00498827, 0.00511495, 0.00293645, 0.00383211, 0.00261024,\n",
            "       0.00489413, 0.00421999, 0.00451936, 0.00474781, 0.00525738,\n",
            "       0.00444016, 0.00440599, 0.00364983, 0.00400981, 0.00342281,\n",
            "       0.00371487, 0.00471957, 0.00357946, 0.00418598, 0.00320994,\n",
            "       0.00394134, 0.00397156], dtype=float32), array([[ 3.6681062e-04, -1.4583535e-04,  3.9760952e-04, ...,\n",
            "        -4.7728702e-04,  1.4868348e-03,  3.8698854e-04],\n",
            "       [ 1.1917278e-03, -2.5060234e-04,  1.0124458e-03, ...,\n",
            "        -6.8178750e-04,  3.9633462e-04, -2.6810751e-04],\n",
            "       [ 3.6032189e-04, -5.8604171e-04, -1.2310670e-03, ...,\n",
            "         1.2702992e-03, -3.1574811e-05, -3.0150122e-04],\n",
            "       ...,\n",
            "       [-1.2112960e-03, -3.2347423e-04, -8.5204776e-04, ...,\n",
            "        -1.7210768e-03,  1.0501067e-03, -9.3646022e-04],\n",
            "       [-5.8312382e-04, -2.1000903e-03,  6.6588179e-04, ...,\n",
            "         4.8858067e-04,  8.8715175e-04, -9.2340144e-04],\n",
            "       [-6.9903728e-04, -3.5219148e-04, -2.9083481e-06, ...,\n",
            "         2.7017411e-05, -9.5537433e-04,  1.0847109e-03]], dtype=float32), array([0.00128909, 0.00118923, 0.00113294, 0.00071236, 0.00090447,\n",
            "       0.00121044, 0.00121728, 0.0012375 , 0.00095398, 0.00147698,\n",
            "       0.00140723, 0.00108495, 0.0010603 , 0.0014271 , 0.00094172,\n",
            "       0.00118167, 0.00213161, 0.0013336 , 0.00092737, 0.00060172,\n",
            "       0.00070161, 0.00072747, 0.00058713, 0.00079406, 0.00133779,\n",
            "       0.00125134, 0.00124731, 0.00034545, 0.00156327, 0.00091389,\n",
            "       0.00047486, 0.0013075 , 0.00141438, 0.00147921, 0.00139141,\n",
            "       0.00133543, 0.0013418 , 0.00128633, 0.00146111, 0.00141494,\n",
            "       0.00160511, 0.00070875, 0.00119895, 0.00130648, 0.00147576,\n",
            "       0.00110767, 0.00159399, 0.00086182, 0.00151201, 0.00089193,\n",
            "       0.00120259, 0.00086762, 0.00090875, 0.00131156, 0.00117797,\n",
            "       0.00111562, 0.00167233, 0.00111172, 0.00089701, 0.00102896,\n",
            "       0.00116174, 0.00110378, 0.0013972 , 0.00129714, 0.00129931,\n",
            "       0.0012102 , 0.0011174 , 0.00106868, 0.00095073, 0.00147761,\n",
            "       0.00131423, 0.00112868, 0.00073378, 0.00065153, 0.00123782,\n",
            "       0.00124008, 0.00115739, 0.00137182, 0.00100195, 0.00118333,\n",
            "       0.00182942, 0.00192452, 0.00127278, 0.00106348, 0.00149509,\n",
            "       0.00136858, 0.00125677, 0.00084337, 0.00095395, 0.00052945,\n",
            "       0.00151927, 0.00087255, 0.00174823, 0.00139196, 0.00166854,\n",
            "       0.00104553, 0.00233361, 0.00144833, 0.00187016, 0.00121135,\n",
            "       0.00118327, 0.00078204, 0.00124561, 0.00104176, 0.00127002,\n",
            "       0.00079021, 0.00138154, 0.00178031, 0.00119175, 0.0009876 ,\n",
            "       0.00142686, 0.00141415, 0.0006916 , 0.00104696, 0.00132584,\n",
            "       0.00127337, 0.00122951, 0.00180446, 0.00133934, 0.00141107,\n",
            "       0.00122937, 0.00161839, 0.00129005, 0.00127878, 0.00116723,\n",
            "       0.00086022, 0.00179071, 0.00092326], dtype=float32), array([[ 2.13568867e-03,  3.18165310e-03,  4.08263551e-03, ...,\n",
            "         4.07267082e-03, -2.24842853e-03,  8.43883306e-03],\n",
            "       [ 2.64348811e-03, -1.72151427e-04, -1.01661216e-03, ...,\n",
            "        -4.41190496e-04,  1.16254175e-02,  3.57058551e-03],\n",
            "       [ 7.75172433e-04, -1.04363961e-02, -5.15866512e-03, ...,\n",
            "        -4.85106139e-03,  5.72772045e-03,  5.07691130e-03],\n",
            "       ...,\n",
            "       [ 4.20858305e-05, -6.05735276e-03,  9.36947949e-03, ...,\n",
            "         4.36634663e-03,  7.51795853e-03,  1.56478002e-03],\n",
            "       [ 1.90698740e-03,  8.96375533e-03,  2.56475597e-03, ...,\n",
            "        -1.01733375e-02,  8.05408880e-03,  4.53115581e-03],\n",
            "       [ 7.94902630e-03, -5.19113103e-03,  2.40813335e-03, ...,\n",
            "         2.97831232e-03,  7.84119405e-03,  4.22628317e-03]], dtype=float32), array([-0.00695067,  0.02232892, -0.00294635, -0.00083978, -0.00034255,\n",
            "       -0.0072303 , -0.00224063,  0.00614844, -0.00703885, -0.00088823],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "[array([[[[ 1.21182529e-02,  1.63573883e-02,  8.67665280e-03,\n",
            "           1.17328763e-02,  2.67642783e-03,  1.08325183e-02,\n",
            "           1.10950759e-02,  7.14898482e-03,  1.65738408e-02,\n",
            "           1.22504486e-02,  1.12094190e-02,  1.37285693e-02,\n",
            "           9.00254492e-03,  1.16680600e-02,  6.49694121e-03,\n",
            "           8.98756739e-03,  1.18654892e-02,  1.36185978e-02,\n",
            "           5.84824290e-03,  7.74353510e-03,  1.50376456e-02,\n",
            "           6.28681527e-03,  2.11141491e-03,  1.05102109e-02,\n",
            "           1.12862764e-02,  1.45274121e-02,  1.10279499e-02,\n",
            "           1.84062086e-02,  3.96798830e-03,  6.65903138e-03,\n",
            "           8.37431755e-03,  1.17310900e-02]],\n",
            "\n",
            "        [[ 1.80052985e-02,  2.16533113e-02,  7.22988416e-03,\n",
            "           1.12609761e-02,  9.67558008e-03,  1.21953348e-02,\n",
            "           2.09618397e-02,  9.25050303e-03,  1.25634540e-02,\n",
            "           1.72268059e-02,  2.08329260e-02,  1.76623259e-02,\n",
            "           3.02086445e-03,  1.01170680e-02,  1.29048089e-02,\n",
            "           1.41092865e-02,  1.53031982e-02,  1.75514221e-02,\n",
            "           1.45809958e-02,  1.40822437e-02,  1.72936749e-02,\n",
            "           8.48012511e-03,  1.59723684e-02,  1.37686841e-02,\n",
            "           7.61063257e-03,  1.32672070e-02,  9.98144224e-03,\n",
            "           1.73011255e-02,  5.89588284e-03,  1.26666930e-02,\n",
            "           1.46402130e-02,  2.50883345e-02]],\n",
            "\n",
            "        [[ 1.61397737e-02,  1.18925087e-02,  1.52337141e-02,\n",
            "           1.27362041e-02,  5.09313866e-03,  1.36335315e-02,\n",
            "           1.88609250e-02,  1.43831503e-02,  1.50702912e-02,\n",
            "           1.20790433e-02,  1.44477282e-02,  1.47674270e-02,\n",
            "           1.23862904e-02,  3.90453381e-03,  9.20919701e-03,\n",
            "           2.26256680e-02,  1.56793520e-02,  1.10768499e-02,\n",
            "           9.23707616e-03,  1.51347127e-02,  1.14448396e-02,\n",
            "           1.29393581e-02,  1.09805865e-02,  1.57953110e-02,\n",
            "           4.03986080e-03,  1.47578828e-02,  1.22957528e-02,\n",
            "           1.08215911e-02,  6.54364703e-03,  9.89246368e-03,\n",
            "           1.78253707e-02,  1.68477986e-02]]],\n",
            "\n",
            "\n",
            "       [[[ 1.28728850e-02,  1.37301320e-02,  8.25876370e-03,\n",
            "           1.64524168e-02,  8.66709650e-03,  6.79375604e-03,\n",
            "           1.98267065e-02,  1.03876004e-02,  1.79524738e-02,\n",
            "           1.21227633e-02,  1.09802103e-02,  1.48718851e-02,\n",
            "           5.34445187e-03,  7.47434329e-03,  5.59306331e-03,\n",
            "           1.36473887e-02,  1.16441362e-02,  1.96415298e-02,\n",
            "           1.45181306e-02,  1.39996484e-02,  1.49831902e-02,\n",
            "           1.02458550e-02,  4.48654406e-03,  2.20088437e-02,\n",
            "           1.26540493e-02,  1.40311876e-02,  1.81110632e-02,\n",
            "           2.42608390e-03,  1.16397599e-02,  1.00354617e-02,\n",
            "           4.93575586e-03,  1.94645040e-02]],\n",
            "\n",
            "        [[ 1.83133837e-02,  2.70069111e-02,  1.42847775e-02,\n",
            "           2.22402588e-02,  4.74569062e-03,  9.89397056e-03,\n",
            "           2.48850398e-02,  1.69132333e-02,  2.15565227e-02,\n",
            "           1.86059345e-02,  1.98173970e-02,  2.46623848e-02,\n",
            "           1.28461923e-02,  8.10622051e-03,  1.55664375e-02,\n",
            "           1.05996020e-02,  1.54930372e-02,  8.25758185e-03,\n",
            "           1.06653310e-02,  1.70669444e-02,  2.06681639e-02,\n",
            "           1.66573245e-02,  9.00477730e-03,  1.82127226e-02,\n",
            "           4.79177386e-03,  1.64212510e-02,  1.52541408e-02,\n",
            "           1.54946856e-02,  9.42171831e-03,  1.62658747e-02,\n",
            "           1.17848385e-02,  2.36491431e-02]],\n",
            "\n",
            "        [[ 1.56739131e-02,  8.45608860e-03,  1.18624186e-02,\n",
            "           1.49064977e-02,  7.82271754e-03,  7.44200405e-03,\n",
            "           2.07436476e-02,  1.17178857e-02,  1.71820428e-02,\n",
            "           2.17454787e-02,  1.00927744e-02,  1.69356856e-02,\n",
            "           8.29244219e-03,  1.60379298e-02,  8.70789401e-03,\n",
            "           1.54197505e-02,  1.15606301e-02,  1.43950861e-02,\n",
            "           1.30489133e-02,  1.60808470e-02,  2.09946539e-02,\n",
            "           4.59409086e-03,  1.28826154e-02,  2.01769043e-02,\n",
            "           9.41383187e-03,  1.57615505e-02,  6.88687200e-03,\n",
            "           1.52098211e-02,  1.12173250e-02,  1.18142152e-02,\n",
            "           1.75056905e-02,  1.41084855e-02]]],\n",
            "\n",
            "\n",
            "       [[[ 2.24370286e-02,  1.23270527e-02,  1.81447528e-02,\n",
            "           1.89111214e-02,  5.79631887e-03,  6.57070940e-03,\n",
            "           1.45139378e-02,  1.14367837e-02,  2.24035587e-02,\n",
            "           1.54711371e-02,  1.10887103e-02,  1.53395087e-02,\n",
            "           5.88608906e-03,  1.42732430e-02,  1.52540877e-02,\n",
            "           2.01598443e-02,  7.71424966e-03,  8.08963832e-03,\n",
            "           1.21246278e-02,  1.22203538e-02,  1.83674395e-02,\n",
            "           1.05232121e-02,  1.00082234e-02,  1.25427488e-02,\n",
            "           1.68453213e-02,  2.08139550e-02,  1.69244427e-02,\n",
            "           5.08035906e-03,  9.72061325e-03,  9.40927770e-03,\n",
            "          -9.48040746e-04,  1.43114002e-02]],\n",
            "\n",
            "        [[ 1.61154009e-02,  1.98248792e-02,  1.26153994e-02,\n",
            "           2.13603210e-02,  7.54904933e-03,  1.73656512e-02,\n",
            "           1.63247380e-02,  1.06472000e-02,  1.37913506e-02,\n",
            "           1.76258143e-02,  1.83601901e-02,  1.64582580e-02,\n",
            "           9.33977775e-03,  1.47721814e-02,  1.66194215e-02,\n",
            "           5.10831038e-03,  1.15265111e-02,  1.31048588e-02,\n",
            "           1.15134725e-02,  2.03307290e-02,  1.53186545e-02,\n",
            "           8.30584392e-03,  8.78737401e-03,  1.46314707e-02,\n",
            "           1.47016374e-02,  1.03752911e-02,  9.48706362e-03,\n",
            "           1.05760470e-02,  1.18365996e-02,  1.32561913e-02,\n",
            "           7.93357007e-03,  2.26261951e-02]],\n",
            "\n",
            "        [[ 3.33725964e-03,  1.25108045e-02,  1.36949923e-02,\n",
            "           1.06444350e-02,  1.52993118e-02,  1.54421311e-02,\n",
            "           1.63252186e-02,  4.66009788e-03,  9.11513530e-03,\n",
            "           9.46675614e-03,  1.03040356e-02,  1.02759013e-02,\n",
            "           3.51090348e-05,  1.38375806e-02,  8.02964251e-03,\n",
            "           8.72049667e-03,  7.55911693e-03,  1.01069473e-02,\n",
            "           1.59462523e-02,  8.84446315e-03,  1.48971146e-02,\n",
            "           7.24869361e-03,  8.47055856e-03,  1.11099649e-02,\n",
            "           1.23502696e-02,  5.76542271e-03,  8.22434016e-03,\n",
            "           1.47076799e-02,  1.20936166e-02,  1.06306737e-02,\n",
            "           1.36928968e-02,  2.05352511e-02]]]], dtype=float32), array([0.00475102, 0.00393988, 0.00378099, 0.00429981, 0.00460198,\n",
            "       0.0036069 , 0.00358696, 0.00492608, 0.00463988, 0.00511853,\n",
            "       0.00504873, 0.00511811, 0.00295048, 0.00383079, 0.00261328,\n",
            "       0.0048966 , 0.00428295, 0.00457394, 0.00476533, 0.00527168,\n",
            "       0.00445546, 0.00450613, 0.00364829, 0.00399976, 0.0034926 ,\n",
            "       0.00374149, 0.00474993, 0.00360659, 0.00421629, 0.00320742,\n",
            "       0.00401485, 0.0040056 ], dtype=float32), array([[ 3.6710044e-04, -1.4529648e-04,  3.9716769e-04, ...,\n",
            "        -4.7775070e-04,  1.4876317e-03,  3.8697990e-04],\n",
            "       [ 1.1919722e-03, -2.5014937e-04,  1.0120794e-03, ...,\n",
            "        -6.8218133e-04,  3.9700602e-04, -2.6811738e-04],\n",
            "       [ 3.6055554e-04, -5.8560644e-04, -1.2314193e-03, ...,\n",
            "         1.2699252e-03, -3.0932853e-05, -3.0150314e-04],\n",
            "       ...,\n",
            "       [-1.2110772e-03, -3.2311666e-04, -8.5235864e-04, ...,\n",
            "        -1.7213591e-03,  1.0506598e-03, -9.3644473e-04],\n",
            "       [-5.8285764e-04, -2.0996479e-03,  6.6549343e-04, ...,\n",
            "         4.8823122e-04,  8.8783813e-04, -9.2338881e-04],\n",
            "       [-6.9875387e-04, -3.5175911e-04, -3.2976104e-06, ...,\n",
            "         2.6691279e-05, -9.5468614e-04,  1.0847341e-03]], dtype=float32), array([0.00135097, 0.00130382, 0.00103502, 0.00038617, 0.00069273,\n",
            "       0.00119049, 0.00118134, 0.00164204, 0.00076894, 0.0016577 ,\n",
            "       0.00144265, 0.00127522, 0.00082229, 0.00202693, 0.00082323,\n",
            "       0.00115854, 0.00323945, 0.00145774, 0.00096166, 0.00021889,\n",
            "       0.00050687, 0.00053651, 0.00050048, 0.00057545, 0.00161835,\n",
            "       0.0014211 , 0.0013707 , 0.00038047, 0.00190582, 0.00078204,\n",
            "       0.000207  , 0.00140993, 0.00137399, 0.00180372, 0.00169142,\n",
            "       0.00148984, 0.00138357, 0.00130192, 0.00175311, 0.0014289 ,\n",
            "       0.00255009, 0.00056231, 0.00124577, 0.00134458, 0.00219615,\n",
            "       0.0012443 , 0.00182645, 0.00073046, 0.00153677, 0.00095302,\n",
            "       0.00157167, 0.00084285, 0.0009043 , 0.00133152, 0.00131991,\n",
            "       0.00101723, 0.00186007, 0.00118309, 0.00074613, 0.00088997,\n",
            "       0.00101156, 0.00114903, 0.00137692, 0.00118221, 0.0012548 ,\n",
            "       0.00109554, 0.00133815, 0.00114358, 0.0009427 , 0.00168502,\n",
            "       0.00138313, 0.00098364, 0.00055012, 0.00052304, 0.0014765 ,\n",
            "       0.00156938, 0.00122958, 0.00139305, 0.00092412, 0.00115579,\n",
            "       0.00264701, 0.00194003, 0.00135681, 0.00106605, 0.0020273 ,\n",
            "       0.00148743, 0.00114221, 0.00061528, 0.00085511, 0.00027733,\n",
            "       0.00219047, 0.00067775, 0.00193542, 0.00177744, 0.00168118,\n",
            "       0.0011002 , 0.00379513, 0.00166338, 0.00242246, 0.00125043,\n",
            "       0.00110899, 0.00047206, 0.00151306, 0.00105486, 0.00165373,\n",
            "       0.00059531, 0.00172565, 0.00247244, 0.00119525, 0.0009311 ,\n",
            "       0.00126783, 0.00152226, 0.00050608, 0.00100493, 0.00130414,\n",
            "       0.00116209, 0.00134004, 0.0019889 , 0.00142534, 0.00141837,\n",
            "       0.0015354 , 0.00168932, 0.00124742, 0.00141593, 0.00114582,\n",
            "       0.00075933, 0.00196107, 0.00092187], dtype=float32), array([[ 0.00258902,  0.00273877,  0.00442961, ...,  0.0037811 ,\n",
            "        -0.00214944,  0.00846101],\n",
            "       [ 0.00298207, -0.00031288, -0.00064571, ..., -0.00062153,\n",
            "         0.01275552,  0.0032003 ],\n",
            "       [ 0.00198457, -0.01268533, -0.0047578 , ..., -0.00546877,\n",
            "         0.00622755,  0.00526169],\n",
            "       ...,\n",
            "       [ 0.0005115 , -0.00813074,  0.01013657, ...,  0.00432257,\n",
            "         0.00820638,  0.0018573 ],\n",
            "       [ 0.00210609,  0.00966632,  0.00320352, ..., -0.01129136,\n",
            "         0.00912196,  0.00405199],\n",
            "       [ 0.01005624, -0.00692317,  0.00234586, ...,  0.00358066,\n",
            "         0.00869113,  0.00404181]], dtype=float32), array([-0.0104256 ,  0.04095443, -0.00216255,  0.00147252, -0.00460501,\n",
            "       -0.02058453, -0.00469623,  0.01130504, -0.0085853 , -0.00267277],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "global model stayed the same?\n",
            "False\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1405 - loss: 2.2965\n",
            "Global Model Accuracy: 0.1300\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vEyWF6-HHqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      5 |             - |             -\n",
            "     2 |      100 |      6 |  0.0564924725 |         ideal\n",
            "     3 |      150 |      8 |  0.4483626535 |         ideal\n",
            "Device 4.0: Loss before: [2.3013622760772705, 0.12333333492279053], Loss after: [2.298837661743164, 0.12333333492279053]\n",
            "Device 5.0: Loss before: [2.3016862869262695, 0.10833333432674408], Loss after: [2.2995495796203613, 0.10833333432674408]\n",
            "Device 6.0: Loss before: [2.3016045093536377, 0.1133333370089531], Loss after: [2.300372362136841, 0.1133333370089531]\n",
            "Device 7.0: Loss before: [2.3015823364257812, 0.10333333164453506], Loss after: [2.2990756034851074, 0.12166666984558105]\n",
            "Device 10.0: Loss before: [2.3015029430389404, 0.11666666716337204], Loss after: [2.299830198287964, 0.11666666716337204]\n",
            "Device 11.0: Loss before: [2.3018038272857666, 0.1133333370089531], Loss after: [2.300283908843994, 0.1133333370089531]\n",
            "Device 13.0: Loss before: [2.3014400005340576, 0.11999999731779099], Loss after: [2.297279119491577, 0.25]\n",
            "Device 26.0: Loss before: [2.301870822906494, 0.10833333432674408], Loss after: [2.3003807067871094, 0.10833333432674408]\n",
            "Device 31.0: Loss before: [2.301523447036743, 0.11999999731779099], Loss after: [2.2993855476379395, 0.11999999731779099]\n",
            "Device 33.0: Loss before: [2.3016774654388428, 0.11500000208616257], Loss after: [2.299994707107544, 0.11500000208616257]\n",
            "Device 36.0: Loss before: [2.301600456237793, 0.11166666448116302], Loss after: [2.299229621887207, 0.1433333307504654]\n",
            "Device 42.0: Loss before: [2.301372766494751, 0.12666666507720947], Loss after: [2.2993016242980957, 0.12666666507720947]\n",
            "Device 43.0: Loss before: [2.301710367202759, 0.11166666448116302], Loss after: [2.3004438877105713, 0.11166666448116302]\n",
            "Device 44.0: Loss before: [2.301706552505493, 0.11500000208616257], Loss after: [2.299513339996338, 0.11500000208616257]\n",
            "Device 46.0: Loss before: [2.301340341567993, 0.11833333224058151], Loss after: [2.298771858215332, 0.11833333224058151]\n",
            "Device 47.0: Loss before: [2.301769733428955, 0.1066666692495346], Loss after: [2.3008229732513428, 0.1066666692495346]\n",
            "Device 50.0: Loss before: [2.301445245742798, 0.12166666984558105], Loss after: [2.299644708633423, 0.12166666984558105]\n",
            "Device 51.0: Loss before: [2.301565170288086, 0.10833333432674408], Loss after: [2.2990448474884033, 0.11999999731779099]\n",
            "Device 52.0: Loss before: [2.3016886711120605, 0.11500000208616257], Loss after: [2.3008017539978027, 0.11500000208616257]\n",
            "Device 53.0: Loss before: [2.3016433715820312, 0.11999999731779099], Loss after: [2.2987868785858154, 0.11999999731779099]\n",
            "Device 54.0: Loss before: [2.3017451763153076, 0.11166666448116302], Loss after: [2.3007490634918213, 0.11166666448116302]\n",
            "Device 55.0: Loss before: [2.301236629486084, 0.13333334028720856], Loss after: [2.2984275817871094, 0.13333334028720856]\n",
            "Device 58.0: Loss before: [2.3018391132354736, 0.10999999940395355], Loss after: [2.2992539405822754, 0.10999999940395355]\n",
            "Device 61.0: Loss before: [2.3017876148223877, 0.10833333432674408], Loss after: [2.300678014755249, 0.10833333432674408]\n",
            "Device 62.0: Loss before: [2.302037477493286, 0.0949999988079071], Loss after: [2.299220085144043, 0.12166666984558105]\n",
            "Device 64.0: Loss before: [2.3012399673461914, 0.14000000059604645], Loss after: [2.2974958419799805, 0.14000000059604645]\n",
            "Device 68.0: Loss before: [2.301774740219116, 0.10999999940395355], Loss after: [2.2996461391448975, 0.2133333384990692]\n",
            "Device 71.0: Loss before: [2.3016538619995117, 0.10999999940395355], Loss after: [2.3001623153686523, 0.10999999940395355]\n",
            "Device 72.0: Loss before: [2.301738977432251, 0.10833333432674408], Loss after: [2.3001327514648438, 0.10833333432674408]\n",
            "Device 75.0: Loss before: [2.3018112182617188, 0.10499999672174454], Loss after: [2.3009819984436035, 0.10499999672174454]\n",
            "Device 77.0: Loss before: [2.301687717437744, 0.10499999672174454], Loss after: [2.299941062927246, 0.14000000059604645]\n",
            "Device 81.0: Loss before: [2.301790475845337, 0.09833333641290665], Loss after: [2.299125909805298, 0.11666666716337204]\n",
            "Device 84.0: Loss before: [2.3017520904541016, 0.10833333432674408], Loss after: [2.3009040355682373, 0.10833333432674408]\n",
            "Device 86.0: Loss before: [2.30180287361145, 0.10999999940395355], Loss after: [2.3009963035583496, 0.10999999940395355]\n",
            "Device 88.0: Loss before: [2.301478862762451, 0.11833333224058151], Loss after: [2.2994086742401123, 0.11833333224058151]\n",
            "Device 89.0: Loss before: [2.3020639419555664, 0.10166666656732559], Loss after: [2.300623893737793, 0.11666666716337204]\n",
            "Device 91.0: Loss before: [2.301717758178711, 0.09833333641290665], Loss after: [2.298187494277954, 0.12333333492279053]\n",
            "Device 94.0: Loss before: [2.301567792892456, 0.10833333432674408], Loss after: [2.299182653427124, 0.12666666507720947]\n",
            "Device 98.0: Loss before: [2.3017964363098145, 0.10166666656732559], Loss after: [2.301105260848999, 0.125]\n",
            "[array([[[[ 0.00631241,  0.00860124,  0.00451629,  0.00611067,\n",
            "           0.00134348,  0.00570449,  0.0057319 ,  0.0036867 ,\n",
            "           0.00870861,  0.0064028 ,  0.00585124,  0.00718754,\n",
            "           0.00473826,  0.00616181,  0.00336784,  0.00465129,\n",
            "           0.0061988 ,  0.00716636,  0.00300328,  0.0039757 ,\n",
            "           0.00790696,  0.00324863,  0.00100589,  0.0054484 ,\n",
            "           0.00595673,  0.0076378 ,  0.00576759,  0.0097769 ,\n",
            "           0.00201289,  0.00343263,  0.00437958,  0.00606318]],\n",
            "\n",
            "        [[ 0.0094523 ,  0.01140833,  0.00367744,  0.00580337,\n",
            "           0.00509945,  0.00639371,  0.0110034 ,  0.00477816,\n",
            "           0.00647518,  0.0090395 ,  0.0110162 ,  0.00925876,\n",
            "           0.00147268,  0.00528122,  0.00680016,  0.00736421,\n",
            "           0.0080103 ,  0.00925665,  0.00768455,  0.00734787,\n",
            "           0.00906374,  0.0044025 ,  0.00846798,  0.00715449,\n",
            "           0.00392892,  0.00690553,  0.00516447,  0.00912239,\n",
            "           0.00301979,  0.00663711,  0.00772123,  0.01322497]],\n",
            "\n",
            "        [[ 0.00846977,  0.00614378,  0.00802541,  0.00662861,\n",
            "           0.00261134,  0.00717824,  0.00988334,  0.00757898,\n",
            "           0.00786334,  0.00626592,  0.00757043,  0.00771205,\n",
            "           0.0065635 ,  0.00192422,  0.00480529,  0.01199467,\n",
            "           0.00823459,  0.00577246,  0.00479858,  0.00793536,\n",
            "           0.00591277,  0.00684013,  0.00576417,  0.00827119,\n",
            "           0.00201206,  0.007747  ,  0.00645328,  0.00561853,\n",
            "           0.00337934,  0.00514667,  0.00944732,  0.00878666]]],\n",
            "\n",
            "\n",
            "       [[[ 0.00666544,  0.00711802,  0.00423068,  0.00861096,\n",
            "           0.00455723,  0.00347145,  0.01039482,  0.0053997 ,\n",
            "           0.00938824,  0.00627752,  0.00567436,  0.00774973,\n",
            "           0.0027388 ,  0.00383937,  0.00282711,  0.00712858,\n",
            "           0.00605067,  0.01039023,  0.00764731,  0.00730876,\n",
            "           0.00781487,  0.00535299,  0.00225544,  0.01162753,\n",
            "           0.00664052,  0.00731339,  0.00956319,  0.00108191,\n",
            "           0.00612749,  0.00522009,  0.00249395,  0.01018708]],\n",
            "\n",
            "        [[ 0.00958632,  0.01427266,  0.00745685,  0.01171612,\n",
            "           0.0023952 ,  0.0051103 ,  0.01308608,  0.00890493,\n",
            "           0.01130685,  0.0097466 ,  0.01042905,  0.01301739,\n",
            "           0.00678872,  0.00415124,  0.00820622,  0.00543507,\n",
            "           0.0081001 ,  0.00418698,  0.00551908,  0.00892662,\n",
            "           0.01084976,  0.00881435,  0.00466175,  0.00952306,\n",
            "           0.00235049,  0.00858125,  0.00799818,  0.0081216 ,\n",
            "           0.00489227,  0.0085584 ,  0.00615552,  0.01240203]],\n",
            "\n",
            "        [[ 0.0082087 ,  0.00426002,  0.00616855,  0.00778515,\n",
            "           0.00407039,  0.00379605,  0.01088401,  0.00612061,\n",
            "           0.00898599,  0.01148282,  0.00519076,  0.00887047,\n",
            "           0.00433835,  0.00847621,  0.00451357,  0.00808334,\n",
            "           0.00600309,  0.00755452,  0.00683902,  0.00843166,\n",
            "           0.01106676,  0.00230994,  0.00677965,  0.01062368,\n",
            "           0.00489263,  0.00827842,  0.00351519,  0.00798652,\n",
            "           0.0058866 ,  0.00617191,  0.00926606,  0.00728324]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01187864,  0.00638579,  0.00959931,  0.00996437,\n",
            "           0.00300162,  0.00335922,  0.00753966,  0.00599113,\n",
            "           0.0118276 ,  0.00811606,  0.00575454,  0.00802767,\n",
            "           0.00305327,  0.00752719,  0.00806926,  0.01068528,\n",
            "           0.00396078,  0.00415904,  0.00635594,  0.00636122,\n",
            "           0.00966696,  0.00551854,  0.00525304,  0.00652177,\n",
            "           0.00890794,  0.01101427,  0.0089373 ,  0.00255081,\n",
            "           0.00508406,  0.00488793, -0.0006669 ,  0.00741907]],\n",
            "\n",
            "        [[ 0.0084462 ,  0.01042297,  0.00657595,  0.01127472,\n",
            "           0.00392519,  0.00918063,  0.00849624,  0.00554729,\n",
            "           0.00714446,  0.00925814,  0.0096762 ,  0.00861254,\n",
            "           0.00491465,  0.00777838,  0.00879595,  0.00250765,\n",
            "           0.00600669,  0.00685918,  0.00600175,  0.01073518,\n",
            "           0.00798518,  0.00431541,  0.00456383,  0.00762571,\n",
            "           0.00772949,  0.00534637,  0.00490618,  0.00549613,\n",
            "           0.00621046,  0.00694642,  0.00410755,  0.01189662]],\n",
            "\n",
            "        [[ 0.00159428,  0.00652052,  0.00720443,  0.00552699,\n",
            "           0.00814372,  0.00816342,  0.00856363,  0.00234939,\n",
            "           0.00467984,  0.00488612,  0.00535974,  0.00532497,\n",
            "          -0.00010182,  0.00730733,  0.00418289,  0.00451961,\n",
            "           0.0039047 ,  0.00527844,  0.00844615,  0.00456931,\n",
            "           0.00781622,  0.00379126,  0.00441945,  0.00577018,\n",
            "           0.00650193,  0.00291837,  0.00428881,  0.00776742,\n",
            "           0.00638215,  0.00556365,  0.00724487,  0.01083665]]]],\n",
            "      dtype=float32), array([0.00256266, 0.002143  , 0.00204756, 0.00233039, 0.00248724,\n",
            "       0.0019584 , 0.00194134, 0.00265705, 0.00251106, 0.00277169,\n",
            "       0.00272972, 0.00277609, 0.0015985 , 0.0020784 , 0.00141724,\n",
            "       0.00265603, 0.0023139 , 0.00247304, 0.00258252, 0.0028577 ,\n",
            "       0.00241476, 0.0024293 , 0.00197943, 0.0021714 , 0.00188409,\n",
            "       0.0020257 , 0.00257221, 0.00195244, 0.00228271, 0.00174041,\n",
            "       0.00216685, 0.00216785], dtype=float32), array([[ 1.99107788e-04, -7.89055193e-05,  2.15530861e-04, ...,\n",
            "        -2.59109045e-04,  8.06918659e-04,  2.09937920e-04],\n",
            "       [ 6.46607659e-04, -1.35775117e-04,  5.49108954e-04, ...,\n",
            "        -3.70023306e-04,  2.15273380e-04, -1.45452184e-04],\n",
            "       [ 1.95565750e-04, -3.17757862e-04, -6.67991233e-04, ...,\n",
            "         6.88991451e-04, -1.68789702e-05, -1.63565157e-04],\n",
            "       ...,\n",
            "       [-6.57042779e-04, -1.75345311e-04, -4.62357159e-04, ...,\n",
            "        -9.33794247e-04,  5.69898577e-04, -5.08023659e-04],\n",
            "       [-3.16240854e-04, -1.13912649e-03,  3.61089420e-04, ...,\n",
            "         2.64918723e-04,  4.81547526e-04, -5.00940369e-04],\n",
            "       [-3.79117177e-04, -1.90895254e-04, -1.72959108e-06, ...,\n",
            "         1.45297545e-05, -5.18022163e-04,  5.88464725e-04]], dtype=float32), array([0.00072346, 0.00068985, 0.00057643, 0.00025924, 0.0004081 ,\n",
            "       0.00064888, 0.00064636, 0.00082911, 0.00044537, 0.00087174,\n",
            "       0.00077724, 0.00066279, 0.00048239, 0.00100814, 0.00046467,\n",
            "       0.00063203, 0.00158846, 0.00077189, 0.00051647, 0.00017713,\n",
            "       0.00030467, 0.00032018, 0.00028473, 0.00034552, 0.00083517,\n",
            "       0.00074506, 0.00072479, 0.00020107, 0.00098167, 0.00044436,\n",
            "       0.00015315, 0.00074927, 0.00075155, 0.00092903, 0.00087185,\n",
            "       0.00078469, 0.00074421, 0.00070392, 0.00090653, 0.00077305,\n",
            "       0.00123932, 0.00032739, 0.00066869, 0.00072362, 0.00108155,\n",
            "       0.0006542 , 0.0009554 , 0.00041631, 0.00082992, 0.0005077 ,\n",
            "       0.00079635, 0.00046102, 0.00049126, 0.00071931, 0.00069441,\n",
            "       0.00056685, 0.00098046, 0.00063094, 0.00042778, 0.00050401,\n",
            "       0.00057167, 0.00061645, 0.00075007, 0.00065888, 0.00068752,\n",
            "       0.00061182, 0.00069228, 0.00060897, 0.00051264, 0.00088249,\n",
            "       0.00073984, 0.00055574, 0.00032645, 0.00030334, 0.0007646 ,\n",
            "       0.00080117, 0.00065604, 0.00075249, 0.0005132 , 0.00063121,\n",
            "       0.00131132, 0.0010501 , 0.00072325, 0.00057794, 0.00101865,\n",
            "       0.0007888 , 0.00063712, 0.00036857, 0.00047897, 0.0001889 ,\n",
            "       0.00108597, 0.00039739, 0.00102142, 0.00090547, 0.00091011,\n",
            "       0.00058852, 0.00183598, 0.00086959, 0.00122996, 0.0006724 ,\n",
            "       0.00061296, 0.00030336, 0.00078005, 0.00057027, 0.00083863,\n",
            "       0.00035268, 0.00088369, 0.00123575, 0.00064789, 0.00051374,\n",
            "       0.00071205, 0.00080934, 0.00030284, 0.00055159, 0.0007108 ,\n",
            "       0.00064741, 0.00071011, 0.00105085, 0.00076013, 0.00076835,\n",
            "       0.00078629, 0.00090564, 0.00068323, 0.00074723, 0.00062487,\n",
            "       0.00042732, 0.0010379 , 0.00050033], dtype=float32), array([[ 0.00133541,  0.00155332,  0.00235015, ...,  0.00209571,\n",
            "        -0.00118117,  0.00458672],\n",
            "       [ 0.00156614, -0.00014828, -0.00040686, ..., -0.00030968,\n",
            "         0.00674753,  0.00179263],\n",
            "       [ 0.00089219, -0.00653883, -0.00264224, ..., -0.00287261,\n",
            "         0.00330222,  0.00282629],\n",
            "       ...,\n",
            "       [ 0.0002059 , -0.00409473,  0.00538211, ...,  0.00235167,\n",
            "         0.00434698,  0.00096298],\n",
            "       [ 0.00111219,  0.00513684,  0.0016405 , ..., -0.00595506,\n",
            "         0.00478581,  0.00227128],\n",
            "       [ 0.00513416, -0.00349168,  0.00128213, ...,  0.00185065,\n",
            "         0.00458532,  0.00222081]], dtype=float32), array([-0.00512596,  0.01937739, -0.00129271,  0.00044622, -0.00184819,\n",
            "       -0.00913059, -0.00217322,  0.0053466 , -0.00442169, -0.00117784],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "aggregate_weights() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(global_model.get_weights())\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m global_model.set_weights(\u001b[43maggregate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_devices\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(global_model.get_weights())\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: aggregate_weights() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(global_model, selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZtp6yLRHqJx"
      },
      "outputs": [],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(global_model, devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAz6GN5kHqJx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnvznFa7HqJy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1GCriJHHqJy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
