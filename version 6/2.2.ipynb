{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import random\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.operators.sampling.rnd import BinaryRandomSampling\n",
        "from pymoo.operators.crossover.pntx import TwoPointCrossover\n",
        "from pymoo.operators.mutation.bitflip import BitflipMutation\n",
        "from pymoo.operators.selection.tournament import TournamentSelection\n",
        "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
        "from tensorflow.keras.models import clone_model\n",
        "from pymoo.core.problem import Problem\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLASSES\n",
        "\n",
        "class Server:\n",
        "    def __init__(self, devices_list):\n",
        "        self.model = Server.create_model()\n",
        "        self.current_learning_iteration = 0\n",
        "        self.LAST_WEIGHTS_SENT_FOR_ALL_DEVICES = []\n",
        "        self.x_test_global = []\n",
        "        self.y_test_global = []\n",
        "        self.devices = devices_list\n",
        "\n",
        "    # def evaluate(self, verbose = 1):\n",
        "    #     test_loss, test_acc = self.model.evaluate(self.x_test_global, self.y_test_global, verbose)\n",
        "    #     return test_loss, test_acc\n",
        "    \n",
        "    def evaluate(self, x_test=None, y_test=None, verbose = 1):\n",
        "        if x_test is None and y_test is None:\n",
        "            test_loss, test_acc = self.model.evaluate(self.x_test_global, self.y_test_global, verbose)\n",
        "            return test_loss, test_acc\n",
        "        test_loss, test_acc = self.model.evaluate(x_test, y_test, verbose=verbose)\n",
        "        return test_loss, test_acc\n",
        "    \n",
        "    def get_weights(self):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def set_aggregated_weight(self):\n",
        "        self.model.set_weight(Server.aggregate_weights())\n",
        "    \n",
        "    def give_global_model_weights_to_bitstring_devices(self, bitstring):\n",
        "        for device in self.devices:\n",
        "            if int(bitstring[int(device.id)]) == 1:\n",
        "                device.model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def create_model():\n",
        "        model = keras.Sequential([\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(10, activation='softmax')\n",
        "        ])\n",
        "        model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "                        # new\n",
        "                        loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    def aggregate_weights(self, bitstring):\n",
        "        \"\"\"Computes the weighted average of model weights from all devices and updates the global model.\"\"\"\n",
        "        def sum_all_nested_lists(list_of_lists):\n",
        "            def recursive_sum(lists):\n",
        "                if isinstance(lists[0], list):\n",
        "                    return [recursive_sum([lst[i] for lst in lists]) for i in range(len(lists[0]))]\n",
        "                else:\n",
        "                    return sum(lists)\n",
        "            \n",
        "            return recursive_sum(list_of_lists)\n",
        "\n",
        "        def multiply_nested_list(self, lst, factor):\n",
        "            result = []\n",
        "            for item in lst:\n",
        "                if isinstance(item, list):\n",
        "                    # Recursively handle sublists\n",
        "                    result.append(self.multiply_nested_list(item, factor))\n",
        "                else:\n",
        "                    # Multiply number\n",
        "                    result.append(item * factor)\n",
        "            return result\n",
        "        \n",
        "        selected_devices = []\n",
        "        for device in self.devices:\n",
        "            if int(bitstring[int(device.id)]) == 1:\n",
        "                selected_devices.append(device)\n",
        "        \n",
        "        num_devices = len(selected_devices)\n",
        "        if num_devices == 0:\n",
        "            print(\"No devices available for aggregation.\")\n",
        "            return\n",
        "\n",
        "        device_weights_all_layers = []\n",
        "        device_participation_ratio = []\n",
        "        data_lengths = []\n",
        "\n",
        "        for device in selected_devices:\n",
        "            device_weights_all_layers.append(device.model.get_weights())\n",
        "            print(\"*******************\")\n",
        "            print(device.id)\n",
        "            device_participation_ratio.append(device.last_round_participated / self.current_learning_iteration)\n",
        "            print(\"this device's participation ratio:\")\n",
        "            print(device.last_round_participated / self.current_learning_iteration)\n",
        "            data_lengths.append(len(device.data[0]))\n",
        "            print(\"this device's data to all ratio:\")\n",
        "            print(len(device.data[0])/60000.0)\n",
        "\n",
        "        sum_data = 0\n",
        "        for data_len in data_lengths:\n",
        "            sum_data += data_len\n",
        "        \n",
        "        data_fractions = []\n",
        "        for device in selected_devices:\n",
        "            data_fractions.append(len(device.data[0])/float(sum_data))\n",
        "\n",
        "        aggregated_weights_devices = []\n",
        "        for d in range(len(selected_devices)):\n",
        "            aggregated_weights_devices.append(multiply_nested_list(self, selected_devices[d].model.get_weights(), data_fractions[d]*device_participation_ratio[d]))\n",
        "        \n",
        "        aggregated_weights = sum_all_nested_lists(aggregated_weights_devices)\n",
        "        # TODO: Weighted multiplication for each node in each layer of the neural network of the received devices and then summing\n",
        "        #       the related parts together so that we get a full weighted average of all these devices' models\n",
        "\n",
        "        print(\"Aggregated weights:\")\n",
        "        for layer_idx, layer_weights in enumerate(aggregated_weights):\n",
        "            print(f\"Layer {layer_idx}: {layer_weights.shape}\")\n",
        "        return aggregated_weights\n",
        "\n",
        "\n",
        "class Device:\n",
        "    def __init__(self, id, ram, storage, cpu, bandwidth, battery, charging):\n",
        "        self.id = id\n",
        "        self.ram = ram\n",
        "        self.storage = storage\n",
        "        self.cpu = cpu\n",
        "        self.bandwidth = bandwidth\n",
        "        self.battery = battery\n",
        "        self.charging = charging\n",
        "        self.model = Server.create_model()\n",
        "        self.last_round_participated = 0\n",
        "        self.data = None  # Placeholder for dataset partition\n",
        "        self.test_data = None\n",
        "        self.number_of_times_fitted = 0\n",
        "\n",
        "\n",
        "class FederatedLearningProblem(Problem):\n",
        "    def __init__(self, devices, server: Server):\n",
        "        super().__init__(\n",
        "            n_var=len(devices),         # Number of variables (bitstring length)\n",
        "            n_obj=3,                   # Number of objectives\n",
        "            n_constr=0,                # No constraints\n",
        "            xl=np.zeros(len(devices)),  # Lower bound (0)\n",
        "            xu=np.ones(len(devices)),   # Upper bound (1)\n",
        "            type_var=np.bool_          # Binary variables (bitstrings)\n",
        "        )\n",
        "        self.devices = devices\n",
        "        self.server = server\n",
        "        self.x_test_global = server.x_test_global\n",
        "        self.y_test_global = server.y_test_global\n",
        "\n",
        "        # Save the initial global model weights\n",
        "        self.initial_global_weights = server.get_weights()\n",
        "\n",
        "    def _evaluate(self, X, out, *args, **kwargs):\n",
        "        \"\"\"Evaluates objective values for each solution in the population.\"\"\"\n",
        "        num_solutions = len(X)\n",
        "        F = np.zeros((num_solutions, 3))  # Initialize objective matrix\n",
        "\n",
        "        for i, bitstring in enumerate(X):\n",
        "            print(f\"evaluating: {bitstring}\")\n",
        "            # TODO: check bitstring type\n",
        "            # Reset the global model to its initial state\n",
        "            # Update device participation based on the bitstring\n",
        "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if int(bit) == 1]\n",
        "            \n",
        "            # Objective 1: Hardware Objectives (maximize)\n",
        "            hardware_score = 0.0\n",
        "            for device in selected_devices:\n",
        "                device_hardware_score = float(6 - (device.ram + device.storage + device.cpu + device.bandwidth + device.battery + device.charging)) / 6.0\n",
        "                hardware_score += device_hardware_score\n",
        "\n",
        "            F[i, 0] = hardware_score  # Minimize (negative of hardware score)\n",
        "\n",
        "            fairness_score = 0\n",
        "            for device in self.devices:\n",
        "                if bitstring[int(device.id)] == 1:\n",
        "                    # new\n",
        "                    _, accuracy = self.server.evaluate(device.test_data[0], device.test_data[1], verbose=0)\n",
        "                    fairness_score += accuracy\n",
        "\n",
        "            F[i, 1] = fairness_score/float(len(selected_devices))  # Minimize (negative of fairness score)  # Added (/Selected Devices) to normalize between 0 and 1\n",
        "            \n",
        "            # Objective 3: Global Model Accuracy (Performance) (maximize)\n",
        "            temp_global_model = Server.create_model()\n",
        "            # new\n",
        "            temp_global_model.set_weights(self.performance_objective_aggregation(selected_devices))\n",
        "            _, global_accuracy = temp_global_model.evaluate(self.server.x_test_global, self.server.y_test_global, verbose=0)\n",
        "            F[i, 2] = 1 - global_accuracy  # Minimize (1 - accuracy)\n",
        "            \n",
        "        out[\"F\"] = F  # Set the objective values\n",
        "\n",
        "    def performance_objective_aggregation(self, selected_devices):\n",
        "\n",
        "        def sum_all_nested_lists(list_of_lists):\n",
        "            def recursive_sum(lists):\n",
        "                if isinstance(lists[0], list):\n",
        "                    return [recursive_sum([lst[i] for lst in lists]) for i in range(len(lists[0]))]\n",
        "                else:\n",
        "                    return sum(lists)\n",
        "            \n",
        "            return recursive_sum(list_of_lists)\n",
        "\n",
        "        def multiply_nested_list(lst, factor):\n",
        "            result = []\n",
        "            for item in lst:\n",
        "                if isinstance(item, list):\n",
        "                    # Recursively handle sublists\n",
        "                    result.append(multiply_nested_list(item, factor))\n",
        "                else:\n",
        "                    # Multiply number\n",
        "                    result.append(item * factor)\n",
        "            return result\n",
        "\n",
        "        num_devices = len(selected_devices)\n",
        "        if num_devices == 0:\n",
        "            print(\"No devices available for aggregation.\")\n",
        "            return\n",
        "\n",
        "        device_weights_all_layers = []\n",
        "        device_participation_ratio = []\n",
        "        data_lengths = []\n",
        "\n",
        "        for device in selected_devices:\n",
        "            device_weights_all_layers.append(self.server.LAST_WEIGHTS_SENT_FOR_ALL_DEVICES[int(device.id)])\n",
        "            # print(\"*******************\")\n",
        "            # print(device.id)\n",
        "            device_participation_ratio.append(device.last_round_participated / self.server.current_learning_iteration)\n",
        "            # print(\"this device's participation ratio:\")\n",
        "            # print(device.last_round_participated / self.server.current_learning_iteration)\n",
        "            \n",
        "            data_lengths.append(len(device.data[0]))\n",
        "            # print(\"this device's data to all ratio:\")\n",
        "            # print(len(device.data[0])/60000.0)\n",
        "\n",
        "        sum_data = 0\n",
        "        for data_len in data_lengths:\n",
        "            sum_data += data_len\n",
        "        \n",
        "        data_fractions = []\n",
        "        for device in selected_devices:\n",
        "            data_fractions.append(len(device.data[0])/float(sum_data))\n",
        "\n",
        "        aggregated_weights_devices = []\n",
        "        for d in range(len(selected_devices)):\n",
        "            aggregated_weights_devices.append(multiply_nested_list(self.server.LAST_WEIGHTS_SENT_FOR_ALL_DEVICES[int(selected_devices[d].id)], data_fractions[d]*device_participation_ratio[d]))\n",
        "        \n",
        "        aggregated_weights = sum_all_nested_lists(aggregated_weights_devices)\n",
        "        # TODO: Weighted multiplication for each node in each layer of the neural network of the received devices and then summing\n",
        "        #       the related parts together so that we get a full weighted average of all these devices' models\n",
        "\n",
        "        # print(\"Aggregated weights:\")\n",
        "        # for layer_idx, layer_weights in enumerate(aggregated_weights):\n",
        "        #     print(f\"Layer {layer_idx}: {layer_weights.shape}\")\n",
        "        return aggregated_weights\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions\n",
        "\n",
        "def fit_bitstring_devices(bitstring, server: Server):\n",
        "    '''\n",
        "    server: for using its \"current_learning_iteration\" variable\n",
        "    '''\n",
        "    print(1)\n",
        "    # global current_learning_iteration\n",
        "    # global LAST_WEIGHTS_SENT_FOR_ALL_DEVICES\n",
        "    server.current_learning_iteration += 1\n",
        "    for device in server.devices:\n",
        "        if bitstring[int(device.id)] == 1:\n",
        "            device.model.fit(device.data[0], device.data[1], epochs=7, verbose=1)\n",
        "            print(device.id)\n",
        "            device.last_round_participated = server.current_learning_iteration\n",
        "            server.LAST_WEIGHTS_SENT_FOR_ALL_DEVICES[int(device.id)] = device.model.get_weights()\n",
        "            device.number_of_times_fitted += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from CSV\n",
        "csv_file = 'devices.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "# Convert CSV rows into device objects\n",
        "devices = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    device = Device(\n",
        "        row['id'], row['ram'], row['storage'], row['cpu'], row['bandwidth'], row['battery'],\n",
        "        row.get('charging', 0)\n",
        "    )\n",
        "    devices.append(device)\n",
        "\n",
        "\n",
        "# LIMIT TO 30 DEVICES\n",
        "devices = devices[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Object Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global Model\n",
        "server = Server(devices_list=devices)\n",
        "server.LAST_WEIGHTS_SENT_FOR_ALL_DEVICES = [None for _ in range(len(devices))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Data Among Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset  # new\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Convert labels to categorical (one-hot encoded)\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Normalize data and reshape for CNN\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)  # Add channel dimension\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1)  # Add channel dimension\n",
        "\n",
        "# Shuffle data\n",
        "# indices = np.arange(len(x_train))\n",
        "# np.random.shuffle(indices)\n",
        "# x_train, y_train = x_train[indices], y_train[indices]\n",
        "\n",
        "# Correct test split\n",
        "split_index = int(0.8 * len(x_test))\n",
        "x_test_devices, y_test_devices = x_test[:split_index], y_test[:split_index]\n",
        "server.x_test_global, server.y_test_global = x_test[split_index:], y_test[split_index:]\n",
        "\n",
        "# Training data (for devices)\n",
        "x_train_devices, y_train_devices = x_train, y_train\n",
        "\n",
        "# Split training data among devices\n",
        "num_devices = len(devices)\n",
        "split_size = len(x_train_devices) // num_devices\n",
        "\n",
        "for i, device in enumerate(devices):\n",
        "    start = i * split_size\n",
        "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_train_devices)\n",
        "    device.data = (x_train_devices[start:end], y_train_devices[start:end])\n",
        "\n",
        "# Split test data (device-level)\n",
        "split_size = len(x_test_devices) // num_devices\n",
        "\n",
        "for i, device in enumerate(devices):\n",
        "    start = i * split_size\n",
        "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_test_devices)\n",
        "    device.test_data = (x_test_devices[start:end], y_test_devices[start:end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Other Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RtoM8SQmHqJs",
        "outputId": "646ea129-2068-4c84-9813-ec9b268b8ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1272 - loss: 2.2839\n",
            "Global Model Accuracy: 0.1185\n",
            "------------------------------------------------------------\n",
            "1\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2806 - loss: 2.2302\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6950 - loss: 1.7668\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7853 - loss: 1.0465\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8546 - loss: 0.6625\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8708 - loss: 0.5021\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8745 - loss: 0.4515\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8881 - loss: 0.4129\n",
            "0.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3065 - loss: 2.2268\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7043 - loss: 1.7273\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8152 - loss: 0.9807\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8636 - loss: 0.5911\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8903 - loss: 0.4559\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9054 - loss: 0.3545\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9288 - loss: 0.3223\n",
            "1.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2603 - loss: 2.2255\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6677 - loss: 1.7134\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8014 - loss: 0.9600\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8638 - loss: 0.6215\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8858 - loss: 0.4665\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8900 - loss: 0.4298\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8900 - loss: 0.3669\n",
            "2.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2956 - loss: 2.2297\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6849 - loss: 1.7776\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7822 - loss: 1.0444\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8372 - loss: 0.6707\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8714 - loss: 0.4965\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8841 - loss: 0.4300\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8922 - loss: 0.4118\n",
            "3.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3046 - loss: 2.2286\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6543 - loss: 1.7361\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8021 - loss: 0.9939\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8394 - loss: 0.6555\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8623 - loss: 0.5118\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8758 - loss: 0.4677\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8882 - loss: 0.4285\n",
            "4.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3023 - loss: 2.2233\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6693 - loss: 1.7122\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8156 - loss: 0.9367\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8440 - loss: 0.6141\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8647 - loss: 0.4744\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8988 - loss: 0.3735\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9048 - loss: 0.3472\n",
            "5.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2694 - loss: 2.2237\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6586 - loss: 1.7653\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7718 - loss: 1.0464\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8489 - loss: 0.6492\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8664 - loss: 0.5406\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8793 - loss: 0.4422\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8877 - loss: 0.3932\n",
            "6.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2526 - loss: 2.2400\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6282 - loss: 1.8019\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7798 - loss: 1.0874\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8308 - loss: 0.7214\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8546 - loss: 0.5528\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8718 - loss: 0.4717\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8946 - loss: 0.4198\n",
            "7.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2597 - loss: 2.2314\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5773 - loss: 1.8156\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7778 - loss: 1.0964\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8422 - loss: 0.6931\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8490 - loss: 0.5633\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8821 - loss: 0.4562\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8826 - loss: 0.4065\n",
            "8.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2810 - loss: 2.2315\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6802 - loss: 1.7694\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7986 - loss: 0.9889\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8759 - loss: 0.5841\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8993 - loss: 0.4479\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9059 - loss: 0.3957\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9054 - loss: 0.3374\n",
            "9.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.2884 - loss: 2.2250\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6581 - loss: 1.7298\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8032 - loss: 0.9770\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.8704 - loss: 0.5982\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8777 - loss: 0.4819\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9062 - loss: 0.3913\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9105 - loss: 0.3549\n",
            "10.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.2878 - loss: 2.2334\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.6834 - loss: 1.7804\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7832 - loss: 1.0495\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8372 - loss: 0.6561\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8736 - loss: 0.4878\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8823 - loss: 0.4171\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8882 - loss: 0.4045\n",
            "11.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.2825 - loss: 2.2279\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7037 - loss: 1.7682\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.7946 - loss: 0.9912\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8375 - loss: 0.6472\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.8641 - loss: 0.5034\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8859 - loss: 0.4161\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9007 - loss: 0.3531\n",
            "12.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 0.2768 - loss: 2.2304\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.6617 - loss: 1.7636\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7926 - loss: 1.0135\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.8690 - loss: 0.6217\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8761 - loss: 0.5119\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - accuracy: 0.8852 - loss: 0.4177\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.9132 - loss: 0.3506\n",
            "13.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.2735 - loss: 2.2288\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.6227 - loss: 1.7562\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7870 - loss: 1.0546\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8338 - loss: 0.6889\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8817 - loss: 0.5044\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8832 - loss: 0.4447\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8776 - loss: 0.4282\n",
            "14.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.2720 - loss: 2.2399\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5903 - loss: 1.8587\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7552 - loss: 1.1790\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8202 - loss: 0.7778\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8587 - loss: 0.5930\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8789 - loss: 0.4657\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8847 - loss: 0.4296\n",
            "15.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2409 - loss: 2.2352\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6524 - loss: 1.8039\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7862 - loss: 1.0771\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8533 - loss: 0.6804\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8699 - loss: 0.5146\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8913 - loss: 0.4099\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9047 - loss: 0.3735\n",
            "16.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2498 - loss: 2.2311\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6450 - loss: 1.7955\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7845 - loss: 1.0528\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8457 - loss: 0.6415\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8819 - loss: 0.4868\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8900 - loss: 0.4115\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9089 - loss: 0.3558\n",
            "17.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2900 - loss: 2.2279\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.6446 - loss: 1.7622\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.7854 - loss: 1.0153\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8375 - loss: 0.6652\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8637 - loss: 0.5117\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.8831 - loss: 0.4397\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8944 - loss: 0.3740\n",
            "18.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.2649 - loss: 2.2351\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.6677 - loss: 1.7706\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7854 - loss: 1.0102\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8452 - loss: 0.6582\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.8701 - loss: 0.5036\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8749 - loss: 0.4549\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9039 - loss: 0.3974\n",
            "19.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.2795 - loss: 2.2314\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.6406 - loss: 1.7960\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8158 - loss: 1.0512\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8563 - loss: 0.6212\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8828 - loss: 0.4833\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8926 - loss: 0.4326\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9093 - loss: 0.3443\n",
            "20.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.2780 - loss: 2.2301\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.6667 - loss: 1.8032\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7723 - loss: 1.0679\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8377 - loss: 0.6639\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8616 - loss: 0.5248\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8763 - loss: 0.4644\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.8909 - loss: 0.3859\n",
            "21.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.2601 - loss: 2.2326\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.6294 - loss: 1.7985\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7979 - loss: 1.0626\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8479 - loss: 0.6679\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.8709 - loss: 0.5374\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8730 - loss: 0.4557\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.8901 - loss: 0.3882\n",
            "22.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.2889 - loss: 2.2347\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.6411 - loss: 1.7849\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8086 - loss: 1.0339\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8223 - loss: 0.6847\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8639 - loss: 0.5115\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8642 - loss: 0.4570\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8782 - loss: 0.4006\n",
            "23.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.2716 - loss: 2.2353\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.6538 - loss: 1.7947\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.7908 - loss: 1.0571\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8453 - loss: 0.6537\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8583 - loss: 0.5189\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8773 - loss: 0.4426\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9073 - loss: 0.3738\n",
            "24.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.2839 - loss: 2.2351\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.6748 - loss: 1.7831\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.8219 - loss: 1.0100\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8587 - loss: 0.6331\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8789 - loss: 0.4949\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8901 - loss: 0.4501\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9084 - loss: 0.3753\n",
            "25.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.2768 - loss: 2.2310\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.6677 - loss: 1.7588\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8046 - loss: 1.0127\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.8420 - loss: 0.6754\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8483 - loss: 0.5412\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8796 - loss: 0.4500\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8978 - loss: 0.3887\n",
            "26.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.2652 - loss: 2.2315\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6674 - loss: 1.7417\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8064 - loss: 0.9624\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.8509 - loss: 0.6142\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8765 - loss: 0.4667\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.8882 - loss: 0.3922\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9003 - loss: 0.3873\n",
            "27.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.2841 - loss: 2.2334\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.7020 - loss: 1.7618\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.8112 - loss: 0.9967\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.8668 - loss: 0.6077\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8948 - loss: 0.4406\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9058 - loss: 0.3739\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.9094 - loss: 0.3399\n",
            "28.0\n",
            "Epoch 1/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.3211 - loss: 2.2104\n",
            "Epoch 2/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.7813 - loss: 1.5931\n",
            "Epoch 3/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.8884 - loss: 0.7307\n",
            "Epoch 4/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9079 - loss: 0.4392\n",
            "Epoch 5/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9288 - loss: 0.3209\n",
            "Epoch 6/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.9380 - loss: 0.2529\n",
            "Epoch 7/7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.9423 - loss: 0.2311\n",
            "29.0\n",
            "*******************\n",
            "0.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "1.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "2.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "3.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "4.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "5.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "6.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "7.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "8.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "9.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "10.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "11.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "12.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "13.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "14.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "15.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "16.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "17.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "18.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "19.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "20.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "21.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "22.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "23.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "24.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "25.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "26.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "27.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "28.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "29.0\n",
            "this device's participation ratio:\n",
            "1.0\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "Aggregated weights:\n",
            "Layer 0: (3, 3, 1, 32)\n",
            "Layer 1: (32,)\n",
            "Layer 2: (5408, 128)\n",
            "Layer 3: (128,)\n",
            "Layer 4: (128, 10)\n",
            "Layer 5: (10,)\n",
            "------------------------------------------------------------\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9544 - loss: 0.2278\n",
            "Global Model Accuracy: 0.9390\n"
          ]
        }
      ],
      "source": [
        "# First Iteration\n",
        "bitstring = [1 for _ in range(len(devices))]\n",
        "print(bitstring)\n",
        "\n",
        "# global model sends its weights to all devices\n",
        "server.give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "\n",
        "test_loss, test_acc = server.evaluate(verbose=0)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "fit_bitstring_devices(bitstring, server)\n",
        "server.model.set_weights(server.aggregate_weights(bitstring))\n",
        "print(\"------------------------------------------------------------\")\n",
        "test_loss, test_acc = server.evaluate(verbose=0)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XliqF4L-Sd",
        "outputId": "9805c23f-170d-4535-c652-97e4b6b9203f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymoo in d:\\github project\\fl\\venv\\lib\\site-packages (0.6.1.3)\n",
            "Requirement already satisfied: numpy>=1.15 in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1 in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (1.14.1)\n",
            "Requirement already satisfied: matplotlib>=3 in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (1.7.0)\n",
            "Requirement already satisfied: cma==3.2.2 in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (3.2.2)\n",
            "Requirement already satisfied: alive-progress in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (3.2.0)\n",
            "Requirement already satisfied: dill in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (0.3.9)\n",
            "Requirement already satisfied: Deprecated in d:\\github project\\fl\\venv\\lib\\site-packages (from pymoo) (1.2.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\github project\\fl\\venv\\lib\\site-packages (from matplotlib>=3->pymoo) (2.9.0.post0)\n",
            "Requirement already satisfied: about-time==4.2.1 in d:\\github project\\fl\\venv\\lib\\site-packages (from alive-progress->pymoo) (4.2.1)\n",
            "Requirement already satisfied: grapheme==0.6.0 in d:\\github project\\fl\\venv\\lib\\site-packages (from alive-progress->pymoo) (0.6.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in d:\\github project\\fl\\venv\\lib\\site-packages (from Deprecated->pymoo) (1.17.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\github project\\fl\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install pymoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NSGA2 Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiEDWLPjHqJt"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "POPULATION_SIZE = 50\n",
        "NUM_GENERATIONS = 3\n",
        "NUM_ROUNDS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x60J9IXFHqJt"
      },
      "outputs": [],
      "source": [
        "# Object Initializations\n",
        "\n",
        "problem = FederatedLearningProblem(\n",
        "    devices=devices,\n",
        "    server=server\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Configure NSGA-II Algorithm\n",
        "algorithm = NSGA2(\n",
        "    pop_size=POPULATION_SIZE,\n",
        "    sampling=BinaryRandomSampling(),      # Random bitstrings\n",
        "    crossover=TwoPointCrossover(),        # Two-point crossover\n",
        "    mutation=BitflipMutation(),           # Bit flip mutation\n",
        "    eliminate_duplicates=True             # Avoid duplicate solutions\n",
        ")\n",
        "\n",
        "server.current_learning_iteration += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhEjXpgNGaxb",
        "outputId": "f2bb719a-aa35-4c87-a62a-3c17b675a8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 28, 28, 1)\n",
            "(2000, 10)\n",
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n"
          ]
        }
      ],
      "source": [
        "# DEBUG:\n",
        "print(server.x_test_global.shape)\n",
        "print(server.y_test_global.shape)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "FM0EBc17HqJu",
        "outputId": "b0ea08e3-fb08-467b-f648-29dc724a5ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GLOBAL MODEL BEFORE OPTIMIZATION\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - accuracy: 0.9553 - loss: 0.2261\n",
            "(0.2743990123271942, 0.9390000104904175)\n",
            "evaluating: [False False  True  True False  True  True False  True False  True False\n",
            " False  True  True False False False False False  True False  True  True\n",
            " False False False False  True False]\n",
            "evaluating: [ True False  True False False  True False False  True  True  True False\n",
            " False  True  True False False False  True  True  True  True False False\n",
            " False  True  True False  True  True]\n",
            "evaluating: [ True  True  True  True  True False  True False False False  True  True\n",
            " False False  True False False  True False  True False  True  True  True\n",
            " False False  True  True  True False]\n",
            "evaluating: [False  True  True False False False False False  True  True False False\n",
            " False False  True  True  True  True False  True False False  True  True\n",
            " False False False  True False False]\n",
            "evaluating: [ True  True  True  True False False False  True False  True  True False\n",
            "  True  True False False  True False  True  True  True False False  True\n",
            "  True  True False  True  True  True]\n",
            "evaluating: [False  True False  True  True False  True False False False False False\n",
            " False  True  True False  True  True False False  True  True False  True\n",
            " False  True  True False  True  True]\n",
            "evaluating: [ True False  True  True False False False  True False  True False False\n",
            " False  True False  True  True False False False False False  True False\n",
            "  True False False False  True  True]\n",
            "evaluating: [ True  True  True  True  True False False False False  True  True  True\n",
            " False False False False False False False  True False  True False False\n",
            "  True  True  True False False False]\n",
            "evaluating: [ True  True  True  True  True  True False  True False False  True False\n",
            " False False  True False  True False  True  True  True False False False\n",
            "  True False False  True  True  True]\n",
            "evaluating: [False False False False False False False  True False False False  True\n",
            "  True False False False False False  True  True False  True False False\n",
            " False  True False False  True  True]\n",
            "evaluating: [False False False  True  True False False  True  True  True False  True\n",
            "  True False  True  True False  True False False  True False False False\n",
            "  True  True False  True  True False]\n",
            "evaluating: [False False  True  True False False  True False False  True False False\n",
            " False False False  True False  True False  True  True False False False\n",
            "  True  True False False False False]\n",
            "evaluating: [ True False False False False  True False  True  True  True  True False\n",
            " False False False False False False  True  True False False False  True\n",
            "  True  True  True False  True  True]\n",
            "evaluating: [False  True  True  True False  True  True  True  True False False False\n",
            " False  True  True False  True  True False  True False False  True False\n",
            "  True  True False False False False]\n",
            "evaluating: [ True False  True False False False False  True  True  True  True  True\n",
            "  True False  True False  True  True False  True  True  True False False\n",
            "  True False False False False  True]\n",
            "evaluating: [False  True False  True False  True  True False False  True False False\n",
            " False False False False  True False  True  True  True False  True  True\n",
            " False False False  True False  True]\n",
            "evaluating: [ True  True False False  True False False  True False False False  True\n",
            " False False  True False  True  True  True False False False  True  True\n",
            " False  True  True False False  True]\n",
            "evaluating: [ True False  True  True  True False  True False  True False False  True\n",
            " False False  True False False False False  True False False False  True\n",
            " False False  True  True  True False]\n",
            "evaluating: [False False False False  True  True  True  True  True False  True False\n",
            "  True  True False  True False  True False False  True False  True  True\n",
            " False  True False  True  True  True]\n",
            "evaluating: [ True False False  True  True  True  True  True False  True False False\n",
            "  True  True False  True False False False  True False  True  True False\n",
            "  True False False  True  True  True]\n",
            "evaluating: [ True False  True False False False False False False  True  True False\n",
            "  True  True  True  True False  True False False False  True False  True\n",
            "  True  True  True False False False]\n",
            "evaluating: [ True  True False  True False  True False False False  True False  True\n",
            " False  True False False False  True False  True False False  True False\n",
            " False  True False False False  True]\n",
            "evaluating: [ True  True False  True False  True False False  True  True  True  True\n",
            "  True False  True  True  True False  True  True False False False False\n",
            "  True False False False False False]\n",
            "evaluating: [False False  True False False False False  True  True  True False False\n",
            " False False  True False False  True False False  True False  True False\n",
            "  True  True  True False False  True]\n",
            "evaluating: [False  True  True False  True False  True False  True  True False  True\n",
            " False  True  True False  True  True  True False  True False  True  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False  True  True False False False False False  True  True False  True\n",
            "  True False False  True False  True  True  True False False  True  True\n",
            "  True False False False False False]\n",
            "evaluating: [ True False  True  True False  True False  True  True False False  True\n",
            " False False  True  True  True  True  True False False  True  True False\n",
            " False False False False False  True]\n",
            "evaluating: [False False  True  True False False False  True  True False False False\n",
            "  True  True  True False False False False  True  True False  True  True\n",
            " False  True False False  True False]\n",
            "evaluating: [ True  True  True False False False  True False False False False  True\n",
            " False False  True False  True False  True False False False False  True\n",
            " False False False  True False False]\n",
            "evaluating: [False  True  True False  True False  True False  True  True  True  True\n",
            "  True  True False False False  True False False False False  True False\n",
            "  True False  True  True  True False]\n",
            "evaluating: [False  True False False False False False  True  True False False  True\n",
            "  True  True  True False  True  True False False  True False  True  True\n",
            "  True  True  True  True  True  True]\n",
            "evaluating: [ True False  True  True  True False  True False False  True False False\n",
            "  True False  True False False False  True False False  True  True  True\n",
            "  True  True  True  True False False]\n",
            "evaluating: [False  True  True  True  True  True False False False False  True  True\n",
            " False False  True  True  True False  True False False  True  True  True\n",
            "  True  True False False False  True]\n",
            "evaluating: [ True  True False  True False False False  True  True False  True  True\n",
            "  True  True  True False False False  True  True  True  True False  True\n",
            " False False  True False  True  True]\n",
            "evaluating: [False False False False False False  True False  True  True  True  True\n",
            " False False  True  True False  True  True  True  True False False False\n",
            "  True  True  True  True False False]\n",
            "evaluating: [ True False False  True  True False  True  True False False  True False\n",
            " False  True False  True False False  True False False  True  True False\n",
            " False False False False False  True]\n",
            "evaluating: [ True  True  True False  True False False  True False False  True  True\n",
            "  True  True False False False  True  True  True False  True False  True\n",
            " False  True False  True  True False]\n",
            "evaluating: [False False False False  True False  True False False  True False False\n",
            "  True False  True False  True False False  True  True False False False\n",
            " False  True False  True  True  True]\n",
            "evaluating: [ True  True False  True False  True  True  True  True False False False\n",
            " False False  True False False  True  True False  True False  True False\n",
            "  True  True False False  True  True]\n",
            "evaluating: [ True False  True  True False False  True  True False  True  True False\n",
            "  True  True False False False  True False False False False  True  True\n",
            " False False False False  True  True]\n",
            "evaluating: [False  True False False False False False False False  True False  True\n",
            "  True False  True False  True False  True  True False  True  True False\n",
            "  True False False  True  True False]\n",
            "evaluating: [ True False False  True False False  True False  True  True False  True\n",
            " False  True False False  True  True False False  True  True False False\n",
            "  True  True False  True False  True]\n",
            "evaluating: [ True False False False False  True False False  True  True False False\n",
            " False False False  True  True  True False False  True  True  True  True\n",
            "  True False False False False False]\n",
            "evaluating: [ True  True  True False False  True  True  True  True False False False\n",
            " False False  True  True  True False False False False  True False False\n",
            " False False False  True  True  True]\n",
            "evaluating: [False False  True  True False False  True  True  True False False  True\n",
            " False  True  True False  True  True  True False False  True False False\n",
            " False  True False False False False]\n",
            "evaluating: [ True  True  True  True False  True  True False False False False  True\n",
            " False False False False False  True  True False  True  True  True  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False False False False  True False False False  True False False  True\n",
            "  True  True False False  True  True  True  True False  True  True False\n",
            "  True  True False  True False  True]\n",
            "evaluating: [False  True False  True  True False False  True  True  True  True False\n",
            " False False  True  True False False False  True  True  True  True False\n",
            " False False False False  True False]\n",
            "evaluating: [ True  True False  True False False  True False  True False False  True\n",
            "  True  True  True False False  True False  True False  True False False\n",
            " False  True  True False  True  True]\n",
            "evaluating: [False  True  True False False False  True  True False False  True  True\n",
            "  True False  True  True False False False  True  True  True  True  True\n",
            "  True  True  True  True  True False]\n",
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      8 |             - |             -\n",
            "evaluating: [ True False False  True False  True False  True  True  True  True False\n",
            " False False False False False False  True  True False False False  True\n",
            "  True  True  True False  True  True]\n",
            "evaluating: [False  True  True False False False False  True False False  True  True\n",
            "  True False  True  True False False False  True  True  True  True  True\n",
            "  True  True  True  True  True False]\n",
            "evaluating: [ True  True False  True False  True  True  True  True False False False\n",
            " False False  True False False  True False False  True False  True False\n",
            "  True  True  True False  True  True]\n",
            "evaluating: [ True  True  True  True False False False  True False False False  True\n",
            " False False  True False False False False  True  True False False  True\n",
            "  True  True False  True  True  True]\n",
            "evaluating: [False False False  True  True  True  True  True  True False  True False\n",
            " False  True False  True False  True False False  True False  True  True\n",
            " False  True False  True  True  True]\n",
            "evaluating: [ True False False  True  True  True  True  True False  True False False\n",
            "  True  True False  True False False False  True False  True False  True\n",
            " False  True False  True  True  True]\n",
            "evaluating: [False  True  True False  True False False False False  True False  True\n",
            " False  True  True False  True  True  True False  True False  True  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False  True  True False False False False False False  True False False\n",
            " False False False  True False  True False  True  True False False False\n",
            "  True  True False False False False]\n",
            "evaluating: [False  True  True False False  True  True  True  True False False False\n",
            " False False  True  True  True  True False  True False False  True  True\n",
            " False False False  True False False]\n",
            "evaluating: [ True False  True  True False  True  True  True  True False False  True\n",
            " False False  True  True  True  True  True False False  True  True False\n",
            " False False False False False  True]\n",
            "evaluating: [False  True False  True  True False False  True False  True  True False\n",
            " False False  True  True False  True False False  True False False  True\n",
            "  True  True  True False  True False]\n",
            "evaluating: [ True  True False  True False False False  True  True  True False  True\n",
            "  True False  True  True False  True False False  True  True False  True\n",
            " False False  True False  True  True]\n",
            "evaluating: [ True False False False False  True False  True  True False  True False\n",
            " False False False False False  True  True  True False False False  True\n",
            "  True  True  True  True  True  True]\n",
            "evaluating: [False  True  True False False False  True False  True  True False  True\n",
            " False  True  True False False  True  True False  True False  True  True\n",
            " False False False  True  True False]\n",
            "evaluating: [ True False  True False False False False False False  True  True False\n",
            "  True  True  True  True False  True False False False  True False False\n",
            "  True  True False False False False]\n",
            "evaluating: [ True  True  True  True False  True  True False False False False  True\n",
            " False False False False False  True False False False  True  True  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False  True False False False False False  True  True False False  True\n",
            "  True  True  True False  True False False False  True False  True  True\n",
            "  True  True  True  True  True  True]\n",
            "evaluating: [ True False  True False False False  True False False False False  True\n",
            " False False  True False  True False  True False False False False  True\n",
            " False False False False False  True]\n",
            "evaluating: [False  True False  True False  True  True  True False  True False False\n",
            " False  True False False  True False  True  True  True  True  True  True\n",
            " False False False  True False  True]\n",
            "evaluating: [ True False  True  True  True False  True  True False False  True False\n",
            "  True  True False  True False  True False False  True False  True  True\n",
            " False  True  True  True  True False]\n",
            "evaluating: [ True  True  True  True  True False False False False  True  True  True\n",
            " False False False False False False False False False  True False False\n",
            "  True  True  True False False False]\n",
            "evaluating: [False False  True  True False False False  True  True False False False\n",
            "  True  True  True False False False False  True  True False  True  True\n",
            "  True False False False False False]\n",
            "evaluating: [False  True False  True  True  True False False  True  True  True  True\n",
            "  True  True  True  True  True False  True  True False  True False False\n",
            "  True  True False  True False False]\n",
            "evaluating: [ True  True  True False False  True  True False False  True False False\n",
            " False False False  True False  True False  True  True False False False\n",
            "  True  True False False  True  True]\n",
            "evaluating: [False  True  True False False  True  True  True  True False False False\n",
            " False  True  True False  True  True False  True False False  True False\n",
            "  True  True False False False False]\n",
            "evaluating: [ True False False  True  True False  True  True False False  True False\n",
            " False  True False  True False False  True False  True  True  True False\n",
            " False  True False False False  True]\n",
            "evaluating: [ True  True  True  True False  True  True  True  True False  True False\n",
            " False False  True False  True False  True  True  True False False False\n",
            "  True False False  True  True  True]\n",
            "evaluating: [False False  True False False False False  True  True  True False False\n",
            " False False  True False False  True  True False False False  True False\n",
            "  True  True  True False False  True]\n",
            "evaluating: [ True False  True  True  True False  True False  True  True  True  True\n",
            "  True  True False False  True False  True  True False False False  True\n",
            " False False  True  True  True False]\n",
            "evaluating: [ True  True  True False  True False False  True False False  True  True\n",
            "  True  True False False False  True  True  True False  True  True False\n",
            "  True False False  True  True  True]\n",
            "evaluating: [ True  True False  True False False  True False  True  True False  True\n",
            " False  True False False False  True False  True False False False False\n",
            " False  True False False False  True]\n",
            "evaluating: [False False  True  True  True False  True False False  True  True  True\n",
            "  True  True False False False  True False False False False  True False\n",
            "  True False  True  True  True False]\n",
            "evaluating: [ True  True  True False False False False False  True  True False False\n",
            " False False  True False False False False False False  True False False\n",
            " False False False  True  True  True]\n",
            "evaluating: [ True  True  True  True False False False False False False False  True\n",
            " False False False False False  True  True False  True  True  True  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False  True False False False False False  True  True False False  True\n",
            "  True  True  True False  True False False False  True  True  True False\n",
            " False False False  True  True  True]\n",
            "evaluating: [False False False  True  True False False  True  True False  True  True\n",
            "  True  True  True False False False  True  True  True False False False\n",
            "  True  True False  True  True False]\n",
            "evaluating: [False  True  True False False False  True  True False False  True  True\n",
            "  True False  True False False False False  True  True  True  True  True\n",
            "  True  True  True False  True False]\n",
            "evaluating: [False  True  True False  True False False False  True  True False False\n",
            " False False  True  True  True  True  True  True False False False  True\n",
            " False  True False  True False False]\n",
            "evaluating: [False False False False  True False False False  True False False  True\n",
            "  True  True False False  True False  True  True False  True  True  True\n",
            "  True  True  True  True False  True]\n",
            "evaluating: [ True  True  True  True False  True  True  True  True False False  True\n",
            " False False  True  True  True  True  True False False  True  True False\n",
            " False False False  True False False]\n",
            "evaluating: [False False False False  True  True  True False  True False False  True\n",
            " False False False False False False False  True False False False  True\n",
            " False False False  True  True  True]\n",
            "evaluating: [ True False False False  True  True  True  True False  True False False\n",
            "  True  True False  True False False  True  True False  True  True False\n",
            "  True False False False  True  True]\n",
            "evaluating: [False  True  True False False False False False  True  True False  True\n",
            "  True False False  True False  True  True  True False False  True  True\n",
            " False  True False False  True False]\n",
            "evaluating: [ True False False  True False False  True False  True  True False  True\n",
            " False  True  True False  True  True False False  True False False False\n",
            "  True False False False False  True]\n",
            "evaluating: [False False  True  True False False  True False  True  True False False\n",
            " False False  True  True  True False False False False  True False  True\n",
            " False False False  True False False]\n",
            "evaluating: [ True  True False  True False  True False False  True  True  True  True\n",
            "  True False  True  True  True  True False  True False False False False\n",
            "  True False False False False False]\n",
            "evaluating: [ True False False False False  True False  True  True  True  True  True\n",
            " False False  True  True False  True False False False  True False  True\n",
            "  True  True  True False  True  True]\n",
            "evaluating: [ True  True False  True  True False False  True  True False False False\n",
            " False False  True False  True  True  True False  True  True False  True\n",
            " False False  True False  True  True]\n",
            "evaluating: [False False  True  True False False  True False False  True False False\n",
            " False False False  True  True False  True  True  True False False False\n",
            "  True  True False False False False]\n",
            "evaluating: [ True False  True False False False False False False  True  True False\n",
            "  True  True False False False False  True  True  True False False  True\n",
            "  True  True False False False False]\n",
            "     2 |      100 |     18 |  0.0714316125 |         ideal\n",
            "evaluating: [ True False False False False  True False False  True  True False False\n",
            " False False False  True  True  True False  True  True  True  True  True\n",
            "  True False False False False False]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# print(server.model.get_weights())\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(server.evaluate())\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m res = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtermination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDefaultMultiObjectiveTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_max_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_GENERATIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# seed=42,\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGLOBAL MODEL AFTER OPTIMIZATION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(server.evaluate())\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\optimize.py:67\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m     algorithm.setup(problem, **kwargs)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# actually execute the algorithm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m res = \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[32m     70\u001b[39m res.algorithm = algorithm\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\algorithm.py:138\u001b[39m, in \u001b[36mAlgorithm.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_next():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\algorithm.py:158\u001b[39m, in \u001b[36mAlgorithm.next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# call the advance with them after evaluation\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infills \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(infills=infills)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# if the algorithm does not follow the infill-advance scheme just call advance\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\evaluator.py:69\u001b[39m, in \u001b[36mEvaluator.eval\u001b[39m\u001b[34m(self, problem, pop, skip_already_evaluated, evaluate_values_of, count_evals, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# evaluate the solutions (if there are any)\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(I) > \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# do the actual evaluation - call the sub-function to set the corresponding values to the population\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# update the function evaluation counter\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_evals:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\evaluator.py:90\u001b[39m, in \u001b[36mEvaluator._eval\u001b[39m\u001b[34m(self, problem, pop, evaluate_values_of, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m X = pop.get(\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# call the problem to evaluate the solutions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m out = \u001b[43mproblem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# for each of the attributes set it to the problem\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m out.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:257\u001b[39m, in \u001b[36mProblem.evaluate\u001b[39m\u001b[34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     only_single_value = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np.ndarray))\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m _out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m out = {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _out.items():\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:299\u001b[39m, in \u001b[36mProblem.do\u001b[39m\u001b[34m(self, X, return_values_of, *args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate_elementwise(X, out, *args, **kwargs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# finally format the output dictionary\u001b[39;00m\n\u001b[32m    302\u001b[39m out = \u001b[38;5;28mself\u001b[39m._format_dict(out, \u001b[38;5;28mlen\u001b[39m(X), return_values_of)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:307\u001b[39m, in \u001b[36mProblem._evaluate_vectorized\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_evaluate_vectorized\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, out, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mFederatedLearningProblem._evaluate\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# new\u001b[39;00m\n\u001b[32m    182\u001b[39m     temp_global_model.set_weights(\u001b[38;5;28mself\u001b[39m.performance_objective_aggregation(selected_devices))\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     _, global_accuracy = \u001b[43mtemp_global_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_test_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43my_test_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     F[i, \u001b[32m2\u001b[39m] = \u001b[32m1\u001b[39m - global_accuracy  \u001b[38;5;66;03m# Minimize (1 - accuracy)\u001b[39;00m\n\u001b[32m    186\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m] = F\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:481\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    480\u001b[39m     callbacks.on_test_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     callbacks.on_test_batch_end(step, logs)\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    214\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    218\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[32m    888\u001b[39m   initializers = []\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    891\u001b[39m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[32m    892\u001b[39m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[32m    893\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[39m, in \u001b[36mFunction._initialize\u001b[39m\u001b[34m(self, args, kwds, add_initializers_to)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_creation_config = \u001b[38;5;28mself\u001b[39m._generate_scoped_tracing_options(\n\u001b[32m    692\u001b[39m     variable_capturing_scope,\n\u001b[32m    693\u001b[39m     tracing_compilation.ScopeType.VARIABLE_CREATION,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34minvalid_creator_scope\u001b[39m(*unused_args, **unused_kwds):\n\u001b[32m    701\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1056\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1058\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1063\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[32m    338\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph artifact\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools.partial):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:129\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         )\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    133\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:906\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[32m    903\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    910\u001b[39m   bound_args = \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.bind(\n\u001b[32m    911\u001b[39m       *args, **kwds\n\u001b[32m    912\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:132\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    130\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    131\u001b[39m kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m function = \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1056\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1058\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1063\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conversion.is_in_allowlist_cache(f, options):\n\u001b[32m    330\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: from cache\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:\n\u001b[32m    334\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph is disabled in context\u001b[39m\u001b[33m'\u001b[39m, f)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:110\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    109\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    112\u001b[39m         outputs,\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    114\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m     )\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:90\u001b[39m, in \u001b[36mTensorFlowTrainer.test_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     89\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(x)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     92\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m._loss_tracker.update_state(\n\u001b[32m     94\u001b[39m     loss, sample_weight=tf.shape(tree.flatten(x)[\u001b[32m0\u001b[39m])[\u001b[32m0\u001b[39m]\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:399\u001b[39m, in \u001b[36mTrainer._compute_loss\u001b[39m\u001b[34m(self, x, y, y_pred, sample_weight, training)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Backwards compatibility wrapper for `compute_loss`.\u001b[39;00m\n\u001b[32m    393\u001b[39m \n\u001b[32m    394\u001b[39m \u001b[33;03mThis should be used instead `compute_loss` within `train_step` and\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[33;03m`test_step` to support overrides of `compute_loss` that may not have\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03mthe `training` argument, as this argument was added in Keras 3.3.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compute_loss_has_training_arg:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss(x, y, y_pred, sample_weight)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:367\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    365\u001b[39m losses = []\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compile_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    369\u001b[39m         losses.append(loss)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:692\u001b[39m, in \u001b[36mCompileLoss.__call__\u001b[39m\u001b[34m(self, y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[38;5;28mself\u001b[39m.name):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:701\u001b[39m, in \u001b[36mCompileLoss.call\u001b[39m\u001b[34m(self, y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28mself\u001b[39m.build(y_true, y_pred)\n\u001b[32m    699\u001b[39m _, loss_fn, loss_weight, _ = \u001b[38;5;28mself\u001b[39m._flat_losses[\u001b[32m0\u001b[39m]\n\u001b[32m    700\u001b[39m loss_value = ops.cast(\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m, dtype=\u001b[38;5;28mself\u001b[39m.dtype\n\u001b[32m    702\u001b[39m )\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    704\u001b[39m     loss_value = ops.multiply(loss_value, loss_weight)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\losses\\loss.py:79\u001b[39m, in \u001b[36mLoss.__call__\u001b[39m\u001b[34m(self, y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m     mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce_weighted_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\losses\\loss.py:192\u001b[39m, in \u001b[36mreduce_weighted_values\u001b[39m\u001b[34m(values, sample_weight, mask, reduction, dtype)\u001b[39m\n\u001b[32m    189\u001b[39m     values = values * sample_weight\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Apply reduction function to the individual weighted losses.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m loss = \u001b[43mreduce_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\losses\\loss.py:155\u001b[39m, in \u001b[36mreduce_values\u001b[39m\u001b[34m(values, sample_weight, reduction)\u001b[39m\n\u001b[32m    151\u001b[39m         divisor = ops.cast(ops.sum(sample_weight), loss.dtype)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    153\u001b[39m         divisor = ops.cast(\n\u001b[32m    154\u001b[39m             ops.prod(\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m                 ops.convert_to_tensor(\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m, dtype=\u001b[33m\"\u001b[39m\u001b[33mint32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m             ),\n\u001b[32m    157\u001b[39m             loss.dtype,\n\u001b[32m    158\u001b[39m         )\n\u001b[32m    159\u001b[39m     loss = ops.divide_no_nan(loss, divisor)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\ops\\core.py:744\u001b[39m, in \u001b[36mshape\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.shape\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187\u001b[39m, in \u001b[36mshape\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    183\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAll tensors passed to `ops.shape` must have a statically known \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrank. Received: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with unknown rank.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m shape = x.shape.as_list()\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m dynamic = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape)):\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shape[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:655\u001b[39m, in \u001b[36mshape_v2\u001b[39m\u001b[34m(input, out_type, name)\u001b[39m\n\u001b[32m    653\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    654\u001b[39m     out_type = dtypes.int32\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:687\u001b[39m, in \u001b[36mshape\u001b[39m\u001b[34m(input, name, out_type)\u001b[39m\n\u001b[32m    685\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    686\u001b[39m     out_type = dtypes.int32\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:728\u001b[39m, in \u001b[36mshape_internal\u001b[39m\u001b[34m(input, name, optimize, out_type)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_type:\n\u001b[32m    727\u001b[39m   out_type = dtypes.int32\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:12046\u001b[39m, in \u001b[36mshape\u001b[39m\u001b[34m(input, out_type, name)\u001b[39m\n\u001b[32m  12044\u001b[39m   out_type = _dtypes.int32\n\u001b[32m  12045\u001b[39m out_type = _execute.make_type(out_type, \u001b[33m\"\u001b[39m\u001b[33mout_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m> \u001b[39m\u001b[32m12046\u001b[39m _, _, _op, _outputs = \u001b[43m_op_def_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12047\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mShape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  12048\u001b[39m _result = _outputs[:]\n\u001b[32m  12049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _execute.must_record_gradient():\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[39m, in \u001b[36m_apply_op_helper\u001b[39m\u001b[34m(op_type_name, name, **keywords)\u001b[39m\n\u001b[32m    791\u001b[39m must_colocate_inputs = [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def.input_arg, inputs)\n\u001b[32m    792\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m arg.is_ref]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[32m    794\u001b[39m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[32m    795\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m   op = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[32m    804\u001b[39m outputs = op.outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[39m, in \u001b[36mFuncGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m    668\u001b[39m   inp = \u001b[38;5;28mself\u001b[39m.capture(inp)\n\u001b[32m    669\u001b[39m   captured_inputs.append(inp)\n\u001b[32m--> \u001b[39m\u001b[32m670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2701\u001b[39m, in \u001b[36mGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m   2698\u001b[39m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[32m   2699\u001b[39m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[32m   2700\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mutation_lock():\n\u001b[32m-> \u001b[39m\u001b[32m2701\u001b[39m   ret = \u001b[43mOperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2702\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2703\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2704\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2705\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2706\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2707\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2708\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2709\u001b[39m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2710\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2711\u001b[39m   \u001b[38;5;28mself\u001b[39m._create_op_helper(ret, compute_device=compute_device)\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1196\u001b[39m, in \u001b[36mOperation.from_node_def\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1193\u001b[39m     control_input_ops.append(control_op)\n\u001b[32m   1195\u001b[39m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1196\u001b[39m c_op = \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28mself\u001b[39m = Operation(c_op, SymbolicTensor)\n\u001b[32m   1198\u001b[39m \u001b[38;5;28mself\u001b[39m._init(g)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\github project\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1053\u001b[39m, in \u001b[36m_create_c_op\u001b[39m\u001b[34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[39m\n\u001b[32m   1049\u001b[39m   pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),\n\u001b[32m   1050\u001b[39m                                          serialized)\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m   c_op = \u001b[43mpywrap_tf_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m errors.InvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[32m   1056\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e.message)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for i in range(NUM_ROUNDS):\n",
        "    # Step 3: Run Optimization\n",
        "    print(\"GLOBAL MODEL BEFORE OPTIMIZATION\")\n",
        "    print(server.evaluate())\n",
        "    res = minimize(\n",
        "        problem=problem,\n",
        "        algorithm=algorithm,\n",
        "        termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "        # seed=42,\n",
        "        verbose=True\n",
        "    )\n",
        "    print(\"GLOBAL MODEL AFTER OPTIMIZATION\")\n",
        "    print(server.evaluate())\n",
        "\n",
        "\n",
        "    # Step 4: Extract the Best Pareto Front\n",
        "    pareto_front = res.F   # Objective values of solutions in Pareto front\n",
        "    pareto_solutions = res.X  # Corresponding bitstrings\n",
        "\n",
        "    # Print the Best Pareto Front Solutions\n",
        "    print(\"Best Pareto Front (Bitstrings):\")\n",
        "    for bitstring in pareto_solutions:\n",
        "        print(\"\".join(map(str, bitstring)).replace('True','1').replace('False','0'))\n",
        "\n",
        "    bitstring = pareto_solutions[0] # for now!\n",
        "    bitstring = str(bitstring).replace('False','0').replace('True','1')\n",
        "    for char in bitstring:\n",
        "        if char != '0' and char != '1':\n",
        "            bitstring = bitstring.replace(char,'')\n",
        "\n",
        "    print(len(bitstring))\n",
        "    print(bitstring)\n",
        "    temp_bitstring = []\n",
        "    for bit in bitstring:\n",
        "        temp_bitstring.append(bit)\n",
        "    bitstring = temp_bitstring\n",
        "\n",
        "\n",
        "    test_loss, test_acc = server.evaluate()\n",
        "    print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    server.give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "    fit_bitstring_devices(bitstring)\n",
        "    server.model.set_weights(server.aggregate_weights(bitstring))\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    test_loss, test_acc = server.evaluate()\n",
        "    print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrcLQ8XXHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Pareto Front (Bitstrings):\n",
            "010010000000001110001100011101\n",
            "001100100000111000011010001110\n",
            "000000100000001110001100011101\n",
            "100100011001100010010010000000\n",
            "010100011010100000111000010000\n",
            "010010001110100010001001000000\n",
            "110000100000001110001100101101\n",
            "100010101000100001110001101000\n",
            "000001000000000110001001000000\n",
            "000100100000100010011010001110\n",
            "000100000000001000111110001011\n",
            "111010000000100100001110000101\n",
            "30\n",
            "010010000000001110001100011101\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1128 - loss: 2.3028\n",
            "Global Model Accuracy: 0.1235\n",
            "------------------------------------------------------------\n",
            "*******************\n",
            "1.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "4.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "14.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "15.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "16.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "20.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "21.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "25.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "26.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "27.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "*******************\n",
            "29.0\n",
            "this device's participation ratio:\n",
            "0.2857142857142857\n",
            "this device's data to all ratio:\n",
            "0.03333333333333333\n",
            "Aggregated weights:\n",
            "Layer 0: (3, 3, 1, 32)\n",
            "Layer 1: (32,)\n",
            "Layer 2: (5408, 128)\n",
            "Layer 3: (128,)\n",
            "Layer 4: (128, 10)\n",
            "Layer 5: (10,)\n",
            "------------------------------------------------------------\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1128 - loss: 2.3027\n",
            "Global Model Accuracy: 0.1235\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Extract the Best Pareto Front\n",
        "pareto_front = res.F   # Objective values of solutions in Pareto front\n",
        "pareto_solutions = res.X  # Corresponding bitstrings\n",
        "\n",
        "# Print the Best Pareto Front Solutions\n",
        "print(\"Best Pareto Front (Bitstrings):\")\n",
        "for bitstring in pareto_solutions:\n",
        "    print(\"\".join(map(str, bitstring)).replace('True','1').replace('False','0'))\n",
        "\n",
        "bitstring = pareto_solutions[0] # for now!\n",
        "bitstring = str(bitstring).replace('False','0').replace('True','1')\n",
        "for char in bitstring:\n",
        "    if char != '0' and char != '1':\n",
        "        bitstring = bitstring.replace(char,'')\n",
        "\n",
        "print(len(bitstring))\n",
        "print(bitstring)\n",
        "temp_bitstring = []\n",
        "for bit in bitstring:\n",
        "    temp_bitstring.append(bit)\n",
        "bitstring = temp_bitstring\n",
        "\n",
        "########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.id)] == '1']\n",
        "# give_global_model_weights_to_bitstring_devices(bitstring)\n",
        "fit_bitstring_devices(bitstring)\n",
        "\n",
        "# new\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(bitstring))\n",
        "print(\"------------------------------------------------------------\")\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6xCkaJlHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0']\n",
            "100\n",
            "39\n"
          ]
        }
      ],
      "source": [
        "print(bitstring) # for now!\n",
        "print(len(bitstring))\n",
        "print(bitstring.count(\"1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DernaKkHqJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      5 |             - |             -\n",
            "     2 |      100 |      6 |  0.0129361668 |         ideal\n",
            "     3 |      150 |      5 |  0.0786670050 |         ideal\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(awiodawo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiQqM7-0HqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 4.0: Loss before: [2.2980031967163086, 0.14666666090488434], Loss after: [2.284519910812378, 0.35333332419395447]\n",
            "Device 5.0: Loss before: [2.298715114593506, 0.11999999731779099], Loss after: [2.2860357761383057, 0.34333333373069763]\n",
            "Device 6.0: Loss before: [2.298321008682251, 0.14166666567325592], Loss after: [2.2865428924560547, 0.4116666615009308]\n",
            "Device 7.0: Loss before: [2.298210859298706, 0.13833333551883698], Loss after: [2.282723903656006, 0.22833333909511566]\n",
            "Device 10.0: Loss before: [2.2980704307556152, 0.1616666615009308], Loss after: [2.2842841148376465, 0.2266666740179062]\n",
            "Device 11.0: Loss before: [2.2986857891082764, 0.13333334028720856], Loss after: [2.285165786743164, 0.22333332896232605]\n",
            "Device 13.0: Loss before: [2.2982802391052246, 0.14499999582767487], Loss after: [2.2767813205718994, 0.1550000011920929]\n",
            "Device 26.0: Loss before: [2.2987937927246094, 0.12166666984558105], Loss after: [2.286884307861328, 0.3266666531562805]\n",
            "Device 31.0: Loss before: [2.298445224761963, 0.1366666704416275], Loss after: [2.286431312561035, 0.3216666579246521]\n",
            "Device 33.0: Loss before: [2.2983906269073486, 0.1483333259820938], Loss after: [2.286226511001587, 0.29499998688697815]\n",
            "Device 36.0: Loss before: [2.298095464706421, 0.14499999582767487], Loss after: [2.2831168174743652, 0.12999999523162842]\n",
            "Device 42.0: Loss before: [2.2977967262268066, 0.15666666626930237], Loss after: [2.2825639247894287, 0.20333333313465118]\n",
            "Device 43.0: Loss before: [2.2984626293182373, 0.12833333015441895], Loss after: [2.2870965003967285, 0.4183333218097687]\n",
            "Device 44.0: Loss before: [2.298483371734619, 0.125], Loss after: [2.2861244678497314, 0.2266666740179062]\n",
            "Device 46.0: Loss before: [2.297833204269409, 0.14000000059604645], Loss after: [2.283404588699341, 0.2750000059604645]\n",
            "Device 47.0: Loss before: [2.2982921600341797, 0.14166666567325592], Loss after: [2.284034490585327, 0.3933333456516266]\n",
            "Device 50.0: Loss before: [2.298464059829712, 0.1366666704416275], Loss after: [2.287050247192383, 0.22833333909511566]\n",
            "Device 51.0: Loss before: [2.2985193729400635, 0.11666666716337204], Loss after: [2.2863316535949707, 0.2316666692495346]\n",
            "Device 52.0: Loss before: [2.2987890243530273, 0.125], Loss after: [2.291125535964966, 0.23499999940395355]\n",
            "Device 53.0: Loss before: [2.298614978790283, 0.13833333551883698], Loss after: [2.288135290145874, 0.2266666740179062]\n",
            "Device 54.0: Loss before: [2.29848313331604, 0.12666666507720947], Loss after: [2.287320852279663, 0.24833333492279053]\n",
            "Device 55.0: Loss before: [2.2981367111206055, 0.14499999582767487], Loss after: [2.2849795818328857, 0.24166665971279144]\n",
            "Device 58.0: Loss before: [2.298809766769409, 0.14499999582767487], Loss after: [2.287022113800049, 0.2866666615009308]\n",
            "Device 61.0: Loss before: [2.2984554767608643, 0.14499999582767487], Loss after: [2.2861742973327637, 0.15666666626930237]\n",
            "Device 62.0: Loss before: [2.2990224361419678, 0.11500000208616257], Loss after: [2.2846298217773438, 0.14666666090488434]\n",
            "Device 64.0: Loss before: [2.297769069671631, 0.16333332657814026], Loss after: [2.283010959625244, 0.20333333313465118]\n",
            "Device 68.0: Loss before: [2.298501968383789, 0.125], Loss after: [2.283834218978882, 0.13833333551883698]\n",
            "Device 71.0: Loss before: [2.2984633445739746, 0.14000000059604645], Loss after: [2.2856762409210205, 0.2933333218097687]\n",
            "Device 72.0: Loss before: [2.2984213829040527, 0.13500000536441803], Loss after: [2.2863972187042236, 0.4116666615009308]\n",
            "Device 75.0: Loss before: [2.2984416484832764, 0.125], Loss after: [2.2861156463623047, 0.3316666781902313]\n",
            "Device 77.0: Loss before: [2.298542022705078, 0.1483333259820938], Loss after: [2.285374879837036, 0.26499998569488525]\n",
            "Device 81.0: Loss before: [2.2985634803771973, 0.125], Loss after: [2.2850818634033203, 0.3283333480358124]\n",
            "Device 84.0: Loss before: [2.298508405685425, 0.12999999523162842], Loss after: [2.2876505851745605, 0.25]\n",
            "Device 86.0: Loss before: [2.2986748218536377, 0.11833333224058151], Loss after: [2.2889621257781982, 0.34833332896232605]\n",
            "Device 88.0: Loss before: [2.2982442378997803, 0.13333334028720856], Loss after: [2.2840864658355713, 0.3383333384990692]\n",
            "Device 89.0: Loss before: [2.2988176345825195, 0.11166666448116302], Loss after: [2.286825656890869, 0.2199999988079071]\n",
            "Device 91.0: Loss before: [2.298393726348877, 0.1133333370089531], Loss after: [2.2810146808624268, 0.14000000059604645]\n",
            "Device 94.0: Loss before: [2.298144578933716, 0.1366666704416275], Loss after: [2.283414363861084, 0.19166666269302368]\n",
            "Device 98.0: Loss before: [2.298098564147949, 0.14499999582767487], Loss after: [2.283278226852417, 0.4350000023841858]\n",
            "[array([[[[ 0.01560311,  0.02257121,  0.01110409,  0.01507172,\n",
            "           0.00252358,  0.01510464,  0.01344061,  0.00847014,\n",
            "           0.02270118,  0.01620024,  0.01468922,  0.01839421,\n",
            "           0.0124475 ,  0.01659005,  0.00809593,  0.01099574,\n",
            "           0.01550036,  0.018876  ,  0.00679025,  0.00889408,\n",
            "           0.02077803,  0.00761622,  0.00098165,  0.01305017,\n",
            "           0.01599254,  0.02000332,  0.01462005,  0.02717552,\n",
            "           0.00411549,  0.00790651,  0.0111158 ,  0.01423351]],\n",
            "\n",
            "        [[ 0.02454352,  0.03025914,  0.00766521,  0.01335663,\n",
            "           0.01355273,  0.01646741,  0.02856857,  0.01110631,\n",
            "           0.01486227,  0.02343186,  0.02989276,  0.02386882,\n",
            "           0.00201672,  0.01325708,  0.01809904,  0.01838938,\n",
            "           0.02030334,  0.02471407,  0.02045402,  0.01833379,\n",
            "           0.02333464,  0.01066581,  0.02327173,  0.01741481,\n",
            "           0.00916306,  0.01697847,  0.01220456,  0.02428175,\n",
            "           0.00664643,  0.01701493,  0.02061685,  0.0351745 ]],\n",
            "\n",
            "        [[ 0.02193822,  0.01435862,  0.02124896,  0.01630747,\n",
            "           0.00577648,  0.01895138,  0.02538386,  0.02006774,\n",
            "           0.01959194,  0.0150976 ,  0.01944805,  0.0194125 ,\n",
            "           0.01799021,  0.00302283,  0.01204218,  0.03292189,\n",
            "           0.02132382,  0.01433191,  0.01168544,  0.0204161 ,\n",
            "           0.01386432,  0.01853496,  0.01494559,  0.02114179,\n",
            "           0.00350407,  0.02010733,  0.01673094,  0.0135516 ,\n",
            "           0.00789044,  0.01261784,  0.02595696,  0.021892  ]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01583286,  0.01706405,  0.00928797,  0.02194787,\n",
            "           0.01194718,  0.00750862,  0.0267721 ,  0.01310973,\n",
            "           0.02374508,  0.01495223,  0.01332487,  0.01925408,\n",
            "           0.00602902,  0.00864317,  0.0056372 ,  0.01789839,\n",
            "           0.01463715,  0.02821155,  0.02027787,  0.01830474,\n",
            "           0.01953365,  0.01347347,  0.00425999,  0.03135091,\n",
            "           0.01717493,  0.01812383,  0.02568616, -0.00023046,\n",
            "           0.01616774,  0.01278134,  0.00497106,  0.02595186]],\n",
            "\n",
            "        [[ 0.02445064,  0.03852054,  0.018642  ,  0.0310827 ,\n",
            "           0.00468171,  0.01192496,  0.03432898,  0.02344587,\n",
            "           0.02912918,  0.02498843,  0.02749189,  0.03494409,\n",
            "           0.01831012,  0.00914495,  0.02187797,  0.01200425,\n",
            "           0.02039989,  0.0085194 ,  0.01306869,  0.0226127 ,\n",
            "           0.02819599,  0.02399428,  0.01103092,  0.02411702,\n",
            "           0.00344842,  0.02161986,  0.02060069,  0.02084474,\n",
            "           0.01183216,  0.02250262,  0.01550617,  0.03198152]],\n",
            "\n",
            "        [[ 0.02100236,  0.00818088,  0.01502692,  0.01960614,\n",
            "           0.00996529,  0.00806254,  0.02817566,  0.01533024,\n",
            "           0.02270831,  0.03086884,  0.01177241,  0.02273101,\n",
            "           0.0109973 ,  0.02290827,  0.01083251,  0.0207416 ,\n",
            "           0.014455  ,  0.01956153,  0.01758006,  0.02170456,\n",
            "           0.0294641 ,  0.00442351,  0.01783276,  0.02806052,\n",
            "           0.0118739 ,  0.02154494,  0.00755924,  0.02070526,\n",
            "           0.01523361,  0.01553781,  0.02527748,  0.01697074]]],\n",
            "\n",
            "\n",
            "       [[[ 0.0324344 ,  0.015249  ,  0.02606078,  0.02648943,\n",
            "           0.00713766,  0.00728631,  0.01831263,  0.01531798,\n",
            "           0.03172095,  0.0210232 ,  0.01390663,  0.02049077,\n",
            "           0.00733794,  0.02008646,  0.02194182,  0.02932457,\n",
            "           0.00885229,  0.00942696,  0.01636209,  0.01565125,\n",
            "           0.0255361 ,  0.01422815,  0.01359377,  0.01596601,\n",
            "           0.02408645,  0.02995463,  0.02401149,  0.00478586,\n",
            "           0.0128755 ,  0.01185623, -0.00434332,  0.01779371]],\n",
            "\n",
            "        [[ 0.02174696,  0.02728774,  0.0163013 ,  0.03028866,\n",
            "           0.00957214,  0.02481695,  0.02091229,  0.01369449,\n",
            "           0.0169973 ,  0.02415604,  0.02573123,  0.02196436,\n",
            "           0.01292184,  0.0205738 ,  0.02399925,  0.00373072,\n",
            "           0.01486036,  0.01748678,  0.01490798,  0.02883988,\n",
            "           0.01987246,  0.0105205 ,  0.011029  ,  0.01893784,\n",
            "           0.0201656 ,  0.0122781 ,  0.01156722,  0.01336054,\n",
            "           0.01603947,  0.01780558,  0.00980261,  0.03119609]],\n",
            "\n",
            "        [[ 0.00169485,  0.01614244,  0.01890111,  0.01342804,\n",
            "           0.02288543,  0.0220295 ,  0.02215771,  0.00453429,\n",
            "           0.0104293 ,  0.01137705,  0.01312875,  0.01275303,\n",
            "          -0.0021973 ,  0.01964887,  0.01038801,  0.01078565,\n",
            "           0.00911237,  0.01325769,  0.0231106 ,  0.01071291,\n",
            "           0.02024084,  0.00966646,  0.01100577,  0.01399866,\n",
            "           0.01709351,  0.00586824,  0.0107098 ,  0.02085855,\n",
            "           0.01707222,  0.01415403,  0.01973128,  0.0290544 ]]]],\n",
            "      dtype=float32), array([0.00771213, 0.00681153, 0.00631751, 0.00721437, 0.0075969 ,\n",
            "       0.0061396 , 0.00598041, 0.00799789, 0.00771072, 0.00854967,\n",
            "       0.00835016, 0.00864327, 0.00493658, 0.00647637, 0.00440599,\n",
            "       0.00825316, 0.00704589, 0.00756239, 0.00800533, 0.00886914,\n",
            "       0.00747405, 0.00731832, 0.00616572, 0.00680429, 0.00570102,\n",
            "       0.00622225, 0.00792158, 0.00602227, 0.00704377, 0.00543422,\n",
            "       0.00656562, 0.00667889], dtype=float32), array([[ 6.1755313e-04, -2.4664932e-04,  6.7078852e-04, ...,\n",
            "        -8.0360885e-04,  2.5043902e-03,  6.5234961e-04],\n",
            "       [ 2.0079555e-03, -4.2310532e-04,  1.7069298e-03, ...,\n",
            "        -1.1483760e-03,  6.6667481e-04, -4.5175949e-04],\n",
            "       [ 6.0673966e-04, -9.8841707e-04, -2.0743026e-03, ...,\n",
            "         2.1416135e-03, -5.4449702e-05, -5.0806307e-04],\n",
            "       ...,\n",
            "       [-2.0420072e-03, -5.4576492e-04, -1.4355610e-03, ...,\n",
            "        -2.9001830e-03,  1.7687751e-03, -1.5782582e-03],\n",
            "       [-9.8339305e-04, -3.5401853e-03,  1.1228601e-03, ...,\n",
            "         8.2406885e-04,  1.4938946e-03, -1.5562265e-03],\n",
            "       [-1.1787841e-03, -5.9427781e-04, -4.3058144e-06, ...,\n",
            "         4.6118908e-05, -1.6114977e-03,  1.8282176e-03]], dtype=float32), array([0.00208667, 0.00189387, 0.00199729, 0.00154911, 0.00176541,\n",
            "       0.00206093, 0.00210721, 0.00184068, 0.00180145, 0.00228131,\n",
            "       0.00232149, 0.00193657, 0.00205213, 0.00194205, 0.00170934,\n",
            "       0.00203107, 0.00255653, 0.00209281, 0.00172378, 0.00140728,\n",
            "       0.00139628, 0.00142322, 0.00107531, 0.00155734, 0.00208906,\n",
            "       0.00194707, 0.0019741 , 0.00080239, 0.00225596, 0.00167879,\n",
            "       0.00110195, 0.00208302, 0.00242299, 0.00226593, 0.00213515,\n",
            "       0.00210372, 0.00218468, 0.00213812, 0.00230887, 0.0023536 ,\n",
            "       0.00205644, 0.00135849, 0.00198219, 0.00222345, 0.00185209,\n",
            "       0.00178986, 0.00245615, 0.00157912, 0.00251533, 0.00143394,\n",
            "       0.00166692, 0.00145071, 0.00156005, 0.00219932, 0.00183136,\n",
            "       0.00198621, 0.00259816, 0.00179712, 0.00165607, 0.00188844,\n",
            "       0.00211427, 0.00188984, 0.00236808, 0.00228475, 0.00222471,\n",
            "       0.00214465, 0.0018378 , 0.00170703, 0.00159359, 0.00233416,\n",
            "       0.00213527, 0.00218075, 0.00142351, 0.00123618, 0.00193511,\n",
            "       0.00188489, 0.00185067, 0.00230161, 0.00176024, 0.00205667,\n",
            "       0.00251559, 0.00318715, 0.00205174, 0.00178891, 0.00225481,\n",
            "       0.00219605, 0.00222357, 0.00166145, 0.0017003 , 0.00114618,\n",
            "       0.00203703, 0.00166267, 0.00272077, 0.00205865, 0.0027835 ,\n",
            "       0.00170427, 0.00261458, 0.00222372, 0.00276402, 0.00197034,\n",
            "       0.00208029, 0.00165495, 0.0019255 , 0.00175673, 0.00191493,\n",
            "       0.00154906, 0.00210837, 0.00250689, 0.00197622, 0.00173402,\n",
            "       0.00255804, 0.00226427, 0.00136213, 0.00181246, 0.00224639,\n",
            "       0.00226412, 0.00194216, 0.00293876, 0.00218041, 0.0023717 ,\n",
            "       0.00189303, 0.00259776, 0.00223523, 0.00201245, 0.00198338,\n",
            "       0.00155573, 0.00282332, 0.00157042], dtype=float32), array([[ 2.7212135e-03,  6.0715782e-03,  6.3353442e-03, ...,\n",
            "         7.5136763e-03, -4.0855831e-03,  1.4342298e-02],\n",
            "       [ 4.1163838e-03, -1.4130400e-04, -2.2077980e-03, ...,\n",
            "        -4.9452571e-04,  1.8071074e-02,  6.4545963e-03],\n",
            "       [-4.8383321e-05, -1.5000661e-02, -9.2021246e-03, ...,\n",
            "        -7.4389344e-03,  8.8850204e-03,  8.3303526e-03],\n",
            "       ...,\n",
            "       [-5.6947261e-04, -7.5646313e-03,  1.4763591e-02, ...,\n",
            "         7.4852011e-03,  1.1638147e-02,  2.2772280e-03],\n",
            "       [ 2.5996622e-03,  1.4541779e-02,  3.6641737e-03, ...,\n",
            "        -1.5829407e-02,  1.2036985e-02,  8.1936205e-03],\n",
            "       [ 1.1044403e-02, -6.6315653e-03,  4.2182137e-03, ...,\n",
            "         4.1996576e-03,  1.2093508e-02,  7.1917675e-03]], dtype=float32), array([-0.00883807,  0.02402571, -0.00454409, -0.00271043,  0.00178446,\n",
            "       -0.00320016, -0.00219019,  0.00676279, -0.01052285, -0.00056717],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "[array([[[[ 0.01853657,  0.0255899 ,  0.01325048,  0.01794863,\n",
            "           0.00372288,  0.0170061 ,  0.01661289,  0.01069272,\n",
            "           0.02590805,  0.01887829,  0.01721864,  0.02123614,\n",
            "           0.01414526,  0.018442  ,  0.00980237,  0.01353807,\n",
            "           0.01833743,  0.02134998,  0.00860709,  0.01143229,\n",
            "           0.02350541,  0.00942591,  0.00255571,  0.01588496,\n",
            "           0.01780889,  0.02272947,  0.01703819,  0.0295219 ,\n",
            "           0.00567911,  0.00992756,  0.01292842,  0.01759894]],\n",
            "\n",
            "        [[ 0.02806525,  0.0340521 ,  0.01042626,  0.01677048,\n",
            "           0.01522873,  0.01894756,  0.03265935,  0.01388947,\n",
            "           0.01874037,  0.02680896,  0.03302965,  0.02741195,\n",
            "           0.00390089,  0.01554412,  0.02027846,  0.02171273,\n",
            "           0.02374921,  0.02766285,  0.02292923,  0.0216443 ,\n",
            "           0.02682882,  0.01285453,  0.02550878,  0.02092806,\n",
            "           0.01137699,  0.02025804,  0.01501914,  0.02727219,\n",
            "           0.00864736,  0.01966366,  0.02307271,  0.03950934]],\n",
            "\n",
            "        [[ 0.02513651,  0.01783038,  0.02396577,  0.01943383,\n",
            "           0.00749725,  0.02141257,  0.0292639 ,  0.02265292,\n",
            "           0.02317629,  0.01827555,  0.02240883,  0.02271207,\n",
            "           0.01975618,  0.00517385,  0.0141285 ,  0.03614635,\n",
            "           0.02452176,  0.01694963,  0.01401895,  0.02353514,\n",
            "           0.01713959,  0.02049664,  0.01712154,  0.02447153,\n",
            "           0.00551249,  0.02300004,  0.01915327,  0.01644222,\n",
            "           0.00979655,  0.01508781,  0.02842446,  0.02584581]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01940412,  0.02075067,  0.01213542,  0.02545523,\n",
            "           0.01356196,  0.00990428,  0.03080706,  0.01583898,\n",
            "           0.02775078,  0.01825983,  0.01645891,  0.02274335,\n",
            "           0.00786063,  0.01103601,  0.00791961,  0.02103575,\n",
            "           0.01774827,  0.03119366,  0.0228071 ,  0.02154345,\n",
            "           0.0229629 ,  0.01579341,  0.00628144,  0.03483718,\n",
            "           0.01972052,  0.02150875,  0.02864049,  0.00240234,\n",
            "           0.01827069,  0.01529946,  0.00697541,  0.03011682]],\n",
            "\n",
            "        [[ 0.02834689,  0.04279963,  0.02198792,  0.03494903,\n",
            "           0.00671215,  0.01482777,  0.03894792,  0.02659522,\n",
            "           0.0335697 ,  0.02884293,  0.03106824,  0.03893242,\n",
            "           0.02036047,  0.01187469,  0.02449614,  0.01561989,\n",
            "           0.02396379,  0.0118091 ,  0.0160464 ,  0.02638634,\n",
            "           0.03219614,  0.02646436,  0.01358275,  0.02811726,\n",
            "           0.00630124,  0.02533112,  0.02371197,  0.02408264,\n",
            "           0.01430692,  0.02550556,  0.0181355 ,  0.03677465]],\n",
            "\n",
            "        [[ 0.0242896 ,  0.01191207,  0.01809579,  0.02292836,\n",
            "           0.01194662,  0.01082233,  0.03228762,  0.01807938,\n",
            "           0.02658   ,  0.03438237,  0.01495643,  0.02624138,\n",
            "           0.01283382,  0.02540681,  0.01314146,  0.02398441,\n",
            "           0.01759083,  0.02242739,  0.0202575 ,  0.0250039 ,\n",
            "           0.03304265,  0.00640731,  0.02021794,  0.0316869 ,\n",
            "           0.01434296,  0.02460437,  0.01000277,  0.02375208,\n",
            "           0.01747752,  0.01820194,  0.02782734,  0.02109626]]],\n",
            "\n",
            "\n",
            "       [[[ 0.0356867 ,  0.0185859 ,  0.02883152,  0.02972068,\n",
            "           0.00873862,  0.00959581,  0.02204921,  0.01775191,\n",
            "           0.03541991,  0.02405087,  0.01679223,  0.02370545,\n",
            "           0.00891395,  0.02249303,  0.02420833,  0.0321755 ,\n",
            "           0.01139511,  0.0119675 ,  0.01883093,  0.01866261,\n",
            "           0.02878221,  0.01636475,  0.01560853,  0.01908396,\n",
            "           0.02675719,  0.03308934,  0.02677112,  0.00707978,\n",
            "           0.01502777,  0.01430301, -0.00273201,  0.02162039]],\n",
            "\n",
            "        [[ 0.02499968,  0.03102347,  0.01934377,  0.03371151,\n",
            "           0.01150476,  0.02754932,  0.02490355,  0.01631334,\n",
            "           0.02082849,  0.02749114,  0.02888351,  0.02543498,\n",
            "           0.01465236,  0.02319067,  0.02640093,  0.00672082,\n",
            "           0.01766387,  0.02027829,  0.01764144,  0.03214663,\n",
            "           0.02345015,  0.01260883,  0.01336302,  0.02239945,\n",
            "           0.02304409,  0.01545657,  0.01425051,  0.01609392,\n",
            "           0.01844178,  0.02058656,  0.01192927,  0.03539412]],\n",
            "\n",
            "        [[ 0.00403864,  0.01918044,  0.02147606,  0.01612904,\n",
            "           0.02467168,  0.02451272,  0.02538406,  0.00658017,\n",
            "           0.01347701,  0.01414546,  0.01570525,  0.01550094,\n",
            "          -0.00079654,  0.02188141,  0.01226082,  0.01318491,\n",
            "           0.01133393,  0.01556333,  0.02539704,  0.01324551,\n",
            "           0.02318135,  0.01117328,  0.01302498,  0.01686782,\n",
            "           0.01942463,  0.00820328,  0.0125973 ,  0.02328101,\n",
            "           0.01910487,  0.01645717,  0.02173702,  0.03239977]]]],\n",
            "      dtype=float32), array([0.00791286, 0.00664543, 0.00634012, 0.00722591, 0.00768217,\n",
            "       0.00607533, 0.00600228, 0.00820303, 0.00777486, 0.00858091,\n",
            "       0.00843693, 0.00860894, 0.00495559, 0.00644935, 0.00439579,\n",
            "       0.00824613, 0.00714699, 0.00764531, 0.00800015, 0.00885637,\n",
            "       0.00748831, 0.00748187, 0.00614393, 0.00673403, 0.00580411,\n",
            "       0.00628124, 0.0079714 , 0.00603828, 0.00706088, 0.00539738,\n",
            "       0.0066829 , 0.00670133], dtype=float32), array([[ 6.1856786e-04, -2.4534186e-04,  6.6978426e-04, ...,\n",
            "        -8.0483733e-04,  2.5066894e-03,  6.5216440e-04],\n",
            "       [ 2.0088295e-03, -4.2197676e-04,  1.7060770e-03, ...,\n",
            "        -1.1494471e-03,  6.6866004e-04, -4.5192245e-04],\n",
            "       [ 6.0756796e-04, -9.8734244e-04, -2.0751080e-03, ...,\n",
            "         2.1406098e-03, -5.2572748e-05, -5.0819246e-04],\n",
            "       ...,\n",
            "       [-2.0412509e-03, -5.4487697e-04, -1.4362830e-03, ...,\n",
            "        -2.9009562e-03,  1.7703994e-03, -1.5783275e-03],\n",
            "       [-9.8248001e-04, -3.5391094e-03,  1.1219664e-03, ...,\n",
            "         8.2312821e-04,  1.4958800e-03, -1.5563300e-03],\n",
            "       [-1.1778235e-03, -5.9321790e-04, -5.2123910e-06, ...,\n",
            "         4.5230514e-05, -1.6094951e-03,  1.8281366e-03]], dtype=float32), array([0.00221744, 0.0020619 , 0.00186364, 0.00101891, 0.00139873,\n",
            "       0.00202919, 0.00202261, 0.0022134 , 0.00150688, 0.00259773,\n",
            "       0.00239793, 0.00177225, 0.00164881, 0.0026467 , 0.00152347,\n",
            "       0.00197099, 0.00413277, 0.00232837, 0.00147915, 0.00080917,\n",
            "       0.00107103, 0.0011233 , 0.00094483, 0.0012241 , 0.00234105,\n",
            "       0.00219342, 0.00216899, 0.00046742, 0.00283221, 0.00146803,\n",
            "       0.00064305, 0.00226656, 0.00236333, 0.00261148, 0.00245453,\n",
            "       0.00232736, 0.00230149, 0.00218355, 0.00254268, 0.00240096,\n",
            "       0.00304351, 0.00110904, 0.00204078, 0.00219072, 0.00281837,\n",
            "       0.00190702, 0.00280661, 0.0013865 , 0.00256553, 0.00153939,\n",
            "       0.0022145 , 0.0014683 , 0.00151678, 0.00221632, 0.00206562,\n",
            "       0.00182502, 0.00293344, 0.00191361, 0.0014366 , 0.00165379,\n",
            "       0.0018765 , 0.00184492, 0.00234792, 0.00213479, 0.00217167,\n",
            "       0.00198494, 0.00190697, 0.00185022, 0.00160692, 0.00257179,\n",
            "       0.00225655, 0.00175707, 0.00113932, 0.00102609, 0.002165  ,\n",
            "       0.00219698, 0.00200279, 0.0023175 , 0.00165136, 0.00196189,\n",
            "       0.00337929, 0.00327299, 0.00219381, 0.00179419, 0.00265797,\n",
            "       0.00236423, 0.00206318, 0.00129626, 0.00155956, 0.00075998,\n",
            "       0.00283353, 0.00137045, 0.00306412, 0.00249581, 0.00282707,\n",
            "       0.00179229, 0.00462045, 0.0025543 , 0.00335422, 0.00207876,\n",
            "       0.00194944, 0.00114239, 0.00218997, 0.00175528, 0.00225806,\n",
            "       0.00121856, 0.00244316, 0.00325789, 0.00202544, 0.00162824,\n",
            "       0.00232493, 0.00244552, 0.00106317, 0.00173956, 0.0022284 ,\n",
            "       0.0020846 , 0.00214001, 0.00309466, 0.00229742, 0.00238161,\n",
            "       0.00216526, 0.00279532, 0.00214244, 0.0022297 , 0.00195882,\n",
            "       0.00139459, 0.00311958, 0.00154858], dtype=float32), array([[ 0.00405737,  0.00499256,  0.00716525, ...,  0.00652536,\n",
            "        -0.00363511,  0.01416043],\n",
            "       [ 0.00463203, -0.00036774, -0.00145562, ..., -0.00087343,\n",
            "         0.02038709,  0.00579013],\n",
            "       [ 0.00201285, -0.01893913, -0.00842966, ..., -0.0085602 ,\n",
            "         0.0100541 ,  0.00867455],\n",
            "       ...,\n",
            "       [ 0.00040482, -0.01158772,  0.01632705, ...,  0.00729322,\n",
            "         0.01320907,  0.00282498],\n",
            "       [ 0.00353433,  0.01540238,  0.00466592, ..., -0.01783254,\n",
            "         0.01437582,  0.00734646],\n",
            "       [ 0.01462393, -0.00985311,  0.00397546, ...,  0.00544713,\n",
            "         0.01380046,  0.00708707]], dtype=float32), array([-0.01321435,  0.04472741, -0.00518561, -0.00074014, -0.0018087 ,\n",
            "       -0.01687076, -0.0046033 ,  0.01223934, -0.01256207, -0.00198182],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "global model stayed the same?\n",
            "False\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4120 - loss: 2.2855\n",
            "Global Model Accuracy: 0.4080\n"
          ]
        }
      ],
      "source": [
        "########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay0ud45DHqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      4 |             - |             -\n",
            "     2 |      100 |      6 |  0.2727559400 |         ideal\n",
            "     3 |      150 |      9 |  0.0710064480 |         ideal\n",
            "Device 4.0: Loss before: [2.299807071685791, 0.12333333492279053], Loss after: [2.2950029373168945, 0.12333333492279053]\n",
            "Device 5.0: Loss before: [2.300266981124878, 0.10833333432674408], Loss after: [2.295975685119629, 0.20333333313465118]\n",
            "Device 6.0: Loss before: [2.3000829219818115, 0.1133333370089531], Loss after: [2.296647548675537, 0.1133333370089531]\n",
            "Device 7.0: Loss before: [2.299997329711914, 0.10333333164453506], Loss after: [2.294761896133423, 0.13500000536441803]\n",
            "Device 10.0: Loss before: [2.2999212741851807, 0.11666666716337204], Loss after: [2.2957682609558105, 0.15000000596046448]\n",
            "Device 11.0: Loss before: [2.3003227710723877, 0.1133333370089531], Loss after: [2.296252965927124, 0.2266666740179062]\n",
            "Device 13.0: Loss before: [2.2998769283294678, 0.11999999731779099], Loss after: [2.292170763015747, 0.23000000417232513]\n",
            "Device 26.0: Loss before: [2.3004539012908936, 0.10833333432674408], Loss after: [2.2967872619628906, 0.21166667342185974]\n",
            "Device 31.0: Loss before: [2.3000717163085938, 0.11999999731779099], Loss after: [2.295858144760132, 0.12333333492279053]\n",
            "Device 33.0: Loss before: [2.3001513481140137, 0.11500000208616257], Loss after: [2.2962687015533447, 0.11500000208616257]\n",
            "Device 36.0: Loss before: [2.300018548965454, 0.11166666448116302], Loss after: [2.2950491905212402, 0.2266666740179062]\n",
            "Device 42.0: Loss before: [2.2997279167175293, 0.12666666507720947], Loss after: [2.294978618621826, 0.12666666507720947]\n",
            "Device 43.0: Loss before: [2.3002068996429443, 0.11166666448116302], Loss after: [2.296827793121338, 0.1483333259820938]\n",
            "Device 44.0: Loss before: [2.300229072570801, 0.11500000208616257], Loss after: [2.2959046363830566, 0.21666666865348816]\n",
            "Device 46.0: Loss before: [2.2997443675994873, 0.11833333224058151], Loss after: [2.2947025299072266, 0.13333334028720856]\n",
            "Device 47.0: Loss before: [2.3001575469970703, 0.1066666692495346], Loss after: [2.2964301109313965, 0.2750000059604645]\n",
            "Device 50.0: Loss before: [2.300027847290039, 0.12166666984558105], Loss after: [2.2961578369140625, 0.12833333015441895]\n",
            "Device 51.0: Loss before: [2.3001415729522705, 0.10833333432674408], Loss after: [2.2955944538116455, 0.1666666716337204]\n",
            "Device 52.0: Loss before: [2.300358772277832, 0.11500000208616257], Loss after: [2.2980241775512695, 0.11500000208616257]\n",
            "Device 53.0: Loss before: [2.300273895263672, 0.11999999731779099], Loss after: [2.2957959175109863, 0.11999999731779099]\n",
            "Device 54.0: Loss before: [2.300271987915039, 0.11166666448116302], Loss after: [2.297130823135376, 0.11500000208616257]\n",
            "Device 55.0: Loss before: [2.2997541427612305, 0.13333334028720856], Loss after: [2.2947449684143066, 0.13333334028720856]\n",
            "Device 58.0: Loss before: [2.300499439239502, 0.10999999940395355], Loss after: [2.296035051345825, 0.2750000059604645]\n",
            "Device 61.0: Loss before: [2.3002665042877197, 0.10833333432674408], Loss after: [2.296799898147583, 0.19333332777023315]\n",
            "Device 62.0: Loss before: [2.300647497177124, 0.0949999988079071], Loss after: [2.2955031394958496, 0.12166666984558105]\n",
            "Device 64.0: Loss before: [2.2996182441711426, 0.14000000059604645], Loss after: [2.2935292720794678, 0.14000000059604645]\n",
            "Device 68.0: Loss before: [2.300259828567505, 0.10999999940395355], Loss after: [2.2954745292663574, 0.20999999344348907]\n",
            "Device 71.0: Loss before: [2.300182342529297, 0.10999999940395355], Loss after: [2.2962915897369385, 0.2150000035762787]\n",
            "Device 72.0: Loss before: [2.3002490997314453, 0.10833333432674408], Loss after: [2.296476364135742, 0.1899999976158142]\n",
            "Device 75.0: Loss before: [2.3002707958221436, 0.10499999672174454], Loss after: [2.29699444770813, 0.27166667580604553]\n",
            "Device 77.0: Loss before: [2.300211191177368, 0.10499999672174454], Loss after: [2.2960739135742188, 0.31166666746139526]\n",
            "Device 81.0: Loss before: [2.3003127574920654, 0.09833333641290665], Loss after: [2.295391321182251, 0.22166666388511658]\n",
            "Device 84.0: Loss before: [2.3002877235412598, 0.10833333432674408], Loss after: [2.2973263263702393, 0.18000000715255737]\n",
            "Device 86.0: Loss before: [2.3003811836242676, 0.10999999940395355], Loss after: [2.297664165496826, 0.11833333224058151]\n",
            "Device 88.0: Loss before: [2.2999584674835205, 0.11833333224058151], Loss after: [2.2953388690948486, 0.1599999964237213]\n",
            "Device 89.0: Loss before: [2.300579071044922, 0.10166666656732559], Loss after: [2.296921968460083, 0.11666666716337204]\n",
            "Device 91.0: Loss before: [2.3001792430877686, 0.09833333641290665], Loss after: [2.2938570976257324, 0.12333333492279053]\n",
            "Device 94.0: Loss before: [2.2999751567840576, 0.10833333432674408], Loss after: [2.295044422149658, 0.12666666507720947]\n",
            "Device 98.0: Loss before: [2.300133228302002, 0.10166666656732559], Loss after: [2.29656720161438, 0.3083333373069763]\n",
            "[array([[[[ 0.0104019 ,  0.01456954,  0.00742552,  0.01006455,\n",
            "           0.00196505,  0.00970365,  0.00921195,  0.00589242,\n",
            "           0.01472005,  0.01065658,  0.00970208,  0.01202225,\n",
            "           0.00804764,  0.01056569,  0.0054691 ,  0.00751565,\n",
            "           0.01030334,  0.01216461,  0.00473745,  0.00626705,\n",
            "           0.01339198,  0.00522474,  0.00119633,  0.00884867,\n",
            "           0.01019728,  0.01293183,  0.00961764,  0.01703923,\n",
            "           0.00305167,  0.0054794 ,  0.0073023 ,  0.00975773]],\n",
            "\n",
            "        [[ 0.01593596,  0.01943301,  0.00562483,  0.00925633,\n",
            "           0.00869492,  0.01073792,  0.01854609,  0.00767518,\n",
            "           0.01033074,  0.01521997,  0.01895976,  0.01554399,\n",
            "           0.00193141,  0.00875781,  0.01158874,  0.01220714,\n",
            "           0.01339054,  0.0158137 ,  0.01310139,  0.01216915,\n",
            "           0.01520795,  0.00718198,  0.01468034,  0.01170296,\n",
            "           0.00630018,  0.01135292,  0.00833906,  0.01557345,\n",
            "           0.00472391,  0.01112853,  0.01319045,  0.02256079]],\n",
            "\n",
            "        [[ 0.01426401,  0.00987343,  0.01366727,  0.01089505,\n",
            "           0.00409848,  0.01220435,  0.01657431,  0.01291508,\n",
            "           0.01302245,  0.01019731,  0.01269388,  0.01280491,\n",
            "           0.01136292,  0.00263244,  0.00795869,  0.0207912 ,\n",
            "           0.01389933,  0.00952451,  0.00784343,  0.01332998,\n",
            "           0.00950352,  0.01176247,  0.00971634,  0.01384273,\n",
            "           0.00286237,  0.01305851,  0.01087173,  0.00916796,\n",
            "           0.00542504,  0.00844987,  0.01636345,  0.01453124]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01078696,  0.01156245,  0.00662137,  0.01439027,\n",
            "           0.00771842,  0.00538942,  0.01745842,  0.00884285,\n",
            "           0.01565097,  0.01016162,  0.00912837,  0.0127849 ,\n",
            "           0.00429155,  0.00606149,  0.00423488,  0.01184328,\n",
            "           0.00989805,  0.01790188,  0.01301793,  0.01212391,\n",
            "           0.01292737,  0.00889904,  0.00331596,  0.01996118,\n",
            "           0.01118324,  0.01207359,  0.01639264,  0.00089005,\n",
            "           0.01041301,  0.00856566,  0.00373119,  0.01702244]],\n",
            "\n",
            "        [[ 0.01602692,  0.02452437,  0.01236583,  0.01995027,\n",
            "           0.00356969,  0.00820757,  0.02216992,  0.01513946,\n",
            "           0.01901511,  0.01632972,  0.01770663,  0.02228894,\n",
            "           0.01166364,  0.0064906 ,  0.01400202,  0.00853262,\n",
            "           0.01349386,  0.00633783,  0.0089154 ,  0.01488859,\n",
            "           0.01828968,  0.01519994,  0.00754023,  0.01586953,\n",
            "           0.00315866,  0.01427518,  0.01343647,  0.01363066,\n",
            "           0.00798557,  0.0145227 ,  0.01022577,  0.02084502]],\n",
            "\n",
            "        [[ 0.01374343,  0.00630915,  0.01011283,  0.01292864,\n",
            "           0.00668546,  0.00586009,  0.01832122,  0.01016811,\n",
            "           0.01498356,  0.01968579,  0.00822673,  0.01485611,\n",
            "           0.00724131,  0.01456667,  0.00732778,  0.01357138,\n",
            "           0.00979961,  0.01272419,  0.01147504,  0.01416478,\n",
            "           0.01887767,  0.0033983 ,  0.01151099,  0.01806353,\n",
            "           0.00800811,  0.01397651,  0.00543812,  0.01347338,\n",
            "           0.00991374,  0.01025811,  0.01599242,  0.01167826]]],\n",
            "\n",
            "\n",
            "       [[[ 0.02051281,  0.01034913,  0.01654332,  0.01697725,\n",
            "           0.00485939,  0.00522392,  0.01232276,  0.0100379 ,\n",
            "           0.02026369,  0.01365456,  0.00937665,  0.01341158,\n",
            "           0.00496849,  0.01285653,  0.01390275,  0.01851111,\n",
            "           0.00624406,  0.00658414,  0.01067102,  0.01046084,\n",
            "           0.0164174 ,  0.00927531,  0.0088514 ,  0.01068916,\n",
            "           0.01533288,  0.01899562,  0.01532307,  0.00373424,\n",
            "           0.00847885,  0.00798894, -0.00194863,  0.01205001]],\n",
            "\n",
            "        [[ 0.01417176,  0.01764766,  0.01085867,  0.01930618,\n",
            "           0.00643319,  0.01579035,  0.01396455,  0.00914675,\n",
            "           0.01157923,  0.01563327,  0.01649658,  0.01438573,\n",
            "           0.00834186,  0.0132277 ,  0.01517621,  0.0033797 ,\n",
            "           0.00991052,  0.01146418,  0.00991145,  0.01840129,\n",
            "           0.01318629,  0.00705661,  0.00745414,  0.01258648,\n",
            "           0.01308753,  0.00852461,  0.0079097 ,  0.00899327,\n",
            "           0.01045365,  0.01164923,  0.00664562,  0.02014691]],\n",
            "\n",
            "        [[ 0.00191969,  0.01076267,  0.01221889,  0.0090207 ,\n",
            "           0.01427533,  0.01403929,  0.01440518,  0.00348824,\n",
            "           0.00737666,  0.00783006,  0.00879456,  0.00863848,\n",
            "          -0.00075743,  0.01252902,  0.00689395,  0.0073352 ,\n",
            "           0.00627308,  0.00876543,  0.014604  ,  0.00734404,\n",
            "           0.01315636,  0.00632309,  0.00731758,  0.00942484,\n",
            "           0.01105128,  0.00439249,  0.00709061,  0.01332083,\n",
            "           0.01092225,  0.00929628,  0.01248947,  0.01854364]]]],\n",
            "      dtype=float32), array([0.00465415, 0.00397673, 0.00375721, 0.00428502, 0.00454075,\n",
            "       0.00361776, 0.0035569 , 0.00482542, 0.00460004, 0.00508499,\n",
            "       0.00498827, 0.00511495, 0.00293645, 0.00383211, 0.00261024,\n",
            "       0.00489413, 0.00421999, 0.00451936, 0.00474781, 0.00525738,\n",
            "       0.00444016, 0.00440599, 0.00364983, 0.00400981, 0.00342281,\n",
            "       0.00371487, 0.00471957, 0.00357946, 0.00418598, 0.00320994,\n",
            "       0.00394134, 0.00397156], dtype=float32), array([[ 3.6681062e-04, -1.4583535e-04,  3.9760952e-04, ...,\n",
            "        -4.7728702e-04,  1.4868348e-03,  3.8698854e-04],\n",
            "       [ 1.1917278e-03, -2.5060234e-04,  1.0124458e-03, ...,\n",
            "        -6.8178750e-04,  3.9633462e-04, -2.6810751e-04],\n",
            "       [ 3.6032189e-04, -5.8604171e-04, -1.2310670e-03, ...,\n",
            "         1.2702992e-03, -3.1574811e-05, -3.0150122e-04],\n",
            "       ...,\n",
            "       [-1.2112960e-03, -3.2347423e-04, -8.5204776e-04, ...,\n",
            "        -1.7210768e-03,  1.0501067e-03, -9.3646022e-04],\n",
            "       [-5.8312382e-04, -2.1000903e-03,  6.6588179e-04, ...,\n",
            "         4.8858067e-04,  8.8715175e-04, -9.2340144e-04],\n",
            "       [-6.9903728e-04, -3.5219148e-04, -2.9083481e-06, ...,\n",
            "         2.7017411e-05, -9.5537433e-04,  1.0847109e-03]], dtype=float32), array([0.00128909, 0.00118923, 0.00113294, 0.00071236, 0.00090447,\n",
            "       0.00121044, 0.00121728, 0.0012375 , 0.00095398, 0.00147698,\n",
            "       0.00140723, 0.00108495, 0.0010603 , 0.0014271 , 0.00094172,\n",
            "       0.00118167, 0.00213161, 0.0013336 , 0.00092737, 0.00060172,\n",
            "       0.00070161, 0.00072747, 0.00058713, 0.00079406, 0.00133779,\n",
            "       0.00125134, 0.00124731, 0.00034545, 0.00156327, 0.00091389,\n",
            "       0.00047486, 0.0013075 , 0.00141438, 0.00147921, 0.00139141,\n",
            "       0.00133543, 0.0013418 , 0.00128633, 0.00146111, 0.00141494,\n",
            "       0.00160511, 0.00070875, 0.00119895, 0.00130648, 0.00147576,\n",
            "       0.00110767, 0.00159399, 0.00086182, 0.00151201, 0.00089193,\n",
            "       0.00120259, 0.00086762, 0.00090875, 0.00131156, 0.00117797,\n",
            "       0.00111562, 0.00167233, 0.00111172, 0.00089701, 0.00102896,\n",
            "       0.00116174, 0.00110378, 0.0013972 , 0.00129714, 0.00129931,\n",
            "       0.0012102 , 0.0011174 , 0.00106868, 0.00095073, 0.00147761,\n",
            "       0.00131423, 0.00112868, 0.00073378, 0.00065153, 0.00123782,\n",
            "       0.00124008, 0.00115739, 0.00137182, 0.00100195, 0.00118333,\n",
            "       0.00182942, 0.00192452, 0.00127278, 0.00106348, 0.00149509,\n",
            "       0.00136858, 0.00125677, 0.00084337, 0.00095395, 0.00052945,\n",
            "       0.00151927, 0.00087255, 0.00174823, 0.00139196, 0.00166854,\n",
            "       0.00104553, 0.00233361, 0.00144833, 0.00187016, 0.00121135,\n",
            "       0.00118327, 0.00078204, 0.00124561, 0.00104176, 0.00127002,\n",
            "       0.00079021, 0.00138154, 0.00178031, 0.00119175, 0.0009876 ,\n",
            "       0.00142686, 0.00141415, 0.0006916 , 0.00104696, 0.00132584,\n",
            "       0.00127337, 0.00122951, 0.00180446, 0.00133934, 0.00141107,\n",
            "       0.00122937, 0.00161839, 0.00129005, 0.00127878, 0.00116723,\n",
            "       0.00086022, 0.00179071, 0.00092326], dtype=float32), array([[ 2.13568867e-03,  3.18165310e-03,  4.08263551e-03, ...,\n",
            "         4.07267082e-03, -2.24842853e-03,  8.43883306e-03],\n",
            "       [ 2.64348811e-03, -1.72151427e-04, -1.01661216e-03, ...,\n",
            "        -4.41190496e-04,  1.16254175e-02,  3.57058551e-03],\n",
            "       [ 7.75172433e-04, -1.04363961e-02, -5.15866512e-03, ...,\n",
            "        -4.85106139e-03,  5.72772045e-03,  5.07691130e-03],\n",
            "       ...,\n",
            "       [ 4.20858305e-05, -6.05735276e-03,  9.36947949e-03, ...,\n",
            "         4.36634663e-03,  7.51795853e-03,  1.56478002e-03],\n",
            "       [ 1.90698740e-03,  8.96375533e-03,  2.56475597e-03, ...,\n",
            "        -1.01733375e-02,  8.05408880e-03,  4.53115581e-03],\n",
            "       [ 7.94902630e-03, -5.19113103e-03,  2.40813335e-03, ...,\n",
            "         2.97831232e-03,  7.84119405e-03,  4.22628317e-03]], dtype=float32), array([-0.00695067,  0.02232892, -0.00294635, -0.00083978, -0.00034255,\n",
            "       -0.0072303 , -0.00224063,  0.00614844, -0.00703885, -0.00088823],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "[array([[[[ 1.21182529e-02,  1.63573883e-02,  8.67665280e-03,\n",
            "           1.17328763e-02,  2.67642783e-03,  1.08325183e-02,\n",
            "           1.10950759e-02,  7.14898482e-03,  1.65738408e-02,\n",
            "           1.22504486e-02,  1.12094190e-02,  1.37285693e-02,\n",
            "           9.00254492e-03,  1.16680600e-02,  6.49694121e-03,\n",
            "           8.98756739e-03,  1.18654892e-02,  1.36185978e-02,\n",
            "           5.84824290e-03,  7.74353510e-03,  1.50376456e-02,\n",
            "           6.28681527e-03,  2.11141491e-03,  1.05102109e-02,\n",
            "           1.12862764e-02,  1.45274121e-02,  1.10279499e-02,\n",
            "           1.84062086e-02,  3.96798830e-03,  6.65903138e-03,\n",
            "           8.37431755e-03,  1.17310900e-02]],\n",
            "\n",
            "        [[ 1.80052985e-02,  2.16533113e-02,  7.22988416e-03,\n",
            "           1.12609761e-02,  9.67558008e-03,  1.21953348e-02,\n",
            "           2.09618397e-02,  9.25050303e-03,  1.25634540e-02,\n",
            "           1.72268059e-02,  2.08329260e-02,  1.76623259e-02,\n",
            "           3.02086445e-03,  1.01170680e-02,  1.29048089e-02,\n",
            "           1.41092865e-02,  1.53031982e-02,  1.75514221e-02,\n",
            "           1.45809958e-02,  1.40822437e-02,  1.72936749e-02,\n",
            "           8.48012511e-03,  1.59723684e-02,  1.37686841e-02,\n",
            "           7.61063257e-03,  1.32672070e-02,  9.98144224e-03,\n",
            "           1.73011255e-02,  5.89588284e-03,  1.26666930e-02,\n",
            "           1.46402130e-02,  2.50883345e-02]],\n",
            "\n",
            "        [[ 1.61397737e-02,  1.18925087e-02,  1.52337141e-02,\n",
            "           1.27362041e-02,  5.09313866e-03,  1.36335315e-02,\n",
            "           1.88609250e-02,  1.43831503e-02,  1.50702912e-02,\n",
            "           1.20790433e-02,  1.44477282e-02,  1.47674270e-02,\n",
            "           1.23862904e-02,  3.90453381e-03,  9.20919701e-03,\n",
            "           2.26256680e-02,  1.56793520e-02,  1.10768499e-02,\n",
            "           9.23707616e-03,  1.51347127e-02,  1.14448396e-02,\n",
            "           1.29393581e-02,  1.09805865e-02,  1.57953110e-02,\n",
            "           4.03986080e-03,  1.47578828e-02,  1.22957528e-02,\n",
            "           1.08215911e-02,  6.54364703e-03,  9.89246368e-03,\n",
            "           1.78253707e-02,  1.68477986e-02]]],\n",
            "\n",
            "\n",
            "       [[[ 1.28728850e-02,  1.37301320e-02,  8.25876370e-03,\n",
            "           1.64524168e-02,  8.66709650e-03,  6.79375604e-03,\n",
            "           1.98267065e-02,  1.03876004e-02,  1.79524738e-02,\n",
            "           1.21227633e-02,  1.09802103e-02,  1.48718851e-02,\n",
            "           5.34445187e-03,  7.47434329e-03,  5.59306331e-03,\n",
            "           1.36473887e-02,  1.16441362e-02,  1.96415298e-02,\n",
            "           1.45181306e-02,  1.39996484e-02,  1.49831902e-02,\n",
            "           1.02458550e-02,  4.48654406e-03,  2.20088437e-02,\n",
            "           1.26540493e-02,  1.40311876e-02,  1.81110632e-02,\n",
            "           2.42608390e-03,  1.16397599e-02,  1.00354617e-02,\n",
            "           4.93575586e-03,  1.94645040e-02]],\n",
            "\n",
            "        [[ 1.83133837e-02,  2.70069111e-02,  1.42847775e-02,\n",
            "           2.22402588e-02,  4.74569062e-03,  9.89397056e-03,\n",
            "           2.48850398e-02,  1.69132333e-02,  2.15565227e-02,\n",
            "           1.86059345e-02,  1.98173970e-02,  2.46623848e-02,\n",
            "           1.28461923e-02,  8.10622051e-03,  1.55664375e-02,\n",
            "           1.05996020e-02,  1.54930372e-02,  8.25758185e-03,\n",
            "           1.06653310e-02,  1.70669444e-02,  2.06681639e-02,\n",
            "           1.66573245e-02,  9.00477730e-03,  1.82127226e-02,\n",
            "           4.79177386e-03,  1.64212510e-02,  1.52541408e-02,\n",
            "           1.54946856e-02,  9.42171831e-03,  1.62658747e-02,\n",
            "           1.17848385e-02,  2.36491431e-02]],\n",
            "\n",
            "        [[ 1.56739131e-02,  8.45608860e-03,  1.18624186e-02,\n",
            "           1.49064977e-02,  7.82271754e-03,  7.44200405e-03,\n",
            "           2.07436476e-02,  1.17178857e-02,  1.71820428e-02,\n",
            "           2.17454787e-02,  1.00927744e-02,  1.69356856e-02,\n",
            "           8.29244219e-03,  1.60379298e-02,  8.70789401e-03,\n",
            "           1.54197505e-02,  1.15606301e-02,  1.43950861e-02,\n",
            "           1.30489133e-02,  1.60808470e-02,  2.09946539e-02,\n",
            "           4.59409086e-03,  1.28826154e-02,  2.01769043e-02,\n",
            "           9.41383187e-03,  1.57615505e-02,  6.88687200e-03,\n",
            "           1.52098211e-02,  1.12173250e-02,  1.18142152e-02,\n",
            "           1.75056905e-02,  1.41084855e-02]]],\n",
            "\n",
            "\n",
            "       [[[ 2.24370286e-02,  1.23270527e-02,  1.81447528e-02,\n",
            "           1.89111214e-02,  5.79631887e-03,  6.57070940e-03,\n",
            "           1.45139378e-02,  1.14367837e-02,  2.24035587e-02,\n",
            "           1.54711371e-02,  1.10887103e-02,  1.53395087e-02,\n",
            "           5.88608906e-03,  1.42732430e-02,  1.52540877e-02,\n",
            "           2.01598443e-02,  7.71424966e-03,  8.08963832e-03,\n",
            "           1.21246278e-02,  1.22203538e-02,  1.83674395e-02,\n",
            "           1.05232121e-02,  1.00082234e-02,  1.25427488e-02,\n",
            "           1.68453213e-02,  2.08139550e-02,  1.69244427e-02,\n",
            "           5.08035906e-03,  9.72061325e-03,  9.40927770e-03,\n",
            "          -9.48040746e-04,  1.43114002e-02]],\n",
            "\n",
            "        [[ 1.61154009e-02,  1.98248792e-02,  1.26153994e-02,\n",
            "           2.13603210e-02,  7.54904933e-03,  1.73656512e-02,\n",
            "           1.63247380e-02,  1.06472000e-02,  1.37913506e-02,\n",
            "           1.76258143e-02,  1.83601901e-02,  1.64582580e-02,\n",
            "           9.33977775e-03,  1.47721814e-02,  1.66194215e-02,\n",
            "           5.10831038e-03,  1.15265111e-02,  1.31048588e-02,\n",
            "           1.15134725e-02,  2.03307290e-02,  1.53186545e-02,\n",
            "           8.30584392e-03,  8.78737401e-03,  1.46314707e-02,\n",
            "           1.47016374e-02,  1.03752911e-02,  9.48706362e-03,\n",
            "           1.05760470e-02,  1.18365996e-02,  1.32561913e-02,\n",
            "           7.93357007e-03,  2.26261951e-02]],\n",
            "\n",
            "        [[ 3.33725964e-03,  1.25108045e-02,  1.36949923e-02,\n",
            "           1.06444350e-02,  1.52993118e-02,  1.54421311e-02,\n",
            "           1.63252186e-02,  4.66009788e-03,  9.11513530e-03,\n",
            "           9.46675614e-03,  1.03040356e-02,  1.02759013e-02,\n",
            "           3.51090348e-05,  1.38375806e-02,  8.02964251e-03,\n",
            "           8.72049667e-03,  7.55911693e-03,  1.01069473e-02,\n",
            "           1.59462523e-02,  8.84446315e-03,  1.48971146e-02,\n",
            "           7.24869361e-03,  8.47055856e-03,  1.11099649e-02,\n",
            "           1.23502696e-02,  5.76542271e-03,  8.22434016e-03,\n",
            "           1.47076799e-02,  1.20936166e-02,  1.06306737e-02,\n",
            "           1.36928968e-02,  2.05352511e-02]]]], dtype=float32), array([0.00475102, 0.00393988, 0.00378099, 0.00429981, 0.00460198,\n",
            "       0.0036069 , 0.00358696, 0.00492608, 0.00463988, 0.00511853,\n",
            "       0.00504873, 0.00511811, 0.00295048, 0.00383079, 0.00261328,\n",
            "       0.0048966 , 0.00428295, 0.00457394, 0.00476533, 0.00527168,\n",
            "       0.00445546, 0.00450613, 0.00364829, 0.00399976, 0.0034926 ,\n",
            "       0.00374149, 0.00474993, 0.00360659, 0.00421629, 0.00320742,\n",
            "       0.00401485, 0.0040056 ], dtype=float32), array([[ 3.6710044e-04, -1.4529648e-04,  3.9716769e-04, ...,\n",
            "        -4.7775070e-04,  1.4876317e-03,  3.8697990e-04],\n",
            "       [ 1.1919722e-03, -2.5014937e-04,  1.0120794e-03, ...,\n",
            "        -6.8218133e-04,  3.9700602e-04, -2.6811738e-04],\n",
            "       [ 3.6055554e-04, -5.8560644e-04, -1.2314193e-03, ...,\n",
            "         1.2699252e-03, -3.0932853e-05, -3.0150314e-04],\n",
            "       ...,\n",
            "       [-1.2110772e-03, -3.2311666e-04, -8.5235864e-04, ...,\n",
            "        -1.7213591e-03,  1.0506598e-03, -9.3644473e-04],\n",
            "       [-5.8285764e-04, -2.0996479e-03,  6.6549343e-04, ...,\n",
            "         4.8823122e-04,  8.8783813e-04, -9.2338881e-04],\n",
            "       [-6.9875387e-04, -3.5175911e-04, -3.2976104e-06, ...,\n",
            "         2.6691279e-05, -9.5468614e-04,  1.0847341e-03]], dtype=float32), array([0.00135097, 0.00130382, 0.00103502, 0.00038617, 0.00069273,\n",
            "       0.00119049, 0.00118134, 0.00164204, 0.00076894, 0.0016577 ,\n",
            "       0.00144265, 0.00127522, 0.00082229, 0.00202693, 0.00082323,\n",
            "       0.00115854, 0.00323945, 0.00145774, 0.00096166, 0.00021889,\n",
            "       0.00050687, 0.00053651, 0.00050048, 0.00057545, 0.00161835,\n",
            "       0.0014211 , 0.0013707 , 0.00038047, 0.00190582, 0.00078204,\n",
            "       0.000207  , 0.00140993, 0.00137399, 0.00180372, 0.00169142,\n",
            "       0.00148984, 0.00138357, 0.00130192, 0.00175311, 0.0014289 ,\n",
            "       0.00255009, 0.00056231, 0.00124577, 0.00134458, 0.00219615,\n",
            "       0.0012443 , 0.00182645, 0.00073046, 0.00153677, 0.00095302,\n",
            "       0.00157167, 0.00084285, 0.0009043 , 0.00133152, 0.00131991,\n",
            "       0.00101723, 0.00186007, 0.00118309, 0.00074613, 0.00088997,\n",
            "       0.00101156, 0.00114903, 0.00137692, 0.00118221, 0.0012548 ,\n",
            "       0.00109554, 0.00133815, 0.00114358, 0.0009427 , 0.00168502,\n",
            "       0.00138313, 0.00098364, 0.00055012, 0.00052304, 0.0014765 ,\n",
            "       0.00156938, 0.00122958, 0.00139305, 0.00092412, 0.00115579,\n",
            "       0.00264701, 0.00194003, 0.00135681, 0.00106605, 0.0020273 ,\n",
            "       0.00148743, 0.00114221, 0.00061528, 0.00085511, 0.00027733,\n",
            "       0.00219047, 0.00067775, 0.00193542, 0.00177744, 0.00168118,\n",
            "       0.0011002 , 0.00379513, 0.00166338, 0.00242246, 0.00125043,\n",
            "       0.00110899, 0.00047206, 0.00151306, 0.00105486, 0.00165373,\n",
            "       0.00059531, 0.00172565, 0.00247244, 0.00119525, 0.0009311 ,\n",
            "       0.00126783, 0.00152226, 0.00050608, 0.00100493, 0.00130414,\n",
            "       0.00116209, 0.00134004, 0.0019889 , 0.00142534, 0.00141837,\n",
            "       0.0015354 , 0.00168932, 0.00124742, 0.00141593, 0.00114582,\n",
            "       0.00075933, 0.00196107, 0.00092187], dtype=float32), array([[ 0.00258902,  0.00273877,  0.00442961, ...,  0.0037811 ,\n",
            "        -0.00214944,  0.00846101],\n",
            "       [ 0.00298207, -0.00031288, -0.00064571, ..., -0.00062153,\n",
            "         0.01275552,  0.0032003 ],\n",
            "       [ 0.00198457, -0.01268533, -0.0047578 , ..., -0.00546877,\n",
            "         0.00622755,  0.00526169],\n",
            "       ...,\n",
            "       [ 0.0005115 , -0.00813074,  0.01013657, ...,  0.00432257,\n",
            "         0.00820638,  0.0018573 ],\n",
            "       [ 0.00210609,  0.00966632,  0.00320352, ..., -0.01129136,\n",
            "         0.00912196,  0.00405199],\n",
            "       [ 0.01005624, -0.00692317,  0.00234586, ...,  0.00358066,\n",
            "         0.00869113,  0.00404181]], dtype=float32), array([-0.0104256 ,  0.04095443, -0.00216255,  0.00147252, -0.00460501,\n",
            "       -0.02058453, -0.00469623,  0.01130504, -0.0085853 , -0.00267277],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n",
            "global model stayed the same?\n",
            "False\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1405 - loss: 2.2965\n",
            "Global Model Accuracy: 0.1300\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEyWF6-HHqJw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      5 |             - |             -\n",
            "     2 |      100 |      6 |  0.0564924725 |         ideal\n",
            "     3 |      150 |      8 |  0.4483626535 |         ideal\n",
            "Device 4.0: Loss before: [2.3013622760772705, 0.12333333492279053], Loss after: [2.298837661743164, 0.12333333492279053]\n",
            "Device 5.0: Loss before: [2.3016862869262695, 0.10833333432674408], Loss after: [2.2995495796203613, 0.10833333432674408]\n",
            "Device 6.0: Loss before: [2.3016045093536377, 0.1133333370089531], Loss after: [2.300372362136841, 0.1133333370089531]\n",
            "Device 7.0: Loss before: [2.3015823364257812, 0.10333333164453506], Loss after: [2.2990756034851074, 0.12166666984558105]\n",
            "Device 10.0: Loss before: [2.3015029430389404, 0.11666666716337204], Loss after: [2.299830198287964, 0.11666666716337204]\n",
            "Device 11.0: Loss before: [2.3018038272857666, 0.1133333370089531], Loss after: [2.300283908843994, 0.1133333370089531]\n",
            "Device 13.0: Loss before: [2.3014400005340576, 0.11999999731779099], Loss after: [2.297279119491577, 0.25]\n",
            "Device 26.0: Loss before: [2.301870822906494, 0.10833333432674408], Loss after: [2.3003807067871094, 0.10833333432674408]\n",
            "Device 31.0: Loss before: [2.301523447036743, 0.11999999731779099], Loss after: [2.2993855476379395, 0.11999999731779099]\n",
            "Device 33.0: Loss before: [2.3016774654388428, 0.11500000208616257], Loss after: [2.299994707107544, 0.11500000208616257]\n",
            "Device 36.0: Loss before: [2.301600456237793, 0.11166666448116302], Loss after: [2.299229621887207, 0.1433333307504654]\n",
            "Device 42.0: Loss before: [2.301372766494751, 0.12666666507720947], Loss after: [2.2993016242980957, 0.12666666507720947]\n",
            "Device 43.0: Loss before: [2.301710367202759, 0.11166666448116302], Loss after: [2.3004438877105713, 0.11166666448116302]\n",
            "Device 44.0: Loss before: [2.301706552505493, 0.11500000208616257], Loss after: [2.299513339996338, 0.11500000208616257]\n",
            "Device 46.0: Loss before: [2.301340341567993, 0.11833333224058151], Loss after: [2.298771858215332, 0.11833333224058151]\n",
            "Device 47.0: Loss before: [2.301769733428955, 0.1066666692495346], Loss after: [2.3008229732513428, 0.1066666692495346]\n",
            "Device 50.0: Loss before: [2.301445245742798, 0.12166666984558105], Loss after: [2.299644708633423, 0.12166666984558105]\n",
            "Device 51.0: Loss before: [2.301565170288086, 0.10833333432674408], Loss after: [2.2990448474884033, 0.11999999731779099]\n",
            "Device 52.0: Loss before: [2.3016886711120605, 0.11500000208616257], Loss after: [2.3008017539978027, 0.11500000208616257]\n",
            "Device 53.0: Loss before: [2.3016433715820312, 0.11999999731779099], Loss after: [2.2987868785858154, 0.11999999731779099]\n",
            "Device 54.0: Loss before: [2.3017451763153076, 0.11166666448116302], Loss after: [2.3007490634918213, 0.11166666448116302]\n",
            "Device 55.0: Loss before: [2.301236629486084, 0.13333334028720856], Loss after: [2.2984275817871094, 0.13333334028720856]\n",
            "Device 58.0: Loss before: [2.3018391132354736, 0.10999999940395355], Loss after: [2.2992539405822754, 0.10999999940395355]\n",
            "Device 61.0: Loss before: [2.3017876148223877, 0.10833333432674408], Loss after: [2.300678014755249, 0.10833333432674408]\n",
            "Device 62.0: Loss before: [2.302037477493286, 0.0949999988079071], Loss after: [2.299220085144043, 0.12166666984558105]\n",
            "Device 64.0: Loss before: [2.3012399673461914, 0.14000000059604645], Loss after: [2.2974958419799805, 0.14000000059604645]\n",
            "Device 68.0: Loss before: [2.301774740219116, 0.10999999940395355], Loss after: [2.2996461391448975, 0.2133333384990692]\n",
            "Device 71.0: Loss before: [2.3016538619995117, 0.10999999940395355], Loss after: [2.3001623153686523, 0.10999999940395355]\n",
            "Device 72.0: Loss before: [2.301738977432251, 0.10833333432674408], Loss after: [2.3001327514648438, 0.10833333432674408]\n",
            "Device 75.0: Loss before: [2.3018112182617188, 0.10499999672174454], Loss after: [2.3009819984436035, 0.10499999672174454]\n",
            "Device 77.0: Loss before: [2.301687717437744, 0.10499999672174454], Loss after: [2.299941062927246, 0.14000000059604645]\n",
            "Device 81.0: Loss before: [2.301790475845337, 0.09833333641290665], Loss after: [2.299125909805298, 0.11666666716337204]\n",
            "Device 84.0: Loss before: [2.3017520904541016, 0.10833333432674408], Loss after: [2.3009040355682373, 0.10833333432674408]\n",
            "Device 86.0: Loss before: [2.30180287361145, 0.10999999940395355], Loss after: [2.3009963035583496, 0.10999999940395355]\n",
            "Device 88.0: Loss before: [2.301478862762451, 0.11833333224058151], Loss after: [2.2994086742401123, 0.11833333224058151]\n",
            "Device 89.0: Loss before: [2.3020639419555664, 0.10166666656732559], Loss after: [2.300623893737793, 0.11666666716337204]\n",
            "Device 91.0: Loss before: [2.301717758178711, 0.09833333641290665], Loss after: [2.298187494277954, 0.12333333492279053]\n",
            "Device 94.0: Loss before: [2.301567792892456, 0.10833333432674408], Loss after: [2.299182653427124, 0.12666666507720947]\n",
            "Device 98.0: Loss before: [2.3017964363098145, 0.10166666656732559], Loss after: [2.301105260848999, 0.125]\n",
            "[array([[[[ 0.00631241,  0.00860124,  0.00451629,  0.00611067,\n",
            "           0.00134348,  0.00570449,  0.0057319 ,  0.0036867 ,\n",
            "           0.00870861,  0.0064028 ,  0.00585124,  0.00718754,\n",
            "           0.00473826,  0.00616181,  0.00336784,  0.00465129,\n",
            "           0.0061988 ,  0.00716636,  0.00300328,  0.0039757 ,\n",
            "           0.00790696,  0.00324863,  0.00100589,  0.0054484 ,\n",
            "           0.00595673,  0.0076378 ,  0.00576759,  0.0097769 ,\n",
            "           0.00201289,  0.00343263,  0.00437958,  0.00606318]],\n",
            "\n",
            "        [[ 0.0094523 ,  0.01140833,  0.00367744,  0.00580337,\n",
            "           0.00509945,  0.00639371,  0.0110034 ,  0.00477816,\n",
            "           0.00647518,  0.0090395 ,  0.0110162 ,  0.00925876,\n",
            "           0.00147268,  0.00528122,  0.00680016,  0.00736421,\n",
            "           0.0080103 ,  0.00925665,  0.00768455,  0.00734787,\n",
            "           0.00906374,  0.0044025 ,  0.00846798,  0.00715449,\n",
            "           0.00392892,  0.00690553,  0.00516447,  0.00912239,\n",
            "           0.00301979,  0.00663711,  0.00772123,  0.01322497]],\n",
            "\n",
            "        [[ 0.00846977,  0.00614378,  0.00802541,  0.00662861,\n",
            "           0.00261134,  0.00717824,  0.00988334,  0.00757898,\n",
            "           0.00786334,  0.00626592,  0.00757043,  0.00771205,\n",
            "           0.0065635 ,  0.00192422,  0.00480529,  0.01199467,\n",
            "           0.00823459,  0.00577246,  0.00479858,  0.00793536,\n",
            "           0.00591277,  0.00684013,  0.00576417,  0.00827119,\n",
            "           0.00201206,  0.007747  ,  0.00645328,  0.00561853,\n",
            "           0.00337934,  0.00514667,  0.00944732,  0.00878666]]],\n",
            "\n",
            "\n",
            "       [[[ 0.00666544,  0.00711802,  0.00423068,  0.00861096,\n",
            "           0.00455723,  0.00347145,  0.01039482,  0.0053997 ,\n",
            "           0.00938824,  0.00627752,  0.00567436,  0.00774973,\n",
            "           0.0027388 ,  0.00383937,  0.00282711,  0.00712858,\n",
            "           0.00605067,  0.01039023,  0.00764731,  0.00730876,\n",
            "           0.00781487,  0.00535299,  0.00225544,  0.01162753,\n",
            "           0.00664052,  0.00731339,  0.00956319,  0.00108191,\n",
            "           0.00612749,  0.00522009,  0.00249395,  0.01018708]],\n",
            "\n",
            "        [[ 0.00958632,  0.01427266,  0.00745685,  0.01171612,\n",
            "           0.0023952 ,  0.0051103 ,  0.01308608,  0.00890493,\n",
            "           0.01130685,  0.0097466 ,  0.01042905,  0.01301739,\n",
            "           0.00678872,  0.00415124,  0.00820622,  0.00543507,\n",
            "           0.0081001 ,  0.00418698,  0.00551908,  0.00892662,\n",
            "           0.01084976,  0.00881435,  0.00466175,  0.00952306,\n",
            "           0.00235049,  0.00858125,  0.00799818,  0.0081216 ,\n",
            "           0.00489227,  0.0085584 ,  0.00615552,  0.01240203]],\n",
            "\n",
            "        [[ 0.0082087 ,  0.00426002,  0.00616855,  0.00778515,\n",
            "           0.00407039,  0.00379605,  0.01088401,  0.00612061,\n",
            "           0.00898599,  0.01148282,  0.00519076,  0.00887047,\n",
            "           0.00433835,  0.00847621,  0.00451357,  0.00808334,\n",
            "           0.00600309,  0.00755452,  0.00683902,  0.00843166,\n",
            "           0.01106676,  0.00230994,  0.00677965,  0.01062368,\n",
            "           0.00489263,  0.00827842,  0.00351519,  0.00798652,\n",
            "           0.0058866 ,  0.00617191,  0.00926606,  0.00728324]]],\n",
            "\n",
            "\n",
            "       [[[ 0.01187864,  0.00638579,  0.00959931,  0.00996437,\n",
            "           0.00300162,  0.00335922,  0.00753966,  0.00599113,\n",
            "           0.0118276 ,  0.00811606,  0.00575454,  0.00802767,\n",
            "           0.00305327,  0.00752719,  0.00806926,  0.01068528,\n",
            "           0.00396078,  0.00415904,  0.00635594,  0.00636122,\n",
            "           0.00966696,  0.00551854,  0.00525304,  0.00652177,\n",
            "           0.00890794,  0.01101427,  0.0089373 ,  0.00255081,\n",
            "           0.00508406,  0.00488793, -0.0006669 ,  0.00741907]],\n",
            "\n",
            "        [[ 0.0084462 ,  0.01042297,  0.00657595,  0.01127472,\n",
            "           0.00392519,  0.00918063,  0.00849624,  0.00554729,\n",
            "           0.00714446,  0.00925814,  0.0096762 ,  0.00861254,\n",
            "           0.00491465,  0.00777838,  0.00879595,  0.00250765,\n",
            "           0.00600669,  0.00685918,  0.00600175,  0.01073518,\n",
            "           0.00798518,  0.00431541,  0.00456383,  0.00762571,\n",
            "           0.00772949,  0.00534637,  0.00490618,  0.00549613,\n",
            "           0.00621046,  0.00694642,  0.00410755,  0.01189662]],\n",
            "\n",
            "        [[ 0.00159428,  0.00652052,  0.00720443,  0.00552699,\n",
            "           0.00814372,  0.00816342,  0.00856363,  0.00234939,\n",
            "           0.00467984,  0.00488612,  0.00535974,  0.00532497,\n",
            "          -0.00010182,  0.00730733,  0.00418289,  0.00451961,\n",
            "           0.0039047 ,  0.00527844,  0.00844615,  0.00456931,\n",
            "           0.00781622,  0.00379126,  0.00441945,  0.00577018,\n",
            "           0.00650193,  0.00291837,  0.00428881,  0.00776742,\n",
            "           0.00638215,  0.00556365,  0.00724487,  0.01083665]]]],\n",
            "      dtype=float32), array([0.00256266, 0.002143  , 0.00204756, 0.00233039, 0.00248724,\n",
            "       0.0019584 , 0.00194134, 0.00265705, 0.00251106, 0.00277169,\n",
            "       0.00272972, 0.00277609, 0.0015985 , 0.0020784 , 0.00141724,\n",
            "       0.00265603, 0.0023139 , 0.00247304, 0.00258252, 0.0028577 ,\n",
            "       0.00241476, 0.0024293 , 0.00197943, 0.0021714 , 0.00188409,\n",
            "       0.0020257 , 0.00257221, 0.00195244, 0.00228271, 0.00174041,\n",
            "       0.00216685, 0.00216785], dtype=float32), array([[ 1.99107788e-04, -7.89055193e-05,  2.15530861e-04, ...,\n",
            "        -2.59109045e-04,  8.06918659e-04,  2.09937920e-04],\n",
            "       [ 6.46607659e-04, -1.35775117e-04,  5.49108954e-04, ...,\n",
            "        -3.70023306e-04,  2.15273380e-04, -1.45452184e-04],\n",
            "       [ 1.95565750e-04, -3.17757862e-04, -6.67991233e-04, ...,\n",
            "         6.88991451e-04, -1.68789702e-05, -1.63565157e-04],\n",
            "       ...,\n",
            "       [-6.57042779e-04, -1.75345311e-04, -4.62357159e-04, ...,\n",
            "        -9.33794247e-04,  5.69898577e-04, -5.08023659e-04],\n",
            "       [-3.16240854e-04, -1.13912649e-03,  3.61089420e-04, ...,\n",
            "         2.64918723e-04,  4.81547526e-04, -5.00940369e-04],\n",
            "       [-3.79117177e-04, -1.90895254e-04, -1.72959108e-06, ...,\n",
            "         1.45297545e-05, -5.18022163e-04,  5.88464725e-04]], dtype=float32), array([0.00072346, 0.00068985, 0.00057643, 0.00025924, 0.0004081 ,\n",
            "       0.00064888, 0.00064636, 0.00082911, 0.00044537, 0.00087174,\n",
            "       0.00077724, 0.00066279, 0.00048239, 0.00100814, 0.00046467,\n",
            "       0.00063203, 0.00158846, 0.00077189, 0.00051647, 0.00017713,\n",
            "       0.00030467, 0.00032018, 0.00028473, 0.00034552, 0.00083517,\n",
            "       0.00074506, 0.00072479, 0.00020107, 0.00098167, 0.00044436,\n",
            "       0.00015315, 0.00074927, 0.00075155, 0.00092903, 0.00087185,\n",
            "       0.00078469, 0.00074421, 0.00070392, 0.00090653, 0.00077305,\n",
            "       0.00123932, 0.00032739, 0.00066869, 0.00072362, 0.00108155,\n",
            "       0.0006542 , 0.0009554 , 0.00041631, 0.00082992, 0.0005077 ,\n",
            "       0.00079635, 0.00046102, 0.00049126, 0.00071931, 0.00069441,\n",
            "       0.00056685, 0.00098046, 0.00063094, 0.00042778, 0.00050401,\n",
            "       0.00057167, 0.00061645, 0.00075007, 0.00065888, 0.00068752,\n",
            "       0.00061182, 0.00069228, 0.00060897, 0.00051264, 0.00088249,\n",
            "       0.00073984, 0.00055574, 0.00032645, 0.00030334, 0.0007646 ,\n",
            "       0.00080117, 0.00065604, 0.00075249, 0.0005132 , 0.00063121,\n",
            "       0.00131132, 0.0010501 , 0.00072325, 0.00057794, 0.00101865,\n",
            "       0.0007888 , 0.00063712, 0.00036857, 0.00047897, 0.0001889 ,\n",
            "       0.00108597, 0.00039739, 0.00102142, 0.00090547, 0.00091011,\n",
            "       0.00058852, 0.00183598, 0.00086959, 0.00122996, 0.0006724 ,\n",
            "       0.00061296, 0.00030336, 0.00078005, 0.00057027, 0.00083863,\n",
            "       0.00035268, 0.00088369, 0.00123575, 0.00064789, 0.00051374,\n",
            "       0.00071205, 0.00080934, 0.00030284, 0.00055159, 0.0007108 ,\n",
            "       0.00064741, 0.00071011, 0.00105085, 0.00076013, 0.00076835,\n",
            "       0.00078629, 0.00090564, 0.00068323, 0.00074723, 0.00062487,\n",
            "       0.00042732, 0.0010379 , 0.00050033], dtype=float32), array([[ 0.00133541,  0.00155332,  0.00235015, ...,  0.00209571,\n",
            "        -0.00118117,  0.00458672],\n",
            "       [ 0.00156614, -0.00014828, -0.00040686, ..., -0.00030968,\n",
            "         0.00674753,  0.00179263],\n",
            "       [ 0.00089219, -0.00653883, -0.00264224, ..., -0.00287261,\n",
            "         0.00330222,  0.00282629],\n",
            "       ...,\n",
            "       [ 0.0002059 , -0.00409473,  0.00538211, ...,  0.00235167,\n",
            "         0.00434698,  0.00096298],\n",
            "       [ 0.00111219,  0.00513684,  0.0016405 , ..., -0.00595506,\n",
            "         0.00478581,  0.00227128],\n",
            "       [ 0.00513416, -0.00349168,  0.00128213, ...,  0.00185065,\n",
            "         0.00458532,  0.00222081]], dtype=float32), array([-0.00512596,  0.01937739, -0.00129271,  0.00044622, -0.00184819,\n",
            "       -0.00913059, -0.00217322,  0.0053466 , -0.00442169, -0.00117784],\n",
            "      dtype=float32)]\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "aggregate_weights() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(global_model.get_weights())\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m global_model.set_weights(\u001b[43maggregate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_devices\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(global_model.get_weights())\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: aggregate_weights() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(global_model, selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZtp6yLRHqJx"
      },
      "outputs": [],
      "source": [
        "# Step 3: Run Optimization\n",
        "problem.initial_global_weights = global_model.get_weights()\n",
        "res = minimize(\n",
        "    problem=problem,\n",
        "    algorithm=algorithm,\n",
        "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
        "    # seed=42,\n",
        "    verbose=True\n",
        ")########################################################\n",
        "# Update device participation based on the bitstring\n",
        "selected_devices = [device for device in devices if bitstring[int(device.id)] == '1']\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "# aggregate_weights(global_model, devices)\n",
        "\n",
        "# new\n",
        "global_model.set_weights(aggregate_weights(global_model, devices))\n",
        "\n",
        "# Distribute the updated global model back to all devices\n",
        "for device in devices:\n",
        "    device.model.set_weights(global_model.get_weights())\n",
        "\n",
        "current_learning_iteration += 1\n",
        "# Train local models for selected devices\n",
        "for device in selected_devices:\n",
        "    device.number_of_times_fitted += 1\n",
        "    device.last_round_participated = current_learning_iteration\n",
        "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
        "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
        "    print(f\"Device {device.id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
        "\n",
        "# Aggregate weights to update the global model\n",
        "w1 = global_model.get_weights()\n",
        "\n",
        "# aggregate_weights(global_model, selected_devices)\n",
        "\n",
        "# new\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "global_model.set_weights(aggregate_weights(selected_devices))\n",
        "print(global_model.get_weights())\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "w2 = global_model.get_weights()\n",
        "print(\"global model stayed the same?\")\n",
        "print(np.array_equal(w1,w2))\n",
        "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
        "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAz6GN5kHqJx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnvznFa7HqJy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1GCriJHHqJy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
