{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "current_learning_iteration = 0\n",
    "\n",
    "# Device Class\n",
    "class Device:\n",
    "    def __init__(self, device_id, ram, storage, cpu, bandwidth, battery, charging):\n",
    "        self.device_id = device_id\n",
    "        self.ram = ram\n",
    "        self.storage = storage\n",
    "        self.cpu = cpu\n",
    "        self.bandwidth = bandwidth\n",
    "        self.battery = battery\n",
    "        self.charging = charging\n",
    "        self.energy_consumption = ram + storage + cpu + bandwidth\n",
    "        self.model = self.create_model()\n",
    "        self.last_round_participated = 0\n",
    "        self.data = None  # Placeholder for dataset partition\n",
    "        \n",
    "        self.test_data = None\n",
    "        # TODO: add test data for each device\n",
    "        # TODO: use the test data retrieved from load dataset from mnist and distribute it between devices and global model\n",
    "        \n",
    "\n",
    "        self.number_of_times_fitted = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Global Model\n",
    "# Define the global model with the same architecture\n",
    "def create_global_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "global_model = create_global_model()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_weights(global_model, devices):\n",
    "    \"\"\"Computes the weighted average of model weights from all devices and updates the global model.\"\"\"\n",
    "\n",
    "    num_devices = len(devices)\n",
    "    if num_devices == 0:\n",
    "        print(\"No devices available for aggregation.\")\n",
    "        return\n",
    "\n",
    "    # Get device weights and participation ratios\n",
    "    device_weights = [device.model.get_weights() for device in devices]\n",
    "    device_participation_ratio = np.array(\n",
    "        [device.last_round_participated / current_learning_iteration for device in devices]\n",
    "    )\n",
    "\n",
    "    # Total weight for normalization\n",
    "    total_weight = np.sum(device_participation_ratio)\n",
    "    if total_weight == 0:\n",
    "        print(\"Total weight is zero, cannot perform aggregation.\")\n",
    "        return\n",
    "\n",
    "    for item in device_participation_ratio:\n",
    "        print(item)\n",
    "    \n",
    "    \n",
    "    len_total_devices_data = 0\n",
    "    for device in devices:\n",
    "        len_total_devices_data += len(device.data[0])\n",
    "\n",
    "    # Compute weighted sum of weights\n",
    "    weighted_sums = [\n",
    "        np.sum(np.stack([device_weights[i][layer] * device_participation_ratio[i] * len(devices[i].data[0])/float(len_total_devices_data) for i in range(num_devices)]), axis=0)\n",
    "        for layer in range(len(device_weights[0]))\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Compute weighted average\n",
    "    weighted_avg_weights = [layer_sum / total_weight for layer_sum in weighted_sums]\n",
    "\n",
    "    # Set the global model's weights to the weighted averaged weights\n",
    "    print(\"old global model weights:\", global_model.get_weights())\n",
    "    global_model.set_weights(weighted_avg_weights)\n",
    "    print(\"new global model weights:\", global_model.get_weights())\n",
    "    print(np.array_equal(global_model.get_weights(), weighted_avg_weights))\n",
    "    # print(global_model.get_weights())\n",
    "    # for i in range(100):\n",
    "    #     print()\n",
    "    # print(weighted_avg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1126 - loss: 2.3011  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2560 - loss: 2.2228 \n",
      "1\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1510 - loss: 2.2923  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1869 - loss: 2.2345 \n",
      "2\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0741 - loss: 2.3036      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3135 - loss: 2.2351 \n",
      "3\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1233 - loss: 2.3006  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2343 - loss: 2.2517 \n",
      "4\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1362 - loss: 2.2844  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3204 - loss: 2.2028 \n",
      "5\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2107 - loss: 2.2899  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3756 - loss: 2.2306 \n",
      "6\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0763 - loss: 2.3065  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3982 - loss: 2.2221 \n",
      "7\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1216 - loss: 2.3008  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3156 - loss: 2.2449 \n",
      "8\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1664 - loss: 2.2862  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2916 - loss: 2.2271 \n",
      "9\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1200 - loss: 2.3010  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2945 - loss: 2.2444 \n",
      "10\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0707 - loss: 2.2869  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3350 - loss: 2.2335 \n",
      "11\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1006 - loss: 2.2842  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3193 - loss: 2.2060 \n",
      "12\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1272 - loss: 2.2906  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3227 - loss: 2.2131 \n",
      "13\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1450 - loss: 2.2852  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3377 - loss: 2.1964 \n",
      "14\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1061 - loss: 2.2973  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2197 - loss: 2.2332 \n",
      "15\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0693 - loss: 2.2877  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2246 - loss: 2.2231 \n",
      "16\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1114 - loss: 2.3047      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2573 - loss: 2.2323 \n",
      "17\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1971 - loss: 2.2868  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4122 - loss: 2.2353 \n",
      "18\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1509 - loss: 2.2855  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3421 - loss: 2.1992 \n",
      "19\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1343 - loss: 2.2896  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3306 - loss: 2.2454 \n",
      "20\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0919 - loss: 2.3005  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2946 - loss: 2.2392 \n",
      "21\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1221 - loss: 2.2961  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3697 - loss: 2.2197 \n",
      "22\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1296 - loss: 2.2827  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4223 - loss: 2.1660 \n",
      "23\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0742 - loss: 2.3067\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2491 - loss: 2.2412 \n",
      "24\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1980 - loss: 2.2490  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3447 - loss: 2.1691 \n",
      "25\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1165 - loss: 2.3106  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1587 - loss: 2.2436 \n",
      "26\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0776 - loss: 2.3011  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1755 - loss: 2.2544 \n",
      "27\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0858 - loss: 2.3112  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2749 - loss: 2.2352 \n",
      "28\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1517 - loss: 2.2921  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2482 - loss: 2.2487 \n",
      "29\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1072 - loss: 2.2959  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2843 - loss: 2.2107 \n",
      "30\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1328 - loss: 2.2850  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4518 - loss: 2.1855 \n",
      "31\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2197 - loss: 2.2764  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2472 - loss: 2.2035 \n",
      "32\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1073 - loss: 2.2940  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2093 - loss: 2.2214 \n",
      "33\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1132 - loss: 2.3105  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3448 - loss: 2.1923 \n",
      "34\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1680 - loss: 2.2908  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2883 - loss: 2.2085 \n",
      "35\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0872 - loss: 2.2963  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2720 - loss: 2.2331 \n",
      "36\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1025 - loss: 2.2948      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3573 - loss: 2.2134 \n",
      "37\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1357 - loss: 2.2700  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2371 - loss: 2.1737 \n",
      "38\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1743 - loss: 2.2676  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3486 - loss: 2.1580 \n",
      "39\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1711 - loss: 2.2869  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3778 - loss: 2.2106 \n",
      "40\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0553 - loss: 2.3186      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3044 - loss: 2.2514 \n",
      "41\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0887 - loss: 2.3099  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2479 - loss: 2.2448 \n",
      "42\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0887 - loss: 2.2908  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3310 - loss: 2.2135 \n",
      "43\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0867 - loss: 2.3163      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2276 - loss: 2.2478 \n",
      "44\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1657 - loss: 2.2899  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1943 - loss: 2.2500 \n",
      "45\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1406 - loss: 2.2841  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3510 - loss: 2.1928 \n",
      "46\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1246 - loss: 2.3081  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3464 - loss: 2.1959 \n",
      "47\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1293 - loss: 2.2916  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3404 - loss: 2.2146 \n",
      "48\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0968 - loss: 2.3015  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2232 - loss: 2.2301 \n",
      "49\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1212 - loss: 2.2847  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3507 - loss: 2.1866 \n",
      "50\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1613 - loss: 2.2864\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4042 - loss: 2.1800\n",
      "51\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1229 - loss: 2.2868\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2709 - loss: 2.2369\n",
      "52\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1403 - loss: 2.2809\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2774 - loss: 2.2288\n",
      "53\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1275 - loss: 2.2924\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3779 - loss: 2.2219\n",
      "54\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0907 - loss: 2.3023\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1431 - loss: 2.2592\n",
      "55\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2003 - loss: 2.2871\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3111 - loss: 2.2313\n",
      "56\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1057 - loss: 2.3018\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2576 - loss: 2.2298\n",
      "57\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0988 - loss: 2.3182\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1402 - loss: 2.2549\n",
      "58\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1659 - loss: 2.2942\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2975 - loss: 2.2555\n",
      "59\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1369 - loss: 2.2700\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2994 - loss: 2.1719\n",
      "60\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1817 - loss: 2.2757\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3567 - loss: 2.2090\n",
      "61\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1341 - loss: 2.2942\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2896 - loss: 2.2391\n",
      "62\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1457 - loss: 2.2942\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3765 - loss: 2.1963\n",
      "63\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1077 - loss: 2.2998\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2837 - loss: 2.2373\n",
      "64\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1174 - loss: 2.2857\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3724 - loss: 2.1951\n",
      "65\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1438 - loss: 2.2969\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3659 - loss: 2.2303\n",
      "66\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1505 - loss: 2.2868\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3513 - loss: 2.2065\n",
      "67\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0608 - loss: 2.3127\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2562 - loss: 2.2387\n",
      "68\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1115 - loss: 2.3078\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2718 - loss: 2.2505\n",
      "69\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1118 - loss: 2.2987\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2750 - loss: 2.2220\n",
      "70\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1221 - loss: 2.3003\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2899 - loss: 2.2264\n",
      "71\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1234 - loss: 2.2871\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2472 - loss: 2.2499\n",
      "72\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1257 - loss: 2.2891\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4211 - loss: 2.2220\n",
      "73\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1360 - loss: 2.2642\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2052 - loss: 2.1657\n",
      "74\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.0908 - loss: 2.3068    \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1765 - loss: 2.2219\n",
      "75\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0950 - loss: 2.2981\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2178 - loss: 2.2572\n",
      "76\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0819 - loss: 2.2855\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3142 - loss: 2.2369\n",
      "77\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0939 - loss: 2.3003\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2044 - loss: 2.2441\n",
      "78\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1471 - loss: 2.2678\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3817 - loss: 2.1773\n",
      "79\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1770 - loss: 2.2854\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3213 - loss: 2.2016\n",
      "80\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1001 - loss: 2.3138\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2446 - loss: 2.2639\n",
      "81\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1697 - loss: 2.2856\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3839 - loss: 2.1909\n",
      "82\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1174 - loss: 2.2836\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3828 - loss: 2.1966\n",
      "83\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1052 - loss: 2.2830\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3464 - loss: 2.2008\n",
      "84\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1579 - loss: 2.2929\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3382 - loss: 2.2371\n",
      "85\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1373 - loss: 2.2852\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2869 - loss: 2.2301\n",
      "86\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0622 - loss: 2.3150\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2447 - loss: 2.2538\n",
      "87\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1593 - loss: 2.2761\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1377 - loss: 2.1887\n",
      "88\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1417 - loss: 2.2892\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3603 - loss: 2.2056\n",
      "89\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1113 - loss: 2.3051\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2085 - loss: 2.2473\n",
      "90\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1000 - loss: 2.3057\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1964 - loss: 2.2475\n",
      "91\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1335 - loss: 2.2814\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3643 - loss: 2.1718\n",
      "92\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1030 - loss: 2.2896\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3540 - loss: 2.2006\n",
      "93\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1205 - loss: 2.2740\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2545 - loss: 2.2121\n",
      "94\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2135 - loss: 2.2921\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2453 - loss: 2.2326\n",
      "95\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1125 - loss: 2.3066\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2719 - loss: 2.2476\n",
      "96\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1223 - loss: 2.2984\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3320 - loss: 2.2107\n",
      "97\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1058 - loss: 2.3017\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2444 - loss: 2.2552\n",
      "98\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1749 - loss: 2.2872\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2333 - loss: 2.2517\n",
      "99\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1382 - loss: 2.3098\n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3238 - loss: 2.2411\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "old global model weights: [array([[[[-0.10570784,  0.05954653,  0.02842663,  0.05944102,\n",
      "           0.01880273,  0.00750096, -0.03998198,  0.08263038,\n",
      "          -0.02258707,  0.08505259,  0.04165046, -0.02771795,\n",
      "           0.11931141,  0.08448464,  0.05171423,  0.02894633,\n",
      "          -0.13385656,  0.10137768,  0.06803179,  0.13670073,\n",
      "          -0.11960216,  0.01581007,  0.1046064 , -0.03959689,\n",
      "          -0.08452889, -0.00620575, -0.1261351 ,  0.01606555,\n",
      "          -0.09185588, -0.04821043,  0.05277303,  0.10708661]],\n",
      "\n",
      "        [[ 0.09093018,  0.03671452, -0.1341505 , -0.07813408,\n",
      "          -0.01359466,  0.08012405, -0.09581266, -0.0141097 ,\n",
      "           0.11657275, -0.08306974,  0.05991645, -0.00828186,\n",
      "           0.13982053,  0.11163877,  0.04169862, -0.06546628,\n",
      "           0.11289744, -0.13081853, -0.05541187,  0.09293434,\n",
      "          -0.05596122,  0.03411517,  0.02557528,  0.06476983,\n",
      "          -0.12644906,  0.05729513, -0.04303773,  0.12642808,\n",
      "           0.11297451,  0.07179129, -0.10106419,  0.03928305]],\n",
      "\n",
      "        [[-0.1198595 , -0.08590624, -0.04244962, -0.00815387,\n",
      "          -0.01318146,  0.10990746, -0.10533491,  0.00945216,\n",
      "           0.00350703,  0.09282418,  0.03211601, -0.07871138,\n",
      "          -0.13059954,  0.12265216,  0.1114931 , -0.04429472,\n",
      "          -0.12964085, -0.11864589,  0.03142793,  0.06643596,\n",
      "           0.13517372,  0.02605292,  0.04072975,  0.08099258,\n",
      "           0.07795055,  0.11433105,  0.09037012,  0.08362901,\n",
      "          -0.10654783,  0.05153024, -0.11988803,  0.01944341]]],\n",
      "\n",
      "\n",
      "       [[[-0.07091957,  0.05484843,  0.04373856, -0.11148197,\n",
      "           0.01763649, -0.01924293,  0.0364662 ,  0.02781154,\n",
      "          -0.06880825,  0.02777898, -0.04560467, -0.11301775,\n",
      "           0.04697616, -0.09652959, -0.14131984,  0.00761282,\n",
      "          -0.01729614, -0.01891266, -0.01062208,  0.05696069,\n",
      "          -0.02289856, -0.01077017, -0.0830287 , -0.14181599,\n",
      "          -0.09005785,  0.04358935, -0.08550274, -0.00667134,\n",
      "           0.11712219, -0.11870044,  0.09377444,  0.09945163]],\n",
      "\n",
      "        [[-0.09300449, -0.01920067,  0.12703024, -0.03207115,\n",
      "           0.01234217,  0.0656653 , -0.064827  , -0.09062292,\n",
      "           0.065588  ,  0.06093179, -0.08842909, -0.02736732,\n",
      "          -0.14192985, -0.00062624, -0.0143338 ,  0.07445911,\n",
      "          -0.05523047, -0.02690059,  0.10800858, -0.08480874,\n",
      "           0.10142495,  0.00415581, -0.07920682,  0.06707485,\n",
      "          -0.05887906,  0.00928466,  0.03292568, -0.12766436,\n",
      "           0.00888479,  0.13570462, -0.09798145,  0.0419631 ]],\n",
      "\n",
      "        [[ 0.0363251 ,  0.12132533,  0.01603915,  0.08340576,\n",
      "           0.04187709, -0.08887698, -0.06156646,  0.00640099,\n",
      "          -0.0776213 ,  0.01331088, -0.07080398, -0.02414965,\n",
      "           0.07827735,  0.12890129,  0.02975376,  0.0140519 ,\n",
      "           0.08770004, -0.09241079, -0.05425143,  0.04681957,\n",
      "          -0.02739351,  0.0234492 , -0.13845263,  0.03506377,\n",
      "          -0.10543671, -0.02709933,  0.00792184, -0.1072081 ,\n",
      "          -0.04013244, -0.05561177, -0.1073672 , -0.11544584]]],\n",
      "\n",
      "\n",
      "       [[[-0.02112164, -0.13776119, -0.03400676, -0.0989258 ,\n",
      "           0.01010036, -0.08317882, -0.10042833,  0.01404826,\n",
      "          -0.11718406, -0.09134001, -0.13075319, -0.10163888,\n",
      "           0.02304173,  0.03968872, -0.00128263, -0.03650836,\n",
      "           0.13315566,  0.01044971,  0.09023024,  0.1389729 ,\n",
      "          -0.12481932,  0.10478301, -0.13770209,  0.07075274,\n",
      "          -0.05323942, -0.04538034, -0.13909978,  0.01515584,\n",
      "           0.12898256,  0.01400578, -0.08155802, -0.0264831 ]],\n",
      "\n",
      "        [[ 0.04142873,  0.00598088, -0.06548356, -0.07524403,\n",
      "          -0.09900469, -0.12859298,  0.05200349,  0.02240437,\n",
      "           0.11045305,  0.1284758 , -0.02516637, -0.13065428,\n",
      "          -0.12995735, -0.10073049, -0.02874769, -0.10904635,\n",
      "          -0.1160938 ,  0.11897676,  0.01237091,  0.13060068,\n",
      "          -0.01203363,  0.0566041 , -0.07025582,  0.04349644,\n",
      "           0.03348635,  0.02213468, -0.01051657, -0.11582114,\n",
      "           0.06282853, -0.1231529 ,  0.10616952, -0.1196801 ]],\n",
      "\n",
      "        [[ 0.11176692,  0.01333481,  0.09205656, -0.07468313,\n",
      "           0.04138888, -0.06747146, -0.12179667, -0.0717354 ,\n",
      "           0.00362869,  0.03869928, -0.03200741, -0.00500424,\n",
      "          -0.02902789, -0.12746446, -0.09586116,  0.07309754,\n",
      "           0.03010549,  0.04541242,  0.04708154, -0.12288919,\n",
      "          -0.07690112,  0.06462683, -0.01088408, -0.07703118,\n",
      "           0.13056658,  0.07958436, -0.06961785, -0.05245595,\n",
      "           0.04898131,  0.03383777, -0.01139462, -0.10934636]]]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([[ 0.00631271, -0.02985715,  0.01670547, ...,  0.02333573,\n",
      "         0.01413368, -0.016024  ],\n",
      "       [ 0.02767067, -0.02091785,  0.02921276, ...,  0.02276747,\n",
      "         0.02823369, -0.01354849],\n",
      "       [-0.03255951, -0.02029519,  0.0138632 , ..., -0.01475096,\n",
      "        -0.00587439,  0.02705471],\n",
      "       ...,\n",
      "       [ 0.02301721, -0.01345547,  0.02555246, ..., -0.02101386,\n",
      "        -0.01379801,  0.01358794],\n",
      "       [-0.02597461, -0.02095904, -0.01640528, ...,  0.00957208,\n",
      "         0.03291158,  0.0019699 ],\n",
      "       [ 0.00929739,  0.03031508,  0.02504452, ..., -0.02306402,\n",
      "         0.02776489, -0.02986318]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.14318244, -0.04798129, -0.15717898, ..., -0.09953883,\n",
      "        -0.09789757,  0.0419706 ],\n",
      "       [-0.08721042, -0.12818973,  0.03453019, ..., -0.12960872,\n",
      "         0.08348225, -0.06833999],\n",
      "       [-0.03543772, -0.06239121, -0.13167253, ..., -0.18312034,\n",
      "        -0.04866381, -0.14889719],\n",
      "       ...,\n",
      "       [ 0.07192113, -0.2039765 , -0.08836354, ...,  0.14870565,\n",
      "        -0.12142265, -0.13652593],\n",
      "       [-0.09785298, -0.1165001 , -0.16607939, ...,  0.10771833,\n",
      "         0.13084204, -0.1736727 ],\n",
      "       [ 0.05276902,  0.1039619 , -0.11444354, ..., -0.00721058,\n",
      "        -0.14994952,  0.17397963]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "new global model weights: [array([[[[ 9.05855177e-06, -4.87280886e-05,  1.87048819e-04,\n",
      "           1.21496902e-04, -8.43499147e-05,  8.51020668e-05,\n",
      "           1.24108628e-04,  1.12743983e-05,  4.30486944e-05,\n",
      "           5.16628133e-05, -3.66222775e-05, -3.76818462e-05,\n",
      "           3.71098904e-05, -5.55631486e-05,  5.69782651e-06,\n",
      "           1.63695004e-04,  2.32627499e-05, -1.49427797e-04,\n",
      "          -3.39728504e-05, -1.90248065e-05,  3.05564972e-05,\n",
      "          -7.73672800e-05,  2.90826192e-05, -4.34065623e-05,\n",
      "          -7.54869397e-05,  5.10538557e-05, -9.35373100e-06,\n",
      "           5.43524766e-05, -1.08619179e-05, -9.27041674e-06,\n",
      "           2.88597839e-05, -1.16917699e-05]],\n",
      "\n",
      "        [[-1.21340658e-04,  5.72720492e-05,  5.50907826e-05,\n",
      "          -4.00955942e-05,  1.45242637e-04, -2.78364769e-05,\n",
      "          -2.97638799e-05,  1.82366668e-04, -8.92678727e-05,\n",
      "          -1.75534122e-04, -5.37910091e-05, -1.30694694e-04,\n",
      "           9.75571602e-05, -4.40488693e-05, -2.85401493e-05,\n",
      "           8.36962427e-05,  4.30085056e-05, -6.59920188e-05,\n",
      "           1.05498373e-04,  9.13020849e-05,  1.46404593e-04,\n",
      "          -1.28554995e-04, -2.03564218e-06, -1.41095661e-04,\n",
      "           7.31980035e-05,  6.59452053e-05,  1.09958724e-04,\n",
      "          -5.14117637e-05, -4.72926222e-05, -1.27273277e-04,\n",
      "           6.57398050e-06,  1.10362547e-04]],\n",
      "\n",
      "        [[ 5.66319832e-05,  7.06961437e-05, -5.18728812e-05,\n",
      "          -1.62854514e-04, -2.61950117e-05, -9.84888829e-05,\n",
      "           9.22413310e-05,  1.11528330e-04, -4.73962200e-06,\n",
      "          -1.03011014e-04,  1.24602186e-04,  4.17385490e-05,\n",
      "           7.19577947e-05, -1.96046691e-04, -1.33519468e-04,\n",
      "           8.34895764e-05,  1.48178719e-04,  1.27515552e-04,\n",
      "           6.38454439e-05,  3.12914781e-05, -2.95797963e-05,\n",
      "           3.04505684e-05, -6.56257980e-05,  8.70697841e-05,\n",
      "           1.96393430e-05,  3.12143129e-05,  3.53231080e-05,\n",
      "           2.02875031e-04, -1.16662814e-05,  4.20800243e-05,\n",
      "           1.21307865e-04,  9.52703704e-05]]],\n",
      "\n",
      "\n",
      "       [[[-3.02719764e-05, -8.23075097e-05,  2.74763897e-05,\n",
      "          -1.56121132e-05, -7.37814626e-05,  1.96166351e-04,\n",
      "          -1.47000406e-04, -3.40479855e-05, -8.14862506e-05,\n",
      "           1.74191824e-04,  1.02038985e-05,  1.32699679e-05,\n",
      "           1.33043213e-04,  9.82845377e-05,  4.47008133e-05,\n",
      "           1.45904793e-04, -6.14325427e-06, -6.08452137e-06,\n",
      "           8.16900574e-05, -1.19831930e-06, -9.89225555e-06,\n",
      "          -3.87122163e-05,  6.04634697e-05, -1.03389502e-04,\n",
      "           7.19326954e-06, -3.09858962e-07, -3.92846778e-05,\n",
      "          -1.25758406e-05, -9.57185548e-05,  1.38429823e-04,\n",
      "           6.50306392e-05,  2.78403459e-05]],\n",
      "\n",
      "        [[-8.26262258e-05, -2.22927865e-05, -6.31468283e-05,\n",
      "          -1.00612255e-04,  5.12851402e-05,  1.29674245e-05,\n",
      "           6.79998811e-06,  1.98567814e-05, -2.62458525e-05,\n",
      "           5.76297025e-05, -1.41897413e-04,  1.27454594e-04,\n",
      "           2.64933624e-05, -2.59808112e-05,  2.52984933e-06,\n",
      "          -7.51211992e-05,  7.02087573e-06, -4.97936435e-06,\n",
      "           5.68348405e-05,  2.06367898e-04, -1.51313498e-05,\n",
      "          -3.03177076e-05, -1.74802830e-04,  9.71530335e-06,\n",
      "          -8.03217845e-05, -7.35470758e-06,  6.54141841e-05,\n",
      "           1.25240058e-05, -1.00658535e-05,  3.64590414e-05,\n",
      "           2.42198395e-04,  9.43653358e-05]],\n",
      "\n",
      "        [[-1.68280858e-05, -1.04367036e-05, -5.29789540e-05,\n",
      "          -1.93717769e-05,  9.16236531e-05, -1.78282673e-04,\n",
      "           9.67951273e-05,  8.15518652e-05, -4.78654802e-05,\n",
      "           1.54332811e-04,  4.78891307e-05,  5.73144916e-07,\n",
      "           3.54421973e-05, -2.01291587e-05, -9.27801811e-05,\n",
      "          -3.37388337e-06, -1.20068173e-04,  1.00351435e-05,\n",
      "           1.18555081e-05, -3.24542161e-05,  1.71951164e-04,\n",
      "          -1.73768494e-05,  1.01992067e-04,  1.81101452e-04,\n",
      "           4.90977209e-06, -4.18559903e-05,  7.99503032e-05,\n",
      "          -1.70363273e-06, -7.79254769e-05, -8.28777866e-06,\n",
      "          -3.99334676e-05,  1.03270671e-04]]],\n",
      "\n",
      "\n",
      "       [[[-5.25631731e-05, -4.78662878e-05, -6.81880410e-05,\n",
      "          -9.04144690e-05,  7.82542847e-05,  7.04365812e-05,\n",
      "           1.06970641e-04,  2.78436986e-04, -2.31962666e-04,\n",
      "           6.32898373e-05, -1.11164358e-04, -6.45121036e-05,\n",
      "          -1.12118163e-04,  2.03607178e-05,  1.11208697e-04,\n",
      "           7.85049997e-05,  3.08351955e-05,  1.31209657e-04,\n",
      "          -1.13168553e-05,  1.33865115e-05, -1.02542086e-04,\n",
      "           5.37693959e-05,  1.68834085e-04,  2.31850991e-05,\n",
      "          -1.01986843e-05, -1.28990678e-05,  1.29781780e-04,\n",
      "           7.94364605e-05,  3.47204696e-05, -5.42972612e-06,\n",
      "          -7.61056572e-06, -1.38194111e-04]],\n",
      "\n",
      "        [[ 4.28271342e-05,  1.24701255e-04, -1.97405418e-04,\n",
      "           6.77573189e-05, -7.43287819e-06,  6.62848397e-05,\n",
      "          -2.77410163e-05, -2.53740436e-05,  3.76240569e-05,\n",
      "          -5.77622814e-05,  1.75274067e-06, -5.19834066e-05,\n",
      "          -7.82733878e-07,  5.68175092e-05, -1.54924142e-04,\n",
      "          -3.48463582e-05,  1.00955433e-04,  1.32325804e-04,\n",
      "           2.56761973e-06,  4.66649944e-05,  5.41573136e-05,\n",
      "          -1.62920842e-05, -1.10607631e-04,  8.39570857e-05,\n",
      "          -2.83348913e-06,  1.42786384e-05, -6.00963494e-06,\n",
      "           3.20639192e-05, -8.77380444e-05, -1.00629222e-04,\n",
      "           1.59854928e-04, -7.62807622e-06]],\n",
      "\n",
      "        [[ 8.91339878e-05,  1.29896769e-04,  1.01978394e-05,\n",
      "           4.49106255e-06, -1.14944478e-05, -1.00449208e-04,\n",
      "           6.32198207e-05, -9.55623691e-05, -1.95780827e-04,\n",
      "          -9.45921929e-05, -9.21643586e-05, -4.87857287e-05,\n",
      "           7.62070267e-05,  1.26305895e-06, -5.73327998e-05,\n",
      "           1.06762964e-04,  8.69398209e-05, -7.95763481e-05,\n",
      "           7.60488683e-06,  8.11631544e-05,  1.62168035e-05,\n",
      "           1.23921694e-04,  5.69902077e-05,  8.72560631e-05,\n",
      "          -1.07193417e-04,  5.29443387e-05, -9.31082104e-05,\n",
      "           6.19423690e-06,  4.22906342e-05,  9.36234755e-06,\n",
      "           1.20875506e-04,  9.71138797e-05]]]], dtype=float32), array([5.99780878e-06, 1.49338666e-05, 1.43860443e-05, 1.25744045e-05,\n",
      "       1.65695437e-05, 1.11501304e-05, 1.70791682e-05, 1.14542336e-05,\n",
      "       1.75610803e-05, 4.09963195e-06, 1.13010865e-05, 1.57797313e-05,\n",
      "       1.67773815e-05, 1.52069324e-05, 1.33328640e-05, 1.48779882e-05,\n",
      "       1.23013588e-05, 1.29642176e-05, 1.45212534e-05, 1.48967674e-05,\n",
      "       8.45271279e-06, 3.25083852e-06, 4.38420284e-06, 8.60946238e-06,\n",
      "       8.64299909e-06, 1.76390749e-05, 2.01590083e-05, 1.79914186e-05,\n",
      "       9.24226242e-06, 1.20556979e-05, 1.79736180e-05, 1.08625463e-05],\n",
      "      dtype=float32), array([[-8.56050247e-06, -6.76393120e-06,  1.30386234e-05, ...,\n",
      "         1.35314540e-05, -3.84625237e-05, -1.57539398e-05],\n",
      "       [ 2.14858774e-05, -1.78129449e-05,  4.81675124e-06, ...,\n",
      "        -2.33926330e-05,  1.29305827e-05, -3.15776088e-05],\n",
      "       [ 5.07693221e-06,  2.12818322e-05, -5.03466026e-06, ...,\n",
      "         2.49790610e-05,  1.08864688e-05, -2.77879899e-05],\n",
      "       ...,\n",
      "       [ 2.85806509e-05, -1.80227435e-05,  2.05780862e-06, ...,\n",
      "        -1.70609783e-05, -1.61372882e-05, -1.13603346e-05],\n",
      "       [ 3.83322476e-05,  1.51282620e-05, -3.46505512e-05, ...,\n",
      "         1.11898692e-06, -1.42572771e-05,  3.82442486e-05],\n",
      "       [ 3.25097280e-05,  5.51425683e-06, -1.80986558e-06, ...,\n",
      "        -2.63600600e-06,  1.50959295e-05,  2.05246579e-05]], dtype=float32), array([ 2.8006521e-06,  4.0817704e-06, -1.1455340e-06,  1.8403842e-06,\n",
      "        2.8641666e-06,  3.1059908e-06,  5.9693311e-06, -7.5700331e-07,\n",
      "        3.2336370e-06,  3.4493469e-06,  3.1477086e-06,  3.0017816e-06,\n",
      "        6.4460191e-06,  2.7606768e-06,  3.9355054e-06,  9.3291931e-07,\n",
      "        7.8808728e-09,  4.0281752e-06,  3.1127445e-06,  2.7123854e-06,\n",
      "        3.4732857e-06,  5.7435741e-06,  4.5337533e-06,  2.3064410e-06,\n",
      "        3.4958575e-06,  6.0990765e-06,  7.8337899e-07,  3.7424879e-06,\n",
      "        1.1535757e-06,  2.7241156e-06,  1.6565078e-06,  2.4903266e-06,\n",
      "        7.3566736e-07,  1.8853891e-06,  2.4665696e-06,  3.0934730e-06,\n",
      "        1.8029541e-06,  4.5786196e-06,  1.7129485e-06,  3.7478558e-06,\n",
      "        2.3756797e-06,  2.4243589e-06,  2.0708153e-06,  4.0024233e-06,\n",
      "        1.4007306e-06,  1.3356872e-06,  1.7473423e-06, -1.3439457e-06,\n",
      "        2.8880768e-06,  5.9404636e-07,  2.1270916e-06,  3.9742872e-06,\n",
      "        5.0480303e-06,  1.6016752e-06,  3.5679277e-06,  3.2572243e-06,\n",
      "        1.9012525e-06,  2.7293033e-06,  8.8407074e-07,  3.1439201e-06,\n",
      "        3.4558204e-06,  1.0505198e-06,  2.6667060e-06,  3.0713331e-06,\n",
      "       -9.1496224e-07,  2.7332396e-06,  3.1778500e-06,  1.7329963e-06,\n",
      "        2.6638579e-06,  3.1784418e-06,  4.8356251e-06,  2.7305064e-06,\n",
      "        3.8054504e-06,  3.6825386e-06,  3.6535591e-06,  2.7141296e-06,\n",
      "        4.4606163e-06,  2.4116703e-06,  2.6567011e-06,  3.8284142e-07,\n",
      "        3.9871588e-06,  4.7894227e-06,  2.1876883e-06,  9.5773551e-08,\n",
      "        2.3324449e-06,  4.6468299e-06,  2.0833090e-06,  1.2180926e-06,\n",
      "        3.7402472e-06,  1.7772287e-06,  2.0855052e-06,  2.1340942e-07,\n",
      "       -1.7108640e-06,  1.9900735e-06,  4.1176850e-06,  4.2236948e-06,\n",
      "       -4.6890619e-07,  2.0395098e-06,  4.6100272e-06,  6.5563549e-06,\n",
      "        6.8091089e-07,  3.1394109e-06,  4.3522541e-06,  3.5384699e-06,\n",
      "        1.5908471e-06,  2.6343207e-06,  4.3878722e-06,  5.1855527e-06,\n",
      "        2.0625132e-06,  5.1434026e-06,  6.1094156e-06,  2.4532878e-06,\n",
      "        1.7485346e-06,  3.7861378e-06,  3.6135391e-06,  1.0862409e-06,\n",
      "        2.7421759e-06,  2.2404793e-06,  4.1977032e-06,  4.8367419e-06,\n",
      "        2.5447398e-06,  2.6203377e-06,  2.7117478e-06,  4.9178602e-06,\n",
      "        7.4147765e-06,  4.0637419e-06,  5.6263557e-06,  2.9871655e-06],\n",
      "      dtype=float32), array([[ 4.11536821e-05,  1.65298610e-04,  6.26431938e-05, ...,\n",
      "        -1.34116170e-04,  4.25128928e-05, -2.84737005e-04],\n",
      "       [ 1.61149055e-05, -4.10081957e-05,  8.23767623e-05, ...,\n",
      "         3.00890708e-04,  8.80938387e-05,  7.61724950e-05],\n",
      "       [-2.76544943e-05, -5.18541856e-05,  1.76490154e-04, ...,\n",
      "         1.42190882e-04, -1.01900703e-04,  1.05527943e-04],\n",
      "       ...,\n",
      "       [ 6.24364693e-05, -1.78539685e-05, -1.02558715e-05, ...,\n",
      "        -5.98260704e-05, -1.00108096e-04,  1.21460340e-04],\n",
      "       [-2.08222482e-04,  2.63230122e-05,  3.28301248e-05, ...,\n",
      "         3.93601622e-05, -7.07845102e-05, -1.43269019e-04],\n",
      "       [-8.82482054e-05,  1.46736435e-04,  1.55886228e-04, ...,\n",
      "        -2.66273146e-05,  1.86042642e-04,  1.15800074e-04]], dtype=float32), array([-1.0414097e-05,  4.8230329e-05, -5.6157564e-06,  5.4086604e-06,\n",
      "       -3.5833480e-06, -2.6495742e-05, -3.3887022e-06,  1.7021899e-05,\n",
      "       -1.8251745e-05, -2.9114949e-06], dtype=float32)]\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1102 - loss: 2.3026\n",
      "Global Model Accuracy: 0.1123\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from CSV\n",
    "csv_file = 'devices.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "\n",
    "# Convert CSV rows into device objects\n",
    "devices = []\n",
    "for _, row in df.iterrows():\n",
    "    device = Device(\n",
    "        row['id'], row['ram'], row['storage'], row['cpu'], row['bandwidth'], row['battery'],\n",
    "        row.get('charging', 0)\n",
    "    )\n",
    "    devices.append(device)\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data and reshape for CNN\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)  # Add channel dimension\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "# Split into global test set (20%) and training set (80%)\n",
    "split_index = int(0.8 * len(x_train))\n",
    "x_train_devices, y_train_devices = x_train[:split_index], y_train[:split_index]\n",
    "x_test_global, y_test_global = x_train[split_index:], y_train[split_index:]\n",
    "\n",
    "\n",
    "# Split dataset among devices\n",
    "num_devices = len(devices)\n",
    "split_size = len(x_train_devices) // num_devices\n",
    "\n",
    "for i, device in enumerate(devices):\n",
    "    start = i * split_size\n",
    "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_train_devices)\n",
    "    device.data = (x_train_devices[start:end], y_train_devices[start:end])\n",
    "\n",
    "\n",
    "with open('bitstring.txt', 'r') as f:\n",
    "    bitstring = f.read()\n",
    "\n",
    "bitstring = [int(bit) for bit in bitstring.split(',')]\n",
    "\n",
    "current_learning_iteration += 1\n",
    "for device in devices:\n",
    "    print(int(device.device_id))\n",
    "    if bitstring[int(device.device_id)] == 1:\n",
    "        device.model.fit(device.data[0], device.data[1], epochs=2, verbose=1)\n",
    "        device.number_of_times_fitted += 1\n",
    "        device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "# Call this function after training the local models:\n",
    "aggregate_weights(global_model, devices)\n",
    "\n",
    "\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == 1]\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(self.global_model, selected_devices)\n",
    "\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            # Update device participation based on the bitstring\\n            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\\n\\n            # Train local models for selected devices\\n            for device in selected_devices:\\n                device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\\n\\n            # Aggregate weights to update the global model\\n            aggregate_weights(self.global_model, selected_devices)\\n\\n            # Distribute the updated global model back to all devices\\n            for device in self.devices:\\n                device.model.set_weights(self.global_model.get_weights())\\n            '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.operators.sampling.rnd import BinaryRandomSampling\n",
    "from pymoo.operators.crossover.pntx import TwoPointCrossover\n",
    "from pymoo.operators.mutation.bitflip import BitflipMutation\n",
    "from pymoo.operators.selection.tournament import TournamentSelection\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "\n",
    "# Parameters\n",
    "NUM_DEVICES = num_devices   # Number of devices (length of bitstring)\n",
    "POPULATION_SIZE = 100\n",
    "NUM_GENERATIONS = 10\n",
    "\n",
    "# Step 1: Define the Problem\n",
    "import numpy as np\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "class FederatedLearningProblem(Problem):\n",
    "    def __init__(self, num_devices, devices, global_model, x_test_global, y_test_global):\n",
    "        super().__init__(\n",
    "            n_var=num_devices,         # Number of variables (bitstring length)\n",
    "            n_obj=3,                   # Number of objectives\n",
    "            n_constr=0,                # No constraints\n",
    "            xl=np.zeros(num_devices),  # Lower bound (0)\n",
    "            xu=np.ones(num_devices),   # Upper bound (1)\n",
    "            type_var=np.bool_          # Binary variables (bitstrings)\n",
    "        )\n",
    "        self.devices = devices\n",
    "        self.global_model = global_model\n",
    "        self.x_test_global = x_test_global\n",
    "        self.y_test_global = y_test_global\n",
    "\n",
    "        # Save the initial global model weights\n",
    "        self.initial_global_weights = global_model.get_weights()\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"Evaluates objective values for each solution in the population.\"\"\"\n",
    "        num_solutions = len(X)\n",
    "        F = np.zeros((num_solutions, 3))  # Initialize objective matrix\n",
    "\n",
    "        for i, bitstring in enumerate(X):\n",
    "            # Reset the global model to its initial state\n",
    "            self.global_model.set_weights(self.initial_global_weights)\n",
    "            # Update device participation based on the bitstring\n",
    "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\n",
    "            # Objective 1: Hardware Objectives (maximize)\n",
    "            hardware_score = sum(\n",
    "                device.ram + device.storage + device.cpu + device.bandwidth + device.battery + device.charging\n",
    "                for device in selected_devices\n",
    "            )\n",
    "            F[i, 0] = ((6 - hardware_score)/6.0)  # Minimize (negative of hardware score)\n",
    "            # Objective 2: Fairness (prioritize devices with lowest local accuracy)\n",
    "            local_accuracies = []\n",
    "            for device in self.devices:\n",
    "              # just the devices in this solution:\n",
    "                if bitstring[int(device.device_id)] == 1:\n",
    "                  _, accuracy = global_model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "                  local_accuracies.append(accuracy)\n",
    "                else:\n",
    "                  local_accuracies.append(0)\n",
    "            # Fairness score: Sum of (1 - accuracy) for selected devices should be minimized\n",
    "            # This prioritizes devices with lower local accuracy\n",
    "\n",
    "            fairness_score = 0  # Initialize fairness score\n",
    "\n",
    "            for j, bit in enumerate(bitstring):\n",
    "                # print(j)\n",
    "                # print(bit)\n",
    "                if bit == 1:  # Check if the device is selected\n",
    "                    fairness_score += local_accuracies[j]  # Add the corresponding accuracy value\n",
    "            # Now, fairness_score contains the final sum\n",
    "\n",
    "            # fairness_score = sum(local_accuracies[j] for j, bit in enumerate(bitstring) if bit == 1)\n",
    "            F[i, 1] = fairness_score/float(len(selected_devices))  # Minimize (negative of fairness score)  # Added (/Selected Devices) to normalize between 0 and 1\n",
    "            # Objective 3: Global Model Accuracy (maximize)\n",
    "            _, global_accuracy = self.global_model.evaluate(self.x_test_global, self.y_test_global, verbose=0)\n",
    "            F[i, 2] = 1 - global_accuracy  # Minimize (1 - accuracy)\n",
    "        out[\"F\"] = F  # Set the objective values\n",
    "\n",
    "'''\n",
    "            # Update device participation based on the bitstring\n",
    "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\n",
    "\n",
    "            # Train local models for selected devices\n",
    "            for device in selected_devices:\n",
    "                device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "\n",
    "            # Aggregate weights to update the global model\n",
    "            aggregate_weights(self.global_model, selected_devices)\n",
    "\n",
    "            # Distribute the updated global model back to all devices\n",
    "            for device in self.devices:\n",
    "                device.model.set_weights(self.global_model.get_weights())\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "problem = FederatedLearningProblem(\n",
    "    num_devices=NUM_DEVICES,\n",
    "    devices=devices,\n",
    "    global_model=global_model,\n",
    "    x_test_global=x_test_global,\n",
    "    y_test_global=y_test_global\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Configure NSGA-II Algorithm\n",
    "algorithm = NSGA2(\n",
    "    pop_size=POPULATION_SIZE,\n",
    "    sampling=BinaryRandomSampling(),      # Random bitstrings\n",
    "    crossover=TwoPointCrossover(),        # Two-point crossover\n",
    "    mutation=BitflipMutation(),           # Bit flip mutation\n",
    "    eliminate_duplicates=True             # Avoid duplicate solutions\n",
    ")\n",
    "\n",
    "\n",
    "NUMBER_OF_LEARNING_ITERATIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |      100 |      6 |             - |             -\n",
      "     2 |      200 |      9 |  0.1672845678 |         nadir\n",
      "     3 |      300 |      9 |  0.1356322545 |         ideal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 3: Run Optimization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m res = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtermination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDefaultMultiObjectiveTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_max_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_GENERATIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# seed=42,\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\optimize.py:67\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m     algorithm.setup(problem, **kwargs)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# actually execute the algorithm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m res = \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[32m     70\u001b[39m res.algorithm = algorithm\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\algorithm.py:138\u001b[39m, in \u001b[36mAlgorithm.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_next():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\algorithm.py:158\u001b[39m, in \u001b[36mAlgorithm.next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# call the advance with them after evaluation\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infills \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(infills=infills)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# if the algorithm does not follow the infill-advance scheme just call advance\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\evaluator.py:69\u001b[39m, in \u001b[36mEvaluator.eval\u001b[39m\u001b[34m(self, problem, pop, skip_already_evaluated, evaluate_values_of, count_evals, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# evaluate the solutions (if there are any)\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(I) > \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# do the actual evaluation - call the sub-function to set the corresponding values to the population\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# update the function evaluation counter\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_evals:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\evaluator.py:90\u001b[39m, in \u001b[36mEvaluator._eval\u001b[39m\u001b[34m(self, problem, pop, evaluate_values_of, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m X = pop.get(\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# call the problem to evaluate the solutions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m out = \u001b[43mproblem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# for each of the attributes set it to the problem\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m out.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:257\u001b[39m, in \u001b[36mProblem.evaluate\u001b[39m\u001b[34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     only_single_value = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np.ndarray))\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m _out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m out = {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _out.items():\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:299\u001b[39m, in \u001b[36mProblem.do\u001b[39m\u001b[34m(self, X, return_values_of, *args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate_elementwise(X, out, *args, **kwargs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# finally format the output dictionary\u001b[39;00m\n\u001b[32m    302\u001b[39m out = \u001b[38;5;28mself\u001b[39m._format_dict(out, \u001b[38;5;28mlen\u001b[39m(X), return_values_of)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\pymoo\\core\\problem.py:307\u001b[39m, in \u001b[36mProblem._evaluate_vectorized\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_evaluate_vectorized\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, out, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mFederatedLearningProblem._evaluate\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.devices:\n\u001b[32m     58\u001b[39m   \u001b[38;5;66;03m# just the devices in this solution:\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bitstring[\u001b[38;5;28mint\u001b[39m(device.device_id)] == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m       _, accuracy = \u001b[43mglobal_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m       local_accuracies.append(accuracy)\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:479\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28mself\u001b[39m.reset_metrics()\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_test_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:729\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:112\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m._current_iterator\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._steps_seen >= \u001b[38;5;28mself\u001b[39m._num_batches:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    707\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    745\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype.args[\u001b[32m0\u001b[39m].args[\u001b[32m0\u001b[39m].args) == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    746\u001b[39m       \u001b[38;5;28mself\u001b[39m._flat_output_types)\n\u001b[32m    747\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource.op.experimental_set_type(fulltype)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github Repos\\FL\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[39m, in \u001b[36mmake_iterator\u001b[39m\u001b[34m(dataset, iterator, name)\u001b[39m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3508\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3509\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3510\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMakeIterator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3512\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    # termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pareto Front (Bitstrings):\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000000000010111000010000001110\n",
      "1101010011010000111100010111001111011100111100111000111111001111011100101000101110100011111010101111\n",
      "0011001011001000101011010111001101001010011001000100000010111101111110000001000000111000010000001110\n",
      "0011001011001000101011011011011001011100111100100100000010000011110100101110000010100011111010101110\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000001000000100011111010101110\n",
      "0011001011001000101011011011010001011100111100100110000010000011111100101000101101011011111010101110\n",
      "0011001011001000101011010111001101001010011001000100000010111101111110000001000000111000111010100110\n",
      "0011001011001000101011011011011011011100111100111000111111001111011100101000101110100011111010101110\n",
      "0011001011001000101011011011011001011100111100110000111111001111011100101000101110100011111010101110\n",
      "0011001011001000101011010111001111011100111100111000111111001111011100101000101110111000010000001110\n",
      "100\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000000000010111000010000001110\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1171 - loss: 2.3023\n",
      "Global Model Accuracy: 0.1115\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract the Best Pareto Front\n",
    "pareto_front = res.F   # Objective values of solutions in Pareto front\n",
    "pareto_solutions = res.X  # Corresponding bitstrings\n",
    "\n",
    "# Print the Best Pareto Front Solutions\n",
    "print(\"Best Pareto Front (Bitstrings):\")\n",
    "for bitstring in pareto_solutions:\n",
    "    print(\"\".join(map(str, bitstring)).replace('True','1').replace('False','0'))\n",
    "\n",
    "bitstring = pareto_solutions[0] # for now!\n",
    "bitstring = str(bitstring).replace('False','0').replace('True','1')\n",
    "for char in bitstring:\n",
    "    if char != '0' and char != '1':\n",
    "        bitstring = bitstring.replace(char,'')\n",
    "\n",
    "print(len(bitstring))\n",
    "print(bitstring)\n",
    "temp_bitstring = []\n",
    "for bit in bitstring:\n",
    "    temp_bitstring.append(bit)\n",
    "bitstring = temp_bitstring\n",
    "\n",
    "########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=5, verbose=0)\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0']\n",
      "100\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(bitstring) # for now!\n",
    "print(len(bitstring))\n",
    "print(bitstring.count(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      4 |             - |             -\n",
      "     2 |       20 |      7 |  0.2307692319 |         ideal\n",
      "     3 |       30 |     10 |  0.0359156015 |         ideal\n",
      "     4 |       40 |     10 |  0.1612903352 |         ideal\n",
      "     5 |       50 |     10 |  0.0201756792 |             f\n",
      "     6 |       60 |     10 |  0.0331854369 |             f\n",
      "     7 |       70 |     10 |  0.0406118674 |             f\n",
      "     8 |       80 |     10 |  0.0963562733 |         ideal\n",
      "     9 |       90 |     10 |  0.0199798763 |             f\n",
      "    10 |      100 |     10 |  0.0048348089 |         ideal\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Device 2.0: Loss before: [2.3014464378356934, 0.12708333134651184], Loss after: [2.2971396446228027, 0.12708333134651184]\n",
      "Device 3.0: Loss before: [2.3013997077941895, 0.10625000298023224], Loss after: [2.2967536449432373, 0.22708334028720856]\n",
      "Device 6.0: Loss before: [2.3017570972442627, 0.09791667014360428], Loss after: [2.296715497970581, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.3015942573547363, 0.09583333134651184], Loss after: [2.296424150466919, 0.125]\n",
      "Device 9.0: Loss before: [2.3014817237854004, 0.11249999701976776], Loss after: [2.2969343662261963, 0.11249999701976776]\n",
      "Device 12.0: Loss before: [2.3015387058258057, 0.09583333134651184], Loss after: [2.2940421104431152, 0.34583333134651184]\n",
      "Device 16.0: Loss before: [2.3022358417510986, 0.11041666567325592], Loss after: [2.295330762863159, 0.13750000298023224]\n",
      "Device 18.0: Loss before: [2.3010990619659424, 0.12291666865348816], Loss after: [2.2960875034332275, 0.23541666567325592]\n",
      "Device 20.0: Loss before: [2.3014144897460938, 0.13124999403953552], Loss after: [2.2960660457611084, 0.13124999403953552]\n",
      "Device 21.0: Loss before: [2.3011155128479004, 0.12708333134651184], Loss after: [2.295438528060913, 0.12708333134651184]\n",
      "Device 23.0: Loss before: [2.3015966415405273, 0.10000000149011612], Loss after: [2.297272205352783, 0.13124999403953552]\n",
      "Device 24.0: Loss before: [2.3016726970672607, 0.10625000298023224], Loss after: [2.2984235286712646, 0.12083332985639572]\n",
      "Device 26.0: Loss before: [2.3017969131469727, 0.11041666567325592], Loss after: [2.299024820327759, 0.12708333134651184]\n",
      "Device 27.0: Loss before: [2.3018524646759033, 0.1041666641831398], Loss after: [2.2998712062835693, 0.2083333283662796]\n",
      "Device 29.0: Loss before: [2.301513433456421, 0.125], Loss after: [2.295506238937378, 0.125]\n",
      "Device 30.0: Loss before: [2.301795482635498, 0.10000000149011612], Loss after: [2.2998080253601074, 0.2854166626930237]\n",
      "Device 33.0: Loss before: [2.3015336990356445, 0.11874999850988388], Loss after: [2.298020601272583, 0.14166666567325592]\n",
      "Device 36.0: Loss before: [2.301365852355957, 0.11249999701976776], Loss after: [2.29657244682312, 0.125]\n",
      "Device 38.0: Loss before: [2.3014628887176514, 0.11249999701976776], Loss after: [2.2964463233947754, 0.12708333134651184]\n",
      "Device 41.0: Loss before: [2.302135467529297, 0.08541666716337204], Loss after: [2.2973666191101074, 0.12083332985639572]\n",
      "Device 42.0: Loss before: [2.3012354373931885, 0.13333334028720856], Loss after: [2.2949578762054443, 0.13333334028720856]\n",
      "Device 45.0: Loss before: [2.301501512527466, 0.1145833358168602], Loss after: [2.298041343688965, 0.1145833358168602]\n",
      "Device 49.0: Loss before: [2.3015267848968506, 0.12291666865348816], Loss after: [2.295661449432373, 0.18541666865348816]\n",
      "Device 56.0: Loss before: [2.301163911819458, 0.125], Loss after: [2.2950427532196045, 0.125]\n",
      "Device 58.0: Loss before: [2.3013153076171875, 0.11666666716337204], Loss after: [2.2963807582855225, 0.15208333730697632]\n",
      "Device 59.0: Loss before: [2.3014938831329346, 0.10208333283662796], Loss after: [2.2965469360351562, 0.20416666567325592]\n",
      "Device 60.0: Loss before: [2.3012070655822754, 0.12708333134651184], Loss after: [2.2964560985565186, 0.12708333134651184]\n",
      "Device 61.0: Loss before: [2.301917552947998, 0.10625000298023224], Loss after: [2.29617977142334, 0.13750000298023224]\n",
      "Device 63.0: Loss before: [2.301499843597412, 0.11874999850988388], Loss after: [2.2974956035614014, 0.11874999850988388]\n",
      "Device 64.0: Loss before: [2.3019778728485107, 0.09166666865348816], Loss after: [2.297877073287964, 0.15416666865348816]\n",
      "Device 66.0: Loss before: [2.301910877227783, 0.10625000298023224], Loss after: [2.2969229221343994, 0.125]\n",
      "Device 67.0: Loss before: [2.3015851974487305, 0.11666666716337204], Loss after: [2.298938035964966, 0.11666666716337204]\n",
      "Device 68.0: Loss before: [2.3014917373657227, 0.11874999850988388], Loss after: [2.296771764755249, 0.11874999850988388]\n",
      "Device 80.0: Loss before: [2.302156925201416, 0.10208333283662796], Loss after: [2.2989695072174072, 0.125]\n",
      "Device 82.0: Loss before: [2.3012452125549316, 0.13124999403953552], Loss after: [2.297199010848999, 0.13124999403953552]\n",
      "Device 83.0: Loss before: [2.3017184734344482, 0.11874999850988388], Loss after: [2.297882318496704, 0.18125000596046448]\n",
      "Device 84.0: Loss before: [2.3016233444213867, 0.09791667014360428], Loss after: [2.298541784286499, 0.11666666716337204]\n",
      "Device 89.0: Loss before: [2.3009369373321533, 0.13750000298023224], Loss after: [2.29496431350708, 0.13750000298023224]\n",
      "Device 96.0: Loss before: [2.301833152770996, 0.09791667014360428], Loss after: [2.297668933868408, 0.1354166716337204]\n",
      "Device 97.0: Loss before: [2.301381826400757, 0.125], Loss after: [2.2985572814941406, 0.125]\n",
      "Device 98.0: Loss before: [2.3016843795776367, 0.1354166716337204], Loss after: [2.295980930328369, 0.1354166716337204]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1169 - loss: 2.2997\n",
      "Global Model Accuracy: 0.1114\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      7 |             - |             -\n",
      "     2 |       20 |     10 |  0.1149033047 |         ideal\n",
      "     3 |       30 |     10 |  0.1263791392 |         ideal\n",
      "     4 |       40 |     10 |  0.0446626673 |             f\n",
      "     5 |       50 |     10 |  0.0531813837 |         ideal\n",
      "     6 |       60 |     10 |  0.0627976867 |         ideal\n",
      "     7 |       70 |     10 |  0.0666441130 |         ideal\n",
      "     8 |       80 |     10 |  0.0185034611 |             f\n",
      "     9 |       90 |     10 |  0.0322993886 |             f\n",
      "    10 |      100 |     10 |  0.0970137703 |         ideal\n",
      "False\n",
      "Device 2.0: Loss before: [2.297051191329956, 0.12708333134651184], Loss after: [2.2878005504608154, 0.12708333134651184]\n",
      "Device 3.0: Loss before: [2.2966902256011963, 0.10625000298023224], Loss after: [2.28582763671875, 0.30416667461395264]\n",
      "Device 6.0: Loss before: [2.2979109287261963, 0.09791667014360428], Loss after: [2.2868010997772217, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.297337293624878, 0.09583333134651184], Loss after: [2.2859737873077393, 0.19166666269302368]\n",
      "Device 9.0: Loss before: [2.2969717979431152, 0.11249999701976776], Loss after: [2.286633253097534, 0.21250000596046448]\n",
      "Device 12.0: Loss before: [2.2970564365386963, 0.09583333134651184], Loss after: [2.282228946685791, 0.3395833373069763]\n",
      "Device 16.0: Loss before: [2.2995731830596924, 0.11041666567325592], Loss after: [2.287156343460083, 0.21875]\n",
      "Device 18.0: Loss before: [2.2960264682769775, 0.12291666865348816], Loss after: [2.286231756210327, 0.20000000298023224]\n",
      "Device 20.0: Loss before: [2.296933174133301, 0.13124999403953552], Loss after: [2.286652088165283, 0.13750000298023224]\n",
      "Device 21.0: Loss before: [2.2959656715393066, 0.12708333134651184], Loss after: [2.284320592880249, 0.25833332538604736]\n",
      "Device 23.0: Loss before: [2.2974159717559814, 0.10000000149011612], Loss after: [2.2876734733581543, 0.13124999403953552]\n",
      "Device 24.0: Loss before: [2.2977235317230225, 0.10625000298023224], Loss after: [2.2892298698425293, 0.20208333432674408]\n",
      "Device 26.0: Loss before: [2.2982523441314697, 0.11041666567325592], Loss after: [2.2909278869628906, 0.14791665971279144]\n",
      "Device 27.0: Loss before: [2.2982442378997803, 0.1041666641831398], Loss after: [2.290889024734497, 0.2395833283662796]\n",
      "Device 29.0: Loss before: [2.2973530292510986, 0.125], Loss after: [2.286468982696533, 0.13750000298023224]\n",
      "Device 30.0: Loss before: [2.298081874847412, 0.10000000149011612], Loss after: [2.290691375732422, 0.36666667461395264]\n",
      "Device 33.0: Loss before: [2.2972919940948486, 0.11874999850988388], Loss after: [2.288032293319702, 0.21875]\n",
      "Device 36.0: Loss before: [2.2969465255737305, 0.11249999701976776], Loss after: [2.2874176502227783, 0.21250000596046448]\n",
      "Device 38.0: Loss before: [2.2969579696655273, 0.11249999701976776], Loss after: [2.2857446670532227, 0.3395833373069763]\n",
      "Device 41.0: Loss before: [2.298980474472046, 0.08541666716337204], Loss after: [2.288041830062866, 0.5041666626930237]\n",
      "Device 42.0: Loss before: [2.296447515487671, 0.13333334028720856], Loss after: [2.2848029136657715, 0.16458334028720856]\n",
      "Device 45.0: Loss before: [2.2973082065582275, 0.1145833358168602], Loss after: [2.288760185241699, 0.16249999403953552]\n",
      "Device 49.0: Loss before: [2.297332525253296, 0.12291666865348816], Loss after: [2.2858645915985107, 0.23541666567325592]\n",
      "Device 56.0: Loss before: [2.296236276626587, 0.125], Loss after: [2.28594970703125, 0.125]\n",
      "Device 58.0: Loss before: [2.29667329788208, 0.11666666716337204], Loss after: [2.286677837371826, 0.1875]\n",
      "Device 59.0: Loss before: [2.2971878051757812, 0.10208333283662796], Loss after: [2.2873311042785645, 0.22291666269302368]\n",
      "Device 60.0: Loss before: [2.2964234352111816, 0.12708333134651184], Loss after: [2.2871131896972656, 0.12708333134651184]\n",
      "Device 61.0: Loss before: [2.2982711791992188, 0.10625000298023224], Loss after: [2.286099433898926, 0.2395833283662796]\n",
      "Device 63.0: Loss before: [2.2971651554107666, 0.11874999850988388], Loss after: [2.288503408432007, 0.125]\n",
      "Device 64.0: Loss before: [2.298503875732422, 0.09166666865348816], Loss after: [2.288339853286743, 0.23749999701976776]\n",
      "Device 66.0: Loss before: [2.2988693714141846, 0.10625000298023224], Loss after: [2.2896809577941895, 0.24166665971279144]\n",
      "Device 67.0: Loss before: [2.297590732574463, 0.11666666716337204], Loss after: [2.290259599685669, 0.13750000298023224]\n",
      "Device 68.0: Loss before: [2.2974421977996826, 0.11874999850988388], Loss after: [2.288296699523926, 0.12916666269302368]\n",
      "Device 80.0: Loss before: [2.2993788719177246, 0.10208333283662796], Loss after: [2.2914416790008545, 0.24791666865348816]\n",
      "Device 82.0: Loss before: [2.2965216636657715, 0.13124999403953552], Loss after: [2.28778338432312, 0.1354166716337204]\n",
      "Device 83.0: Loss before: [2.2981009483337402, 0.11874999850988388], Loss after: [2.289912462234497, 0.15416666865348816]\n",
      "Device 84.0: Loss before: [2.297713041305542, 0.09791667014360428], Loss after: [2.289996385574341, 0.27291667461395264]\n",
      "Device 89.0: Loss before: [2.295679807662964, 0.13750000298023224], Loss after: [2.2857773303985596, 0.13750000298023224]\n",
      "Device 96.0: Loss before: [2.298491954803467, 0.09791667014360428], Loss after: [2.2899844646453857, 0.1354166716337204]\n",
      "Device 97.0: Loss before: [2.2969021797180176, 0.125], Loss after: [2.289393663406372, 0.125]\n",
      "Device 98.0: Loss before: [2.297952890396118, 0.1354166716337204], Loss after: [2.287442922592163, 0.1354166716337204]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1382 - loss: 2.2909\n",
      "Global Model Accuracy: 0.1340\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      6 |             - |             -\n",
      "     2 |       20 |     10 |  0.1140514774 |         ideal\n",
      "     3 |       30 |     10 |  0.0191277690 |         ideal\n",
      "     4 |       40 |     10 |  0.0872905024 |         ideal\n",
      "     5 |       50 |     10 |  0.0777815630 |         ideal\n",
      "     6 |       60 |     10 |  0.0156152389 |         ideal\n",
      "     7 |       70 |     10 |  0.0436081211 |         ideal\n",
      "     8 |       80 |     10 |  0.0334872969 |         ideal\n",
      "     9 |       90 |     10 |  0.0040579713 |         nadir\n",
      "    10 |      100 |     10 |  0.0163946562 |             f\n",
      "False\n",
      "Device 2.0: Loss before: [2.2921078205108643, 0.13124999403953552], Loss after: [2.2730791568756104, 0.20208333432674408]\n",
      "Device 3.0: Loss before: [2.291351318359375, 0.12083332985639572], Loss after: [2.267686605453491, 0.44583332538604736]\n",
      "Device 6.0: Loss before: [2.2930214405059814, 0.1041666641831398], Loss after: [2.2700908184051514, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.2922558784484863, 0.10208333283662796], Loss after: [2.268527030944824, 0.38749998807907104]\n",
      "Device 9.0: Loss before: [2.2917940616607666, 0.12291666865348816], Loss after: [2.269821882247925, 0.3291666805744171]\n",
      "Device 12.0: Loss before: [2.2916953563690186, 0.10208333283662796], Loss after: [2.2612757682800293, 0.375]\n",
      "Device 16.0: Loss before: [2.295356512069702, 0.11874999850988388], Loss after: [2.272477626800537, 0.34583333134651184]\n",
      "Device 18.0: Loss before: [2.290817975997925, 0.125], Loss after: [2.2713074684143066, 0.24791666865348816]\n",
      "Device 20.0: Loss before: [2.291989803314209, 0.1354166716337204], Loss after: [2.271836042404175, 0.25833332538604736]\n",
      "Device 21.0: Loss before: [2.2905099391937256, 0.12916666269302368], Loss after: [2.2663254737854004, 0.34375]\n",
      "Device 23.0: Loss before: [2.2924582958221436, 0.1041666641831398], Loss after: [2.2721166610717773, 0.3125]\n",
      "Device 24.0: Loss before: [2.2928855419158936, 0.10833333432674408], Loss after: [2.2743468284606934, 0.2854166626930237]\n",
      "Device 26.0: Loss before: [2.2937607765197754, 0.1145833358168602], Loss after: [2.2778289318084717, 0.32083332538604736]\n",
      "Device 27.0: Loss before: [2.293498992919922, 0.11041666567325592], Loss after: [2.275768995285034, 0.34375]\n",
      "Device 29.0: Loss before: [2.2925779819488525, 0.13333334028720856], Loss after: [2.2720718383789062, 0.34166666865348816]\n",
      "Device 30.0: Loss before: [2.2932887077331543, 0.10208333283662796], Loss after: [2.2754194736480713, 0.46666666865348816]\n",
      "Device 33.0: Loss before: [2.292241096496582, 0.12708333134651184], Loss after: [2.271362543106079, 0.27291667461395264]\n",
      "Device 36.0: Loss before: [2.2920639514923096, 0.11249999701976776], Loss after: [2.2731411457061768, 0.35624998807907104]\n",
      "Device 38.0: Loss before: [2.291724681854248, 0.11874999850988388], Loss after: [2.2678017616271973, 0.36250001192092896]\n",
      "Device 41.0: Loss before: [2.2943332195281982, 0.09166666865348816], Loss after: [2.2715742588043213, 0.5583333373069763]\n",
      "Device 42.0: Loss before: [2.291267156600952, 0.13333334028720856], Loss after: [2.2687759399414062, 0.3083333373069763]\n",
      "Device 45.0: Loss before: [2.2923858165740967, 0.12083332985639572], Loss after: [2.273850679397583, 0.40833333134651184]\n",
      "Device 49.0: Loss before: [2.2923998832702637, 0.12916666269302368], Loss after: [2.2699198722839355, 0.28333333134651184]\n",
      "Device 56.0: Loss before: [2.2912817001342773, 0.12708333134651184], Loss after: [2.2724294662475586, 0.22291666269302368]\n",
      "Device 58.0: Loss before: [2.291612386703491, 0.12708333134651184], Loss after: [2.271458148956299, 0.3583333194255829]\n",
      "Device 59.0: Loss before: [2.2922933101654053, 0.10208333283662796], Loss after: [2.2726926803588867, 0.49166667461395264]\n",
      "Device 60.0: Loss before: [2.291414499282837, 0.12708333134651184], Loss after: [2.272871971130371, 0.22291666269302368]\n",
      "Device 61.0: Loss before: [2.293398141860962, 0.11041666567325592], Loss after: [2.2678656578063965, 0.25208333134651184]\n",
      "Device 63.0: Loss before: [2.2922961711883545, 0.125], Loss after: [2.274266242980957, 0.3187499940395355]\n",
      "Device 64.0: Loss before: [2.2937252521514893, 0.09791667014360428], Loss after: [2.271533250808716, 0.26875001192092896]\n",
      "Device 66.0: Loss before: [2.2948005199432373, 0.10833333432674408], Loss after: [2.2778875827789307, 0.3395833373069763]\n",
      "Device 67.0: Loss before: [2.292853832244873, 0.125], Loss after: [2.2763571739196777, 0.31458333134651184]\n",
      "Device 68.0: Loss before: [2.2927968502044678, 0.12291666865348816], Loss after: [2.2751736640930176, 0.23541666567325592]\n",
      "Device 80.0: Loss before: [2.295168399810791, 0.1041666641831398], Loss after: [2.2783591747283936, 0.35208332538604736]\n",
      "Device 82.0: Loss before: [2.291489362716675, 0.13958333432674408], Loss after: [2.2730438709259033, 0.25208333134651184]\n",
      "Device 83.0: Loss before: [2.2936971187591553, 0.12083332985639572], Loss after: [2.277120351791382, 0.21250000596046448]\n",
      "Device 84.0: Loss before: [2.2930305004119873, 0.10833333432674408], Loss after: [2.2764456272125244, 0.39375001192092896]\n",
      "Device 89.0: Loss before: [2.2906413078308105, 0.14166666567325592], Loss after: [2.2726314067840576, 0.15625]\n",
      "Device 96.0: Loss before: [2.294203281402588, 0.10208333283662796], Loss after: [2.277503728866577, 0.13750000298023224]\n",
      "Device 97.0: Loss before: [2.2919440269470215, 0.12916666269302368], Loss after: [2.2752206325531006, 0.23333333432674408]\n",
      "Device 98.0: Loss before: [2.2933833599090576, 0.1458333283662796], Loss after: [2.273547887802124, 0.20416666567325592]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3403 - loss: 2.2767\n",
      "Global Model Accuracy: 0.3383\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      6 |             - |             -\n",
      "     2 |       20 |     10 |  0.0962540945 |         ideal\n",
      "     3 |       30 |     10 |  0.1702600751 |         ideal\n",
      "     4 |       40 |     10 |  0.0457933994 |         ideal\n",
      "     5 |       50 |     10 |  0.0358850306 |             f\n",
      "     6 |       60 |     10 |  0.0693756153 |         ideal\n",
      "     7 |       70 |     10 |  0.0096587875 |             f\n",
      "     8 |       80 |     10 |  0.0514744528 |         ideal\n",
      "     9 |       90 |     10 |  0.0085521767 |             f\n",
      "    10 |      100 |     10 |  0.0276725565 |             f\n",
      "False\n",
      "Device 2.0: Loss before: [2.279834747314453, 0.31458333134651184], Loss after: [2.2243316173553467, 0.5]\n",
      "Device 3.0: Loss before: [2.277980327606201, 0.2874999940395355], Loss after: [2.204665184020996, 0.512499988079071]\n",
      "Device 6.0: Loss before: [2.2804510593414307, 0.2916666567325592], Loss after: [2.213855504989624, 0.24583333730697632]\n",
      "Device 8.0: Loss before: [2.2793796062469482, 0.2916666567325592], Loss after: [2.208789110183716, 0.42916667461395264]\n",
      "Device 9.0: Loss before: [2.2788119316101074, 0.3166666626930237], Loss after: [2.212261199951172, 0.4416666626930237]\n",
      "Device 12.0: Loss before: [2.278029441833496, 0.32083332538604736], Loss after: [2.185131549835205, 0.35208332538604736]\n",
      "Device 16.0: Loss before: [2.283895969390869, 0.27916666865348816], Loss after: [2.221362352371216, 0.40416666865348816]\n",
      "Device 18.0: Loss before: [2.278273105621338, 0.30000001192092896], Loss after: [2.2222506999969482, 0.4833333194255829]\n",
      "Device 20.0: Loss before: [2.2797892093658447, 0.3062500059604645], Loss after: [2.22204852104187, 0.3958333432674408]\n",
      "Device 21.0: Loss before: [2.277156352996826, 0.34166666865348816], Loss after: [2.203223466873169, 0.3791666626930237]\n",
      "Device 23.0: Loss before: [2.2799623012542725, 0.3333333432674408], Loss after: [2.2199366092681885, 0.518750011920929]\n",
      "Device 24.0: Loss before: [2.280587911605835, 0.27291667461395264], Loss after: [2.225346088409424, 0.4375]\n",
      "Device 26.0: Loss before: [2.2822470664978027, 0.2708333432674408], Loss after: [2.2348573207855225, 0.5104166865348816]\n",
      "Device 27.0: Loss before: [2.2811665534973145, 0.2770833373069763], Loss after: [2.2246882915496826, 0.4583333432674408]\n",
      "Device 29.0: Loss before: [2.280632257461548, 0.3125], Loss after: [2.2238428592681885, 0.4270833432674408]\n",
      "Device 30.0: Loss before: [2.2808899879455566, 0.2770833373069763], Loss after: [2.2239229679107666, 0.4854166805744171]\n",
      "Device 33.0: Loss before: [2.2794175148010254, 0.3020833432674408], Loss after: [2.212902545928955, 0.3499999940395355]\n",
      "Device 36.0: Loss before: [2.280041217803955, 0.3083333373069763], Loss after: [2.2262723445892334, 0.6541666388511658]\n",
      "Device 38.0: Loss before: [2.2785353660583496, 0.3270833194255829], Loss after: [2.2056126594543457, 0.37708333134651184]\n",
      "Device 41.0: Loss before: [2.2819442749023438, 0.2604166567325592], Loss after: [2.2155814170837402, 0.44583332538604736]\n",
      "Device 42.0: Loss before: [2.2785634994506836, 0.30000001192092896], Loss after: [2.2146947383880615, 0.3687500059604645]\n",
      "Device 45.0: Loss before: [2.280015707015991, 0.3062500059604645], Loss after: [2.2237002849578857, 0.5520833134651184]\n",
      "Device 49.0: Loss before: [2.2799885272979736, 0.3083333373069763], Loss after: [2.2169034481048584, 0.36250001192092896]\n",
      "Device 56.0: Loss before: [2.2794337272644043, 0.28125], Loss after: [2.228609800338745, 0.518750011920929]\n",
      "Device 58.0: Loss before: [2.2791526317596436, 0.3229166567325592], Loss after: [2.2206947803497314, 0.5291666388511658]\n",
      "Device 59.0: Loss before: [2.2800791263580322, 0.2666666805744171], Loss after: [2.2241368293762207, 0.6270833611488342]\n",
      "Device 60.0: Loss before: [2.2792551517486572, 0.28958332538604736], Loss after: [2.2261433601379395, 0.5583333373069763]\n",
      "Device 61.0: Loss before: [2.2805705070495605, 0.3166666626930237], Loss after: [2.201840877532959, 0.2874999940395355]\n",
      "Device 63.0: Loss before: [2.28016996383667, 0.2854166626930237], Loss after: [2.2269866466522217, 0.4437499940395355]\n",
      "Device 64.0: Loss before: [2.2811272144317627, 0.27916666865348816], Loss after: [2.2128794193267822, 0.28333333134651184]\n",
      "Device 66.0: Loss before: [2.2842795848846436, 0.2562499940395355], Loss after: [2.23970890045166, 0.44583332538604736]\n",
      "Device 67.0: Loss before: [2.2808799743652344, 0.2874999940395355], Loss after: [2.2299065589904785, 0.4854166805744171]\n",
      "Device 68.0: Loss before: [2.2812530994415283, 0.27291667461395264], Loss after: [2.232931613922119, 0.6145833134651184]\n",
      "Device 80.0: Loss before: [2.2838504314422607, 0.28333333134651184], Loss after: [2.23396372795105, 0.42916667461395264]\n",
      "Device 82.0: Loss before: [2.279125213623047, 0.3229166567325592], Loss after: [2.2231991291046143, 0.3708333373069763]\n",
      "Device 83.0: Loss before: [2.282504081726074, 0.2666666805744171], Loss after: [2.234520435333252, 0.3895833194255829]\n",
      "Device 84.0: Loss before: [2.281235456466675, 0.25208333134651184], Loss after: [2.232619285583496, 0.5520833134651184]\n",
      "Device 89.0: Loss before: [2.2788283824920654, 0.27291667461395264], Loss after: [2.230720043182373, 0.4729166626930237]\n",
      "Device 96.0: Loss before: [2.283189535140991, 0.2645833194255829], Loss after: [2.236621618270874, 0.3062500059604645]\n",
      "Device 97.0: Loss before: [2.2797067165374756, 0.29374998807907104], Loss after: [2.228773355484009, 0.581250011920929]\n",
      "Device 98.0: Loss before: [2.281740427017212, 0.30416667461395264], Loss after: [2.226771831512451, 0.3958333432674408]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5205 - loss: 2.2294\n",
      "Global Model Accuracy: 0.5207\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=5, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# w1 = global_model.get_weights()\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# w2 = global_model.get_weights()\n",
    "# print(\"global model stayed the same?\")\n",
    "# print(np.array_equal(w1,w2))\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before aggregation:\", w1[0].flatten()[:5])\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# print(\"After aggregation:\", global_model.get_weights()[0].flatten()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "#     loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "#     loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "#     print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# w1 = global_model.get_weights()\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# w2 = global_model.get_weights()\n",
    "# print(\"global model stayed the same?\")\n",
    "# print(np.array_equal(w1,w2))\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(NUMBER_OF_LEARNING_ITERATIONS):\n",
    "#     # Step 3: Run Optimization\n",
    "#     res = minimize(\n",
    "#         problem=problem,\n",
    "#         algorithm=algorithm,\n",
    "#         # termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\n",
    "#         termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "#         seed=42,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Step 4: Extract the Best Pareto Front\n",
    "#     pareto_front = res.F   # Objective values of solutions in Pareto front\n",
    "#     pareto_solutions = res.X  # Corresponding bitstrings\n",
    "\n",
    "#     # Print the Best Pareto Front Solutions\n",
    "#     print(\"Best Pareto Front (Bitstrings):\")\n",
    "#     for bitstring in pareto_solutions:\n",
    "#         print(\"\".join(map(str, bitstring)))\n",
    "\n",
    "#     bitstring = pareto_solutions[0] # for now!\n",
    "#     print(bitstring)\n",
    "#     # Convert to bitstring (list of 1s and 0s)\n",
    "#     bitstring = [1 if word == \"True\" else 0 for word in str(bitstring).replace(\"True\", \"1 \").replace(\"False\", \"0 \").split()]\n",
    "\n",
    "\n",
    "#     ########################################################\n",
    "#     # Update device participation based on the bitstring\n",
    "#     selected_devices = [device for device in devices if bitstring[int(device.device_id)] == 1]\n",
    "\n",
    "#     # Aggregate weights to update the global model\n",
    "#     aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "#     # Distribute the updated global model back to all devices\n",
    "#     for device in devices:\n",
    "#         device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "#     current_learning_iteration += 1\n",
    "#     # Train local models for selected devices\n",
    "#     for device in selected_devices:\n",
    "#         device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "#         device.number_of_times_fitted += 1\n",
    "#         device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "#     test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "#     print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#     ################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
