{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "current_learning_iteration = 0\n",
    "\n",
    "# Device Class\n",
    "class Device:\n",
    "    def __init__(self, device_id, ram, storage, cpu, bandwidth, battery, charging):\n",
    "        self.device_id = device_id\n",
    "        self.ram = ram\n",
    "        self.storage = storage\n",
    "        self.cpu = cpu\n",
    "        self.bandwidth = bandwidth\n",
    "        self.battery = battery\n",
    "        self.charging = charging\n",
    "        self.energy_consumption = ram + storage + cpu + bandwidth\n",
    "        self.model = self.create_model()\n",
    "        self.last_round_participated = 0\n",
    "        self.data = None  # Placeholder for dataset partition\n",
    "\n",
    "        self.number_of_times_fitted = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Global Model\n",
    "# Define the global model with the same architecture\n",
    "def create_global_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "global_model = create_global_model()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_weights(global_model, devices):\n",
    "    \"\"\"Computes the weighted average of model weights from all devices and updates the global model.\"\"\"\n",
    "\n",
    "    num_devices = len(devices)\n",
    "    if num_devices == 0:\n",
    "        print(\"No devices available for aggregation.\")\n",
    "        return\n",
    "\n",
    "    # Get device weights and participation ratios\n",
    "    device_weights = [device.model.get_weights() for device in devices]\n",
    "    device_participation_ratio = np.array(\n",
    "        [device.last_round_participated / current_learning_iteration for device in devices]\n",
    "    )\n",
    "\n",
    "    # Total weight for normalization\n",
    "    total_weight = np.sum(device_participation_ratio)\n",
    "    if total_weight == 0:\n",
    "        print(\"Total weight is zero, cannot perform aggregation.\")\n",
    "        return\n",
    "\n",
    "    # Compute weighted sum of weights\n",
    "    weighted_sums = [\n",
    "        np.sum(np.stack([device_weights[i][layer] * device_participation_ratio[i] for i in range(num_devices)]), axis=0)\n",
    "        for layer in range(len(device_weights[0]))\n",
    "    ]\n",
    "\n",
    "    # Compute weighted average\n",
    "    weighted_avg_weights = [layer_sum / total_weight for layer_sum in weighted_sums]\n",
    "\n",
    "    # Set the global model's weights to the weighted averaged weights\n",
    "    global_model.set_weights(weighted_avg_weights)\n",
    "    print(np.array_equal(global_model.get_weights(), weighted_avg_weights))\n",
    "    # print(global_model.get_weights())\n",
    "    # for i in range(100):\n",
    "    #     print()\n",
    "    # print(weighted_avg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Repos\\FL\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0829 - loss: 2.2924      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2496 - loss: 2.2190 \n",
      "1\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1401 - loss: 2.2755  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3932 - loss: 2.1995 \n",
      "2\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1163 - loss: 2.3010  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2039 - loss: 2.2394 \n",
      "3\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1026 - loss: 2.3141  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2877 - loss: 2.2154 \n",
      "4\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1067 - loss: 2.2995  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2897 - loss: 2.2211 \n",
      "5\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0902 - loss: 2.2782  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4037 - loss: 2.1764 \n",
      "6\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0782 - loss: 2.3011  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1835 - loss: 2.2110 \n",
      "7\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1734 - loss: 2.2810  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3813 - loss: 2.1984 \n",
      "8\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1153 - loss: 2.3039  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2177 - loss: 2.2146 \n",
      "9\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1341 - loss: 2.2977  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2353 - loss: 2.2409 \n",
      "10\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0993 - loss: 2.3015  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1858 - loss: 2.2642 \n",
      "11\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1973 - loss: 2.2676  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3599 - loss: 2.1955 \n",
      "12\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1458 - loss: 2.2977  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3088 - loss: 2.1806 \n",
      "13\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1379 - loss: 2.2926  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2616 - loss: 2.2215 \n",
      "14\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1148 - loss: 2.3005  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1791 - loss: 2.2283 \n",
      "15\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1280 - loss: 2.2976  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2597 - loss: 2.2589 \n",
      "16\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1761 - loss: 2.2836  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3356 - loss: 2.2044 \n",
      "17\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1477 - loss: 2.2996  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2213 - loss: 2.2225 \n",
      "18\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1018 - loss: 2.3107  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2000 - loss: 2.2816 \n",
      "19\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1569 - loss: 2.2908      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3952 - loss: 2.2108 \n",
      "20\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1675 - loss: 2.2908  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2806 - loss: 2.2276 \n",
      "21\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1696 - loss: 2.2762  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4262 - loss: 2.1797 \n",
      "22\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1100 - loss: 2.2881  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2790 - loss: 2.2406 \n",
      "23\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1106 - loss: 2.3017  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2822 - loss: 2.2251 \n",
      "24\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1554 - loss: 2.2819  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3172 - loss: 2.2109 \n",
      "25\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1450 - loss: 2.3085  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2136 - loss: 2.2517 \n",
      "26\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1258 - loss: 2.3108  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2045 - loss: 2.2334 \n",
      "27\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1468 - loss: 2.2899  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2162 - loss: 2.2134 \n",
      "28\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1345 - loss: 2.2981  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3604 - loss: 2.1903 \n",
      "29\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1461 - loss: 2.3006  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2615 - loss: 2.2426 \n",
      "30\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1276 - loss: 2.2859  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2951 - loss: 2.2179 \n",
      "31\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1165 - loss: 2.3017  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3315 - loss: 2.1856 \n",
      "32\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1192 - loss: 2.2761  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3125 - loss: 2.1884 \n",
      "33\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1442 - loss: 2.3040  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2047 - loss: 2.2499 \n",
      "34\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1339 - loss: 2.2913  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3342 - loss: 2.2142 \n",
      "35\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1804 - loss: 2.2834  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3193 - loss: 2.2021 \n",
      "36\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1323 - loss: 2.2927  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3294 - loss: 2.2341 \n",
      "37\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0973 - loss: 2.3021  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3105 - loss: 2.2212 \n",
      "38\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1162 - loss: 2.2953  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2662 - loss: 2.2171 \n",
      "39\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1108 - loss: 2.2961  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3044 - loss: 2.2109 \n",
      "40\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2077 - loss: 2.2687  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3924 - loss: 2.1764 \n",
      "41\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1431 - loss: 2.2955  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3050 - loss: 2.2135 \n",
      "42\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1075 - loss: 2.2935  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3101 - loss: 2.2071 \n",
      "43\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1438 - loss: 2.2956  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3017 - loss: 2.2115 \n",
      "44\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1365 - loss: 2.2874  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3665 - loss: 2.2011 \n",
      "45\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0974 - loss: 2.3036  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2261 - loss: 2.2689 \n",
      "46\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0701 - loss: 2.3194  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2318 - loss: 2.2347 \n",
      "47\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1749 - loss: 2.2922  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3359 - loss: 2.2402 \n",
      "48\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1837 - loss: 2.2800  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3161 - loss: 2.2065 \n",
      "49\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1217 - loss: 2.3018  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2130 - loss: 2.2265\n",
      "50\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0671 - loss: 2.3019      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2006 - loss: 2.2494\n",
      "51\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1153 - loss: 2.2992  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2339 - loss: 2.2635 \n",
      "52\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1342 - loss: 2.2828  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3001 - loss: 2.1714 \n",
      "53\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1387 - loss: 2.2938  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3409 - loss: 2.2487 \n",
      "54\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1068 - loss: 2.2943      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3176 - loss: 2.2394 \n",
      "55\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1348 - loss: 2.3077  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3106 - loss: 2.2311 \n",
      "56\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1412 - loss: 2.2877  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3196 - loss: 2.2110 \n",
      "57\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1356 - loss: 2.2996  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2265 - loss: 2.2383 \n",
      "58\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1639 - loss: 2.2888  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2464 - loss: 2.2308 \n",
      "59\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1091 - loss: 2.3061  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2788 - loss: 2.2280 \n",
      "60\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0791 - loss: 2.3033  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3226 - loss: 2.2213 \n",
      "61\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1157 - loss: 2.2925  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2302 - loss: 2.2423 \n",
      "62\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0903 - loss: 2.2962  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3366 - loss: 2.2184 \n",
      "63\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1278 - loss: 2.2747  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3801 - loss: 2.2016 \n",
      "64\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1900 - loss: 2.2943  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3150 - loss: 2.2421 \n",
      "65\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1323 - loss: 2.2807  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3140 - loss: 2.2158 \n",
      "66\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1618 - loss: 2.2902  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2967 - loss: 2.2367 \n",
      "67\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1072 - loss: 2.2945  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2683 - loss: 2.2336 \n",
      "68\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1067 - loss: 2.2784  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3915 - loss: 2.1908 \n",
      "69\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1581 - loss: 2.2827  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3411 - loss: 2.2111 \n",
      "70\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1438 - loss: 2.2806  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3144 - loss: 2.2057 \n",
      "71\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1238 - loss: 2.2890  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2383 - loss: 2.2582 \n",
      "72\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0684 - loss: 2.3000  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1760 - loss: 2.2442 \n",
      "73\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1519 - loss: 2.2887  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3858 - loss: 2.1947 \n",
      "74\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0844 - loss: 2.3140  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2442 - loss: 2.2589 \n",
      "75\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1564 - loss: 2.2820  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1945 - loss: 2.2338 \n",
      "76\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0805 - loss: 2.2961  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2871 - loss: 2.2266 \n",
      "77\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1403 - loss: 2.2882  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2782 - loss: 2.2371 \n",
      "78\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1468 - loss: 2.2764  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3657 - loss: 2.2000 \n",
      "79\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0980 - loss: 2.3097  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2316 - loss: 2.2561 \n",
      "80\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1094 - loss: 2.2825  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2563 - loss: 2.1955 \n",
      "81\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1380 - loss: 2.3059  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2385 - loss: 2.2661 \n",
      "82\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1298 - loss: 2.3021  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3828 - loss: 2.2079 \n",
      "83\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1159 - loss: 2.2727  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2882 - loss: 2.2012 \n",
      "84\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0681 - loss: 2.3014  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2120 - loss: 2.2411 \n",
      "85\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0832 - loss: 2.3057      \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2402 - loss: 2.2638 \n",
      "86\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0902 - loss: 2.3080  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2687 - loss: 2.2152 \n",
      "87\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0959 - loss: 2.3041  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3205 - loss: 2.2220 \n",
      "88\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1309 - loss: 2.2927  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3117 - loss: 2.2516 \n",
      "89\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0603 - loss: 2.2989  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2077 - loss: 2.2550 \n",
      "90\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1659 - loss: 2.2709  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4858 - loss: 2.1649 \n",
      "91\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0917 - loss: 2.3045  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3627 - loss: 2.2354 \n",
      "92\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1489 - loss: 2.2955  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2792 - loss: 2.1850 \n",
      "93\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1665 - loss: 2.2910  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3447 - loss: 2.2143 \n",
      "94\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1736 - loss: 2.2937  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3287 - loss: 2.1902 \n",
      "95\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1179 - loss: 2.2905  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2969 - loss: 2.2403 \n",
      "96\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1649 - loss: 2.2739  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3063 - loss: 2.1927 \n",
      "97\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1046 - loss: 2.3033  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2339 - loss: 2.2137 \n",
      "98\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1342 - loss: 2.2848  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3647 - loss: 2.2120 \n",
      "99\n",
      "Epoch 1/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1106 - loss: 2.2943  \n",
      "Epoch 2/2\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4482 - loss: 2.1587 \n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1169 - loss: 2.3024\n",
      "Global Model Accuracy: 0.1114\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from CSV\n",
    "csv_file = 'devices.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "\n",
    "# Convert CSV rows into device objects\n",
    "devices = []\n",
    "for _, row in df.iterrows():\n",
    "    device = Device(\n",
    "        row['id'], row['ram'], row['storage'], row['cpu'], row['bandwidth'], row['battery'],\n",
    "        row.get('charging', 0)\n",
    "    )\n",
    "    devices.append(device)\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data and reshape for CNN\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)  # Add channel dimension\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "# Split into global test set (20%) and training set (80%)\n",
    "split_index = int(0.8 * len(x_train))\n",
    "x_train_devices, y_train_devices = x_train[:split_index], y_train[:split_index]\n",
    "x_test_global, y_test_global = x_train[split_index:], y_train[split_index:]\n",
    "\n",
    "\n",
    "# Split dataset among devices\n",
    "num_devices = len(devices)\n",
    "split_size = len(x_train_devices) // num_devices\n",
    "\n",
    "for i, device in enumerate(devices):\n",
    "    start = i * split_size\n",
    "    end = (i + 1) * split_size if i < num_devices - 1 else len(x_train_devices)\n",
    "    device.data = (x_train_devices[start:end], y_train_devices[start:end])\n",
    "\n",
    "\n",
    "with open('bitstring.txt', 'r') as f:\n",
    "    bitstring = f.read()\n",
    "\n",
    "bitstring = [int(bit) for bit in bitstring.split(',')]\n",
    "\n",
    "current_learning_iteration += 1\n",
    "for device in devices:\n",
    "    print(int(device.device_id))\n",
    "    if bitstring[int(device.device_id)] == 1:\n",
    "        device.model.fit(device.data[0], device.data[1], epochs=2, verbose=1)\n",
    "        device.number_of_times_fitted += 1\n",
    "        device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "# Call this function after training the local models:\n",
    "aggregate_weights(global_model, devices)\n",
    "\n",
    "\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == 1]\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(self.global_model, selected_devices)\n",
    "\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            # Update device participation based on the bitstring\\n            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\\n\\n            # Train local models for selected devices\\n            for device in selected_devices:\\n                device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\\n\\n            # Aggregate weights to update the global model\\n            aggregate_weights(self.global_model, selected_devices)\\n\\n            # Distribute the updated global model back to all devices\\n            for device in self.devices:\\n                device.model.set_weights(self.global_model.get_weights())\\n            '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.operators.sampling.rnd import BinaryRandomSampling\n",
    "from pymoo.operators.crossover.pntx import TwoPointCrossover\n",
    "from pymoo.operators.mutation.bitflip import BitflipMutation\n",
    "from pymoo.operators.selection.tournament import TournamentSelection\n",
    "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
    "\n",
    "# Parameters\n",
    "NUM_DEVICES = num_devices   # Number of devices (length of bitstring)\n",
    "POPULATION_SIZE = 10 # TODO: make it 100\n",
    "NUM_GENERATIONS = 10\n",
    "\n",
    "# Step 1: Define the Problem\n",
    "import numpy as np\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "class FederatedLearningProblem(Problem):\n",
    "    def __init__(self, num_devices, devices, global_model, x_test_global, y_test_global):\n",
    "        super().__init__(\n",
    "            n_var=num_devices,         # Number of variables (bitstring length)\n",
    "            n_obj=3,                   # Number of objectives\n",
    "            n_constr=0,                # No constraints\n",
    "            xl=np.zeros(num_devices),  # Lower bound (0)\n",
    "            xu=np.ones(num_devices),   # Upper bound (1)\n",
    "            type_var=np.bool_          # Binary variables (bitstrings)\n",
    "        )\n",
    "        self.devices = devices\n",
    "        self.global_model = global_model\n",
    "        self.x_test_global = x_test_global\n",
    "        self.y_test_global = y_test_global\n",
    "\n",
    "        # Save the initial global model weights\n",
    "        self.initial_global_weights = global_model.get_weights()\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"Evaluates objective values for each solution in the population.\"\"\"\n",
    "        num_solutions = len(X)\n",
    "        F = np.zeros((num_solutions, 3))  # Initialize objective matrix\n",
    "\n",
    "        for i, bitstring in enumerate(X):\n",
    "            # Reset the global model to its initial state\n",
    "            self.global_model.set_weights(self.initial_global_weights)\n",
    "            # Update device participation based on the bitstring\n",
    "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\n",
    "            # Objective 1: Hardware Objectives (maximize)\n",
    "            hardware_score = sum(\n",
    "                device.ram + device.storage + device.cpu + device.bandwidth + device.battery + device.charging\n",
    "                for device in selected_devices\n",
    "            )\n",
    "            F[i, 0] = 6 - hardware_score  # Minimize (negative of hardware score)\n",
    "            # Objective 2: Fairness (prioritize devices with lowest local accuracy)\n",
    "            local_accuracies = []\n",
    "            for device in self.devices:\n",
    "              # just the devices in this solution:\n",
    "                if bitstring[int(device.device_id)] == 1:\n",
    "                  _, accuracy = global_model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "                  local_accuracies.append(accuracy)\n",
    "                else:\n",
    "                  local_accuracies.append(0)\n",
    "            # Fairness score: Sum of (1 - accuracy) for selected devices should be minimized\n",
    "            # This prioritizes devices with lower local accuracy\n",
    "\n",
    "            fairness_score = 0  # Initialize fairness score\n",
    "\n",
    "            for j, bit in enumerate(bitstring):\n",
    "                # print(j)\n",
    "                # print(bit)\n",
    "                if bit == 1:  # Check if the device is selected\n",
    "                    fairness_score += local_accuracies[j]  # Add the corresponding accuracy value\n",
    "            # Now, fairness_score contains the final sum\n",
    "\n",
    "            # fairness_score = sum(local_accuracies[j] for j, bit in enumerate(bitstring) if bit == 1)\n",
    "            F[i, 1] = fairness_score  # Minimize (negative of fairness score)\n",
    "            # Objective 3: Global Model Accuracy (maximize)\n",
    "            _, global_accuracy = self.global_model.evaluate(self.x_test_global, self.y_test_global, verbose=0)\n",
    "            F[i, 2] = 1 - global_accuracy  # Minimize (1 - accuracy)\n",
    "        out[\"F\"] = F  # Set the objective values\n",
    "\n",
    "'''\n",
    "            # Update device participation based on the bitstring\n",
    "            selected_devices = [device for device, bit in zip(self.devices, bitstring) if bit == 1]\n",
    "\n",
    "            # Train local models for selected devices\n",
    "            for device in selected_devices:\n",
    "                device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "\n",
    "            # Aggregate weights to update the global model\n",
    "            aggregate_weights(self.global_model, selected_devices)\n",
    "\n",
    "            # Distribute the updated global model back to all devices\n",
    "            for device in self.devices:\n",
    "                device.model.set_weights(self.global_model.get_weights())\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "problem = FederatedLearningProblem(\n",
    "    num_devices=NUM_DEVICES,\n",
    "    devices=devices,\n",
    "    global_model=global_model,\n",
    "    x_test_global=x_test_global,\n",
    "    y_test_global=y_test_global\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Configure NSGA-II Algorithm\n",
    "algorithm = NSGA2(\n",
    "    pop_size=POPULATION_SIZE,\n",
    "    sampling=BinaryRandomSampling(),      # Random bitstrings\n",
    "    crossover=TwoPointCrossover(),        # Two-point crossover\n",
    "    mutation=BitflipMutation(),           # Bit flip mutation\n",
    "    eliminate_duplicates=True             # Avoid duplicate solutions\n",
    ")\n",
    "\n",
    "\n",
    "NUMBER_OF_LEARNING_ITERATIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      7 |             - |             -\n",
      "     2 |       20 |     10 |  0.2074782304 |         ideal\n",
      "     3 |       30 |     10 |  0.1102184289 |         ideal\n",
      "     4 |       40 |     10 |  0.0182857114 |             f\n",
      "     5 |       50 |     10 |  0.0637970254 |         ideal\n",
      "     6 |       60 |     10 |  0.0154374476 |             f\n",
      "     7 |       70 |     10 |  0.0258859262 |             f\n",
      "     8 |       80 |     10 |  0.0205107551 |             f\n",
      "     9 |       90 |     10 |  0.0118530364 |             f\n",
      "    10 |      100 |     10 |  0.0203734623 |             f\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    # termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pareto Front (Bitstrings):\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000000000010111000010000001110\n",
      "1101010011010000111100010111001111011100111100111000111111001111011100101000101110100011111010101111\n",
      "0011001011001000101011010111001101001010011001000100000010111101111110000001000000111000010000001110\n",
      "0011001011001000101011011011011001011100111100100100000010000011110100101110000010100011111010101110\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000001000000100011111010101110\n",
      "0011001011001000101011011011010001011100111100100110000010000011111100101000101101011011111010101110\n",
      "0011001011001000101011010111001101001010011001000100000010111101111110000001000000111000111010100110\n",
      "0011001011001000101011011011011011011100111100111000111111001111011100101000101110100011111010101110\n",
      "0011001011001000101011011011011001011100111100110000111111001111011100101000101110100011111010101110\n",
      "0011001011001000101011010111001111011100111100111000111111001111011100101000101110111000010000001110\n",
      "100\n",
      "0011001011001000101011011011011001001010011001000100000010111101101110000000000010111000010000001110\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1171 - loss: 2.3023\n",
      "Global Model Accuracy: 0.1115\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract the Best Pareto Front\n",
    "pareto_front = res.F   # Objective values of solutions in Pareto front\n",
    "pareto_solutions = res.X  # Corresponding bitstrings\n",
    "\n",
    "# Print the Best Pareto Front Solutions\n",
    "print(\"Best Pareto Front (Bitstrings):\")\n",
    "for bitstring in pareto_solutions:\n",
    "    print(\"\".join(map(str, bitstring)).replace('True','1').replace('False','0'))\n",
    "\n",
    "bitstring = pareto_solutions[0] # for now!\n",
    "bitstring = str(bitstring).replace('False','0').replace('True','1')\n",
    "for char in bitstring:\n",
    "    if char != '0' and char != '1':\n",
    "        bitstring = bitstring.replace(char,'')\n",
    "\n",
    "print(len(bitstring))\n",
    "print(bitstring)\n",
    "temp_bitstring = []\n",
    "for bit in bitstring:\n",
    "    temp_bitstring.append(bit)\n",
    "bitstring = temp_bitstring\n",
    "\n",
    "########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=5, verbose=0)\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0']\n",
      "100\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(bitstring) # for now!\n",
    "print(len(bitstring))\n",
    "print(bitstring.count(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      4 |             - |             -\n",
      "     2 |       20 |      7 |  0.2307692319 |         ideal\n",
      "     3 |       30 |     10 |  0.0359156015 |         ideal\n",
      "     4 |       40 |     10 |  0.1612903352 |         ideal\n",
      "     5 |       50 |     10 |  0.0201756792 |             f\n",
      "     6 |       60 |     10 |  0.0331854369 |             f\n",
      "     7 |       70 |     10 |  0.0406118674 |             f\n",
      "     8 |       80 |     10 |  0.0963562733 |         ideal\n",
      "     9 |       90 |     10 |  0.0199798763 |             f\n",
      "    10 |      100 |     10 |  0.0048348089 |         ideal\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Device 2.0: Loss before: [2.3014464378356934, 0.12708333134651184], Loss after: [2.2971396446228027, 0.12708333134651184]\n",
      "Device 3.0: Loss before: [2.3013997077941895, 0.10625000298023224], Loss after: [2.2967536449432373, 0.22708334028720856]\n",
      "Device 6.0: Loss before: [2.3017570972442627, 0.09791667014360428], Loss after: [2.296715497970581, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.3015942573547363, 0.09583333134651184], Loss after: [2.296424150466919, 0.125]\n",
      "Device 9.0: Loss before: [2.3014817237854004, 0.11249999701976776], Loss after: [2.2969343662261963, 0.11249999701976776]\n",
      "Device 12.0: Loss before: [2.3015387058258057, 0.09583333134651184], Loss after: [2.2940421104431152, 0.34583333134651184]\n",
      "Device 16.0: Loss before: [2.3022358417510986, 0.11041666567325592], Loss after: [2.295330762863159, 0.13750000298023224]\n",
      "Device 18.0: Loss before: [2.3010990619659424, 0.12291666865348816], Loss after: [2.2960875034332275, 0.23541666567325592]\n",
      "Device 20.0: Loss before: [2.3014144897460938, 0.13124999403953552], Loss after: [2.2960660457611084, 0.13124999403953552]\n",
      "Device 21.0: Loss before: [2.3011155128479004, 0.12708333134651184], Loss after: [2.295438528060913, 0.12708333134651184]\n",
      "Device 23.0: Loss before: [2.3015966415405273, 0.10000000149011612], Loss after: [2.297272205352783, 0.13124999403953552]\n",
      "Device 24.0: Loss before: [2.3016726970672607, 0.10625000298023224], Loss after: [2.2984235286712646, 0.12083332985639572]\n",
      "Device 26.0: Loss before: [2.3017969131469727, 0.11041666567325592], Loss after: [2.299024820327759, 0.12708333134651184]\n",
      "Device 27.0: Loss before: [2.3018524646759033, 0.1041666641831398], Loss after: [2.2998712062835693, 0.2083333283662796]\n",
      "Device 29.0: Loss before: [2.301513433456421, 0.125], Loss after: [2.295506238937378, 0.125]\n",
      "Device 30.0: Loss before: [2.301795482635498, 0.10000000149011612], Loss after: [2.2998080253601074, 0.2854166626930237]\n",
      "Device 33.0: Loss before: [2.3015336990356445, 0.11874999850988388], Loss after: [2.298020601272583, 0.14166666567325592]\n",
      "Device 36.0: Loss before: [2.301365852355957, 0.11249999701976776], Loss after: [2.29657244682312, 0.125]\n",
      "Device 38.0: Loss before: [2.3014628887176514, 0.11249999701976776], Loss after: [2.2964463233947754, 0.12708333134651184]\n",
      "Device 41.0: Loss before: [2.302135467529297, 0.08541666716337204], Loss after: [2.2973666191101074, 0.12083332985639572]\n",
      "Device 42.0: Loss before: [2.3012354373931885, 0.13333334028720856], Loss after: [2.2949578762054443, 0.13333334028720856]\n",
      "Device 45.0: Loss before: [2.301501512527466, 0.1145833358168602], Loss after: [2.298041343688965, 0.1145833358168602]\n",
      "Device 49.0: Loss before: [2.3015267848968506, 0.12291666865348816], Loss after: [2.295661449432373, 0.18541666865348816]\n",
      "Device 56.0: Loss before: [2.301163911819458, 0.125], Loss after: [2.2950427532196045, 0.125]\n",
      "Device 58.0: Loss before: [2.3013153076171875, 0.11666666716337204], Loss after: [2.2963807582855225, 0.15208333730697632]\n",
      "Device 59.0: Loss before: [2.3014938831329346, 0.10208333283662796], Loss after: [2.2965469360351562, 0.20416666567325592]\n",
      "Device 60.0: Loss before: [2.3012070655822754, 0.12708333134651184], Loss after: [2.2964560985565186, 0.12708333134651184]\n",
      "Device 61.0: Loss before: [2.301917552947998, 0.10625000298023224], Loss after: [2.29617977142334, 0.13750000298023224]\n",
      "Device 63.0: Loss before: [2.301499843597412, 0.11874999850988388], Loss after: [2.2974956035614014, 0.11874999850988388]\n",
      "Device 64.0: Loss before: [2.3019778728485107, 0.09166666865348816], Loss after: [2.297877073287964, 0.15416666865348816]\n",
      "Device 66.0: Loss before: [2.301910877227783, 0.10625000298023224], Loss after: [2.2969229221343994, 0.125]\n",
      "Device 67.0: Loss before: [2.3015851974487305, 0.11666666716337204], Loss after: [2.298938035964966, 0.11666666716337204]\n",
      "Device 68.0: Loss before: [2.3014917373657227, 0.11874999850988388], Loss after: [2.296771764755249, 0.11874999850988388]\n",
      "Device 80.0: Loss before: [2.302156925201416, 0.10208333283662796], Loss after: [2.2989695072174072, 0.125]\n",
      "Device 82.0: Loss before: [2.3012452125549316, 0.13124999403953552], Loss after: [2.297199010848999, 0.13124999403953552]\n",
      "Device 83.0: Loss before: [2.3017184734344482, 0.11874999850988388], Loss after: [2.297882318496704, 0.18125000596046448]\n",
      "Device 84.0: Loss before: [2.3016233444213867, 0.09791667014360428], Loss after: [2.298541784286499, 0.11666666716337204]\n",
      "Device 89.0: Loss before: [2.3009369373321533, 0.13750000298023224], Loss after: [2.29496431350708, 0.13750000298023224]\n",
      "Device 96.0: Loss before: [2.301833152770996, 0.09791667014360428], Loss after: [2.297668933868408, 0.1354166716337204]\n",
      "Device 97.0: Loss before: [2.301381826400757, 0.125], Loss after: [2.2985572814941406, 0.125]\n",
      "Device 98.0: Loss before: [2.3016843795776367, 0.1354166716337204], Loss after: [2.295980930328369, 0.1354166716337204]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1169 - loss: 2.2997\n",
      "Global Model Accuracy: 0.1114\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      7 |             - |             -\n",
      "     2 |       20 |     10 |  0.1149033047 |         ideal\n",
      "     3 |       30 |     10 |  0.1263791392 |         ideal\n",
      "     4 |       40 |     10 |  0.0446626673 |             f\n",
      "     5 |       50 |     10 |  0.0531813837 |         ideal\n",
      "     6 |       60 |     10 |  0.0627976867 |         ideal\n",
      "     7 |       70 |     10 |  0.0666441130 |         ideal\n",
      "     8 |       80 |     10 |  0.0185034611 |             f\n",
      "     9 |       90 |     10 |  0.0322993886 |             f\n",
      "    10 |      100 |     10 |  0.0970137703 |         ideal\n",
      "False\n",
      "Device 2.0: Loss before: [2.297051191329956, 0.12708333134651184], Loss after: [2.2878005504608154, 0.12708333134651184]\n",
      "Device 3.0: Loss before: [2.2966902256011963, 0.10625000298023224], Loss after: [2.28582763671875, 0.30416667461395264]\n",
      "Device 6.0: Loss before: [2.2979109287261963, 0.09791667014360428], Loss after: [2.2868010997772217, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.297337293624878, 0.09583333134651184], Loss after: [2.2859737873077393, 0.19166666269302368]\n",
      "Device 9.0: Loss before: [2.2969717979431152, 0.11249999701976776], Loss after: [2.286633253097534, 0.21250000596046448]\n",
      "Device 12.0: Loss before: [2.2970564365386963, 0.09583333134651184], Loss after: [2.282228946685791, 0.3395833373069763]\n",
      "Device 16.0: Loss before: [2.2995731830596924, 0.11041666567325592], Loss after: [2.287156343460083, 0.21875]\n",
      "Device 18.0: Loss before: [2.2960264682769775, 0.12291666865348816], Loss after: [2.286231756210327, 0.20000000298023224]\n",
      "Device 20.0: Loss before: [2.296933174133301, 0.13124999403953552], Loss after: [2.286652088165283, 0.13750000298023224]\n",
      "Device 21.0: Loss before: [2.2959656715393066, 0.12708333134651184], Loss after: [2.284320592880249, 0.25833332538604736]\n",
      "Device 23.0: Loss before: [2.2974159717559814, 0.10000000149011612], Loss after: [2.2876734733581543, 0.13124999403953552]\n",
      "Device 24.0: Loss before: [2.2977235317230225, 0.10625000298023224], Loss after: [2.2892298698425293, 0.20208333432674408]\n",
      "Device 26.0: Loss before: [2.2982523441314697, 0.11041666567325592], Loss after: [2.2909278869628906, 0.14791665971279144]\n",
      "Device 27.0: Loss before: [2.2982442378997803, 0.1041666641831398], Loss after: [2.290889024734497, 0.2395833283662796]\n",
      "Device 29.0: Loss before: [2.2973530292510986, 0.125], Loss after: [2.286468982696533, 0.13750000298023224]\n",
      "Device 30.0: Loss before: [2.298081874847412, 0.10000000149011612], Loss after: [2.290691375732422, 0.36666667461395264]\n",
      "Device 33.0: Loss before: [2.2972919940948486, 0.11874999850988388], Loss after: [2.288032293319702, 0.21875]\n",
      "Device 36.0: Loss before: [2.2969465255737305, 0.11249999701976776], Loss after: [2.2874176502227783, 0.21250000596046448]\n",
      "Device 38.0: Loss before: [2.2969579696655273, 0.11249999701976776], Loss after: [2.2857446670532227, 0.3395833373069763]\n",
      "Device 41.0: Loss before: [2.298980474472046, 0.08541666716337204], Loss after: [2.288041830062866, 0.5041666626930237]\n",
      "Device 42.0: Loss before: [2.296447515487671, 0.13333334028720856], Loss after: [2.2848029136657715, 0.16458334028720856]\n",
      "Device 45.0: Loss before: [2.2973082065582275, 0.1145833358168602], Loss after: [2.288760185241699, 0.16249999403953552]\n",
      "Device 49.0: Loss before: [2.297332525253296, 0.12291666865348816], Loss after: [2.2858645915985107, 0.23541666567325592]\n",
      "Device 56.0: Loss before: [2.296236276626587, 0.125], Loss after: [2.28594970703125, 0.125]\n",
      "Device 58.0: Loss before: [2.29667329788208, 0.11666666716337204], Loss after: [2.286677837371826, 0.1875]\n",
      "Device 59.0: Loss before: [2.2971878051757812, 0.10208333283662796], Loss after: [2.2873311042785645, 0.22291666269302368]\n",
      "Device 60.0: Loss before: [2.2964234352111816, 0.12708333134651184], Loss after: [2.2871131896972656, 0.12708333134651184]\n",
      "Device 61.0: Loss before: [2.2982711791992188, 0.10625000298023224], Loss after: [2.286099433898926, 0.2395833283662796]\n",
      "Device 63.0: Loss before: [2.2971651554107666, 0.11874999850988388], Loss after: [2.288503408432007, 0.125]\n",
      "Device 64.0: Loss before: [2.298503875732422, 0.09166666865348816], Loss after: [2.288339853286743, 0.23749999701976776]\n",
      "Device 66.0: Loss before: [2.2988693714141846, 0.10625000298023224], Loss after: [2.2896809577941895, 0.24166665971279144]\n",
      "Device 67.0: Loss before: [2.297590732574463, 0.11666666716337204], Loss after: [2.290259599685669, 0.13750000298023224]\n",
      "Device 68.0: Loss before: [2.2974421977996826, 0.11874999850988388], Loss after: [2.288296699523926, 0.12916666269302368]\n",
      "Device 80.0: Loss before: [2.2993788719177246, 0.10208333283662796], Loss after: [2.2914416790008545, 0.24791666865348816]\n",
      "Device 82.0: Loss before: [2.2965216636657715, 0.13124999403953552], Loss after: [2.28778338432312, 0.1354166716337204]\n",
      "Device 83.0: Loss before: [2.2981009483337402, 0.11874999850988388], Loss after: [2.289912462234497, 0.15416666865348816]\n",
      "Device 84.0: Loss before: [2.297713041305542, 0.09791667014360428], Loss after: [2.289996385574341, 0.27291667461395264]\n",
      "Device 89.0: Loss before: [2.295679807662964, 0.13750000298023224], Loss after: [2.2857773303985596, 0.13750000298023224]\n",
      "Device 96.0: Loss before: [2.298491954803467, 0.09791667014360428], Loss after: [2.2899844646453857, 0.1354166716337204]\n",
      "Device 97.0: Loss before: [2.2969021797180176, 0.125], Loss after: [2.289393663406372, 0.125]\n",
      "Device 98.0: Loss before: [2.297952890396118, 0.1354166716337204], Loss after: [2.287442922592163, 0.1354166716337204]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1382 - loss: 2.2909\n",
      "Global Model Accuracy: 0.1340\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      6 |             - |             -\n",
      "     2 |       20 |     10 |  0.1140514774 |         ideal\n",
      "     3 |       30 |     10 |  0.0191277690 |         ideal\n",
      "     4 |       40 |     10 |  0.0872905024 |         ideal\n",
      "     5 |       50 |     10 |  0.0777815630 |         ideal\n",
      "     6 |       60 |     10 |  0.0156152389 |         ideal\n",
      "     7 |       70 |     10 |  0.0436081211 |         ideal\n",
      "     8 |       80 |     10 |  0.0334872969 |         ideal\n",
      "     9 |       90 |     10 |  0.0040579713 |         nadir\n",
      "    10 |      100 |     10 |  0.0163946562 |             f\n",
      "False\n",
      "Device 2.0: Loss before: [2.2921078205108643, 0.13124999403953552], Loss after: [2.2730791568756104, 0.20208333432674408]\n",
      "Device 3.0: Loss before: [2.291351318359375, 0.12083332985639572], Loss after: [2.267686605453491, 0.44583332538604736]\n",
      "Device 6.0: Loss before: [2.2930214405059814, 0.1041666641831398], Loss after: [2.2700908184051514, 0.14166666567325592]\n",
      "Device 8.0: Loss before: [2.2922558784484863, 0.10208333283662796], Loss after: [2.268527030944824, 0.38749998807907104]\n",
      "Device 9.0: Loss before: [2.2917940616607666, 0.12291666865348816], Loss after: [2.269821882247925, 0.3291666805744171]\n",
      "Device 12.0: Loss before: [2.2916953563690186, 0.10208333283662796], Loss after: [2.2612757682800293, 0.375]\n",
      "Device 16.0: Loss before: [2.295356512069702, 0.11874999850988388], Loss after: [2.272477626800537, 0.34583333134651184]\n",
      "Device 18.0: Loss before: [2.290817975997925, 0.125], Loss after: [2.2713074684143066, 0.24791666865348816]\n",
      "Device 20.0: Loss before: [2.291989803314209, 0.1354166716337204], Loss after: [2.271836042404175, 0.25833332538604736]\n",
      "Device 21.0: Loss before: [2.2905099391937256, 0.12916666269302368], Loss after: [2.2663254737854004, 0.34375]\n",
      "Device 23.0: Loss before: [2.2924582958221436, 0.1041666641831398], Loss after: [2.2721166610717773, 0.3125]\n",
      "Device 24.0: Loss before: [2.2928855419158936, 0.10833333432674408], Loss after: [2.2743468284606934, 0.2854166626930237]\n",
      "Device 26.0: Loss before: [2.2937607765197754, 0.1145833358168602], Loss after: [2.2778289318084717, 0.32083332538604736]\n",
      "Device 27.0: Loss before: [2.293498992919922, 0.11041666567325592], Loss after: [2.275768995285034, 0.34375]\n",
      "Device 29.0: Loss before: [2.2925779819488525, 0.13333334028720856], Loss after: [2.2720718383789062, 0.34166666865348816]\n",
      "Device 30.0: Loss before: [2.2932887077331543, 0.10208333283662796], Loss after: [2.2754194736480713, 0.46666666865348816]\n",
      "Device 33.0: Loss before: [2.292241096496582, 0.12708333134651184], Loss after: [2.271362543106079, 0.27291667461395264]\n",
      "Device 36.0: Loss before: [2.2920639514923096, 0.11249999701976776], Loss after: [2.2731411457061768, 0.35624998807907104]\n",
      "Device 38.0: Loss before: [2.291724681854248, 0.11874999850988388], Loss after: [2.2678017616271973, 0.36250001192092896]\n",
      "Device 41.0: Loss before: [2.2943332195281982, 0.09166666865348816], Loss after: [2.2715742588043213, 0.5583333373069763]\n",
      "Device 42.0: Loss before: [2.291267156600952, 0.13333334028720856], Loss after: [2.2687759399414062, 0.3083333373069763]\n",
      "Device 45.0: Loss before: [2.2923858165740967, 0.12083332985639572], Loss after: [2.273850679397583, 0.40833333134651184]\n",
      "Device 49.0: Loss before: [2.2923998832702637, 0.12916666269302368], Loss after: [2.2699198722839355, 0.28333333134651184]\n",
      "Device 56.0: Loss before: [2.2912817001342773, 0.12708333134651184], Loss after: [2.2724294662475586, 0.22291666269302368]\n",
      "Device 58.0: Loss before: [2.291612386703491, 0.12708333134651184], Loss after: [2.271458148956299, 0.3583333194255829]\n",
      "Device 59.0: Loss before: [2.2922933101654053, 0.10208333283662796], Loss after: [2.2726926803588867, 0.49166667461395264]\n",
      "Device 60.0: Loss before: [2.291414499282837, 0.12708333134651184], Loss after: [2.272871971130371, 0.22291666269302368]\n",
      "Device 61.0: Loss before: [2.293398141860962, 0.11041666567325592], Loss after: [2.2678656578063965, 0.25208333134651184]\n",
      "Device 63.0: Loss before: [2.2922961711883545, 0.125], Loss after: [2.274266242980957, 0.3187499940395355]\n",
      "Device 64.0: Loss before: [2.2937252521514893, 0.09791667014360428], Loss after: [2.271533250808716, 0.26875001192092896]\n",
      "Device 66.0: Loss before: [2.2948005199432373, 0.10833333432674408], Loss after: [2.2778875827789307, 0.3395833373069763]\n",
      "Device 67.0: Loss before: [2.292853832244873, 0.125], Loss after: [2.2763571739196777, 0.31458333134651184]\n",
      "Device 68.0: Loss before: [2.2927968502044678, 0.12291666865348816], Loss after: [2.2751736640930176, 0.23541666567325592]\n",
      "Device 80.0: Loss before: [2.295168399810791, 0.1041666641831398], Loss after: [2.2783591747283936, 0.35208332538604736]\n",
      "Device 82.0: Loss before: [2.291489362716675, 0.13958333432674408], Loss after: [2.2730438709259033, 0.25208333134651184]\n",
      "Device 83.0: Loss before: [2.2936971187591553, 0.12083332985639572], Loss after: [2.277120351791382, 0.21250000596046448]\n",
      "Device 84.0: Loss before: [2.2930305004119873, 0.10833333432674408], Loss after: [2.2764456272125244, 0.39375001192092896]\n",
      "Device 89.0: Loss before: [2.2906413078308105, 0.14166666567325592], Loss after: [2.2726314067840576, 0.15625]\n",
      "Device 96.0: Loss before: [2.294203281402588, 0.10208333283662796], Loss after: [2.277503728866577, 0.13750000298023224]\n",
      "Device 97.0: Loss before: [2.2919440269470215, 0.12916666269302368], Loss after: [2.2752206325531006, 0.23333333432674408]\n",
      "Device 98.0: Loss before: [2.2933833599090576, 0.1458333283662796], Loss after: [2.273547887802124, 0.20416666567325592]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3403 - loss: 2.2767\n",
      "Global Model Accuracy: 0.3383\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       10 |      6 |             - |             -\n",
      "     2 |       20 |     10 |  0.0962540945 |         ideal\n",
      "     3 |       30 |     10 |  0.1702600751 |         ideal\n",
      "     4 |       40 |     10 |  0.0457933994 |         ideal\n",
      "     5 |       50 |     10 |  0.0358850306 |             f\n",
      "     6 |       60 |     10 |  0.0693756153 |         ideal\n",
      "     7 |       70 |     10 |  0.0096587875 |             f\n",
      "     8 |       80 |     10 |  0.0514744528 |         ideal\n",
      "     9 |       90 |     10 |  0.0085521767 |             f\n",
      "    10 |      100 |     10 |  0.0276725565 |             f\n",
      "False\n",
      "Device 2.0: Loss before: [2.279834747314453, 0.31458333134651184], Loss after: [2.2243316173553467, 0.5]\n",
      "Device 3.0: Loss before: [2.277980327606201, 0.2874999940395355], Loss after: [2.204665184020996, 0.512499988079071]\n",
      "Device 6.0: Loss before: [2.2804510593414307, 0.2916666567325592], Loss after: [2.213855504989624, 0.24583333730697632]\n",
      "Device 8.0: Loss before: [2.2793796062469482, 0.2916666567325592], Loss after: [2.208789110183716, 0.42916667461395264]\n",
      "Device 9.0: Loss before: [2.2788119316101074, 0.3166666626930237], Loss after: [2.212261199951172, 0.4416666626930237]\n",
      "Device 12.0: Loss before: [2.278029441833496, 0.32083332538604736], Loss after: [2.185131549835205, 0.35208332538604736]\n",
      "Device 16.0: Loss before: [2.283895969390869, 0.27916666865348816], Loss after: [2.221362352371216, 0.40416666865348816]\n",
      "Device 18.0: Loss before: [2.278273105621338, 0.30000001192092896], Loss after: [2.2222506999969482, 0.4833333194255829]\n",
      "Device 20.0: Loss before: [2.2797892093658447, 0.3062500059604645], Loss after: [2.22204852104187, 0.3958333432674408]\n",
      "Device 21.0: Loss before: [2.277156352996826, 0.34166666865348816], Loss after: [2.203223466873169, 0.3791666626930237]\n",
      "Device 23.0: Loss before: [2.2799623012542725, 0.3333333432674408], Loss after: [2.2199366092681885, 0.518750011920929]\n",
      "Device 24.0: Loss before: [2.280587911605835, 0.27291667461395264], Loss after: [2.225346088409424, 0.4375]\n",
      "Device 26.0: Loss before: [2.2822470664978027, 0.2708333432674408], Loss after: [2.2348573207855225, 0.5104166865348816]\n",
      "Device 27.0: Loss before: [2.2811665534973145, 0.2770833373069763], Loss after: [2.2246882915496826, 0.4583333432674408]\n",
      "Device 29.0: Loss before: [2.280632257461548, 0.3125], Loss after: [2.2238428592681885, 0.4270833432674408]\n",
      "Device 30.0: Loss before: [2.2808899879455566, 0.2770833373069763], Loss after: [2.2239229679107666, 0.4854166805744171]\n",
      "Device 33.0: Loss before: [2.2794175148010254, 0.3020833432674408], Loss after: [2.212902545928955, 0.3499999940395355]\n",
      "Device 36.0: Loss before: [2.280041217803955, 0.3083333373069763], Loss after: [2.2262723445892334, 0.6541666388511658]\n",
      "Device 38.0: Loss before: [2.2785353660583496, 0.3270833194255829], Loss after: [2.2056126594543457, 0.37708333134651184]\n",
      "Device 41.0: Loss before: [2.2819442749023438, 0.2604166567325592], Loss after: [2.2155814170837402, 0.44583332538604736]\n",
      "Device 42.0: Loss before: [2.2785634994506836, 0.30000001192092896], Loss after: [2.2146947383880615, 0.3687500059604645]\n",
      "Device 45.0: Loss before: [2.280015707015991, 0.3062500059604645], Loss after: [2.2237002849578857, 0.5520833134651184]\n",
      "Device 49.0: Loss before: [2.2799885272979736, 0.3083333373069763], Loss after: [2.2169034481048584, 0.36250001192092896]\n",
      "Device 56.0: Loss before: [2.2794337272644043, 0.28125], Loss after: [2.228609800338745, 0.518750011920929]\n",
      "Device 58.0: Loss before: [2.2791526317596436, 0.3229166567325592], Loss after: [2.2206947803497314, 0.5291666388511658]\n",
      "Device 59.0: Loss before: [2.2800791263580322, 0.2666666805744171], Loss after: [2.2241368293762207, 0.6270833611488342]\n",
      "Device 60.0: Loss before: [2.2792551517486572, 0.28958332538604736], Loss after: [2.2261433601379395, 0.5583333373069763]\n",
      "Device 61.0: Loss before: [2.2805705070495605, 0.3166666626930237], Loss after: [2.201840877532959, 0.2874999940395355]\n",
      "Device 63.0: Loss before: [2.28016996383667, 0.2854166626930237], Loss after: [2.2269866466522217, 0.4437499940395355]\n",
      "Device 64.0: Loss before: [2.2811272144317627, 0.27916666865348816], Loss after: [2.2128794193267822, 0.28333333134651184]\n",
      "Device 66.0: Loss before: [2.2842795848846436, 0.2562499940395355], Loss after: [2.23970890045166, 0.44583332538604736]\n",
      "Device 67.0: Loss before: [2.2808799743652344, 0.2874999940395355], Loss after: [2.2299065589904785, 0.4854166805744171]\n",
      "Device 68.0: Loss before: [2.2812530994415283, 0.27291667461395264], Loss after: [2.232931613922119, 0.6145833134651184]\n",
      "Device 80.0: Loss before: [2.2838504314422607, 0.28333333134651184], Loss after: [2.23396372795105, 0.42916667461395264]\n",
      "Device 82.0: Loss before: [2.279125213623047, 0.3229166567325592], Loss after: [2.2231991291046143, 0.3708333373069763]\n",
      "Device 83.0: Loss before: [2.282504081726074, 0.2666666805744171], Loss after: [2.234520435333252, 0.3895833194255829]\n",
      "Device 84.0: Loss before: [2.281235456466675, 0.25208333134651184], Loss after: [2.232619285583496, 0.5520833134651184]\n",
      "Device 89.0: Loss before: [2.2788283824920654, 0.27291667461395264], Loss after: [2.230720043182373, 0.4729166626930237]\n",
      "Device 96.0: Loss before: [2.283189535140991, 0.2645833194255829], Loss after: [2.236621618270874, 0.3062500059604645]\n",
      "Device 97.0: Loss before: [2.2797067165374756, 0.29374998807907104], Loss after: [2.228773355484009, 0.581250011920929]\n",
      "Device 98.0: Loss before: [2.281740427017212, 0.30416667461395264], Loss after: [2.226771831512451, 0.3958333432674408]\n",
      "False\n",
      "global model stayed the same?\n",
      "False\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5205 - loss: 2.2294\n",
      "Global Model Accuracy: 0.5207\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run Optimization\n",
    "problem.initial_global_weights = global_model.get_weights()\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "    # seed=42,\n",
    "    verbose=True\n",
    ")########################################################\n",
    "# Update device participation based on the bitstring\n",
    "selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "aggregate_weights(global_model, devices)\n",
    "# Distribute the updated global model back to all devices\n",
    "for device in devices:\n",
    "    device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "current_learning_iteration += 1\n",
    "# Train local models for selected devices\n",
    "for device in selected_devices:\n",
    "    device.number_of_times_fitted += 1\n",
    "    device.last_round_participated = current_learning_iteration\n",
    "    loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "    loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "    print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# Aggregate weights to update the global model\n",
    "w1 = global_model.get_weights()\n",
    "aggregate_weights(global_model, selected_devices)\n",
    "w2 = global_model.get_weights()\n",
    "print(\"global model stayed the same?\")\n",
    "print(np.array_equal(w1,w2))\n",
    "test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=5, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# w1 = global_model.get_weights()\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# w2 = global_model.get_weights()\n",
    "# print(\"global model stayed the same?\")\n",
    "# print(np.array_equal(w1,w2))\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before aggregation:\", w1[0].flatten()[:5])\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# print(\"After aggregation:\", global_model.get_weights()[0].flatten()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # Update device participation based on the bitstring\n",
    "# selected_devices = [device for device in devices if bitstring[int(device.device_id)] == '1']\n",
    "\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# # Distribute the updated global model back to all devices\n",
    "# for device in devices:\n",
    "#     device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "# current_learning_iteration += 1\n",
    "# # Train local models for selected devices\n",
    "# for device in selected_devices:\n",
    "#     device.number_of_times_fitted += 1\n",
    "#     device.last_round_participated = current_learning_iteration\n",
    "#     loss_before = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "#     device.model.fit(device.data[0], device.data[1], epochs=10, verbose=0)\n",
    "#     loss_after = device.model.evaluate(device.data[0], device.data[1], verbose=0)\n",
    "#     print(f\"Device {device.device_id}: Loss before: {loss_before}, Loss after: {loss_after}\")\n",
    "\n",
    "# # Aggregate weights to update the global model\n",
    "# w1 = global_model.get_weights()\n",
    "# aggregate_weights(global_model, selected_devices)\n",
    "# w2 = global_model.get_weights()\n",
    "# print(\"global model stayed the same?\")\n",
    "# print(np.array_equal(w1,w2))\n",
    "# test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "# print(f\"Global Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(NUMBER_OF_LEARNING_ITERATIONS):\n",
    "#     # Step 3: Run Optimization\n",
    "#     res = minimize(\n",
    "#         problem=problem,\n",
    "#         algorithm=algorithm,\n",
    "#         # termination=MultiObjectiveSpaceToleranceTermination(tol=1e-6, n_last=10, nth_gen=5, n_max_gen=NUM_GENERATIONS),\n",
    "#         termination=DefaultMultiObjectiveTermination(n_max_gen=NUM_GENERATIONS),\n",
    "#         seed=42,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Step 4: Extract the Best Pareto Front\n",
    "#     pareto_front = res.F   # Objective values of solutions in Pareto front\n",
    "#     pareto_solutions = res.X  # Corresponding bitstrings\n",
    "\n",
    "#     # Print the Best Pareto Front Solutions\n",
    "#     print(\"Best Pareto Front (Bitstrings):\")\n",
    "#     for bitstring in pareto_solutions:\n",
    "#         print(\"\".join(map(str, bitstring)))\n",
    "\n",
    "#     bitstring = pareto_solutions[0] # for now!\n",
    "#     print(bitstring)\n",
    "#     # Convert to bitstring (list of 1s and 0s)\n",
    "#     bitstring = [1 if word == \"True\" else 0 for word in str(bitstring).replace(\"True\", \"1 \").replace(\"False\", \"0 \").split()]\n",
    "\n",
    "\n",
    "#     ########################################################\n",
    "#     # Update device participation based on the bitstring\n",
    "#     selected_devices = [device for device in devices if bitstring[int(device.device_id)] == 1]\n",
    "\n",
    "#     # Aggregate weights to update the global model\n",
    "#     aggregate_weights(global_model, selected_devices)\n",
    "\n",
    "#     # Distribute the updated global model back to all devices\n",
    "#     for device in devices:\n",
    "#         device.model.set_weights(global_model.get_weights())\n",
    "\n",
    "#     current_learning_iteration += 1\n",
    "#     # Train local models for selected devices\n",
    "#     for device in selected_devices:\n",
    "#         device.model.fit(device.data[0], device.data[1], epochs=1, verbose=0)\n",
    "#         device.number_of_times_fitted += 1\n",
    "#         device.last_round_participated = current_learning_iteration\n",
    "\n",
    "\n",
    "#     test_loss, test_acc = global_model.evaluate(x_test_global, y_test_global)\n",
    "#     print(f\"Global Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#     ################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
